Publication date,Title,Link
2020/06/01,ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning,https://arxiv.org/pdf/2006.00719
2020/05/16,MicroNet for Efficient Language Modeling,https://arxiv.org/pdf/2005.07877
2020/05/06,Learning Architectures from an Extended Search Space for Language Modeling,https://arxiv.org/pdf/2005.02593
2020/04/30,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,https://arxiv.org/pdf/2004.14996
2020/03/17,PowerNorm: Rethinking Batch Normalization in Transformers,https://arxiv.org/pdf/2003.07845
2020/03/12,Efficient Content-Based Sparse Attention with Routing Transformers,https://arxiv.org/pdf/2003.05997
2020/02/27,Temporal Convolutional Attention-based Network For Sequence Modeling,https://arxiv.org/pdf/2002.12530
2019/11/29,One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation,https://arxiv.org/pdf/1912.00120
2019/11/25,Rigging the Lottery: Making All Tickets Winners,https://arxiv.org/pdf/1911.11134
2019/10/31,Generalization through Memorization: Nearest Neighbor Language Models,https://arxiv.org/pdf/1911.00172
2019/10/20,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,https://arxiv.org/pdf/1910.08910
2019/09/24,Gap Aware Mitigation of Gradient Staleness,https://arxiv.org/pdf/1909.10802
2019/07/21,Efficient Novelty-Driven Neural Architecture Search,https://arxiv.org/pdf/1907.09109
2019/06/10,Improving Neural Language Modeling via Adversarial Training,https://arxiv.org/pdf/1906.03805
2019/06/04,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",https://arxiv.org/pdf/1906.01702