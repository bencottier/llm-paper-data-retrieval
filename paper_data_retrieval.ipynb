{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieving data from papers using GPT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /Users/bencottier/miniconda3/envs/nlp\n",
            "\n",
            "  added / updated specs:\n",
            "    - openai\n",
            "\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2022.12.~ --> pkgs/main::ca-certificates-2023.01.10-hecd8cb5_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge/noarch::certifi-2022.12.7~ --> pkgs/main/osx-64::certifi-2022.12.7-py39hecd8cb5_0\n",
            "  openssl            conda-forge::openssl-1.1.1t-hfd90126_0 --> pkgs/main::openssl-1.1.1t-hca72f7f_0\n",
            "\n",
            "\n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda install -y openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ee3887XnrDa",
        "outputId": "5cde1173-289a-44e4-d6a1-c911b058c4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.11.0\n",
            "  latest version: 23.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /Users/bencottier/miniconda3/envs/nlp\n",
            "\n",
            "  added / updated specs:\n",
            "    - pdfminer.six\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2023.5.7   |       h8857fd0_0         145 KB  conda-forge\n",
            "    certifi-2023.5.7           |     pyhd8ed1ab_0         149 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         294 KB\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2023.5.7-h8857fd0_0\n",
            "  certifi            pkgs/main/osx-64::certifi-2022.12.7-p~ --> conda-forge/noarch::certifi-2023.5.7-pyhd8ed1ab_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  openssl              pkgs/main::openssl-1.1.1t-hca72f7f_0 --> conda-forge::openssl-1.1.1t-hfd90126_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ca-certificates-2023 | 145 KB    | ##################################### | 100% \n",
            "certifi-2023.5.7     | 149 KB    | ##################################### | 100% \n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda install -y -c conda-forge pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from pdfminer.high_level import extract_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('output_data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_paper_text = \"PaLM: Scaling Language Modeling with Pathways\\nAakanksha Chowdhery∗ Sharan Narang∗ Jacob Devlin∗\\nMaarten Bosma Gaurav Mishra Adam Roberts Paul Barham\\nHyung Won Chung Charles Sutton Sebastian Gehrmann Parker Schuh Kensen Shi\\nSasha Tsvyashchenko Joshua Maynez Abhishek Rao† Parker Barnes Yi Tay\\nNoam Shazeer‡ Vinodkumar Prabhakaran Emily Reif Nan Du Ben Hutchinson\\nReiner Pope James Bradbury Jacob Austin Michael Isard Guy Gur-Ari\\nPengcheng Yin Toju Duke Anselm Levskaya Sanjay Ghemawat Sunipa Dev\\nHenryk Michalewski Xavier Garcia Vedant Misra Kevin Robinson Liam Fedus\\nDenny Zhou Daphne Ippolito David Luan‡ Hyeontaek Lim Barret Zoph\\nAlexander Spiridonov Ryan Sepassi David Dohan Shivani Agrawal Mark Omernick\\nAndrew M. Dai Thanumalayan Sankaranarayana Pillai Marie Pellat Aitor Lewkowycz\\nErica Moreira Rewon Child Oleksandr Polozov† Katherine Lee Zongwei Zhou\\nXuezhi Wang Brennan Saeta Mark Diaz Orhan Firat Michele Catasta† Jason Wei\\nKathy Meier-Hellstern Douglas Eck Jeff Dean Slav Petrov Noah Fiedel\\nGoogle Research\\nAbstract\\nLarge language models have been shown to achieve remarkable performance across a variety of natural\\nlanguage tasks using few-shot learning, which drastically reduces the number of task-specific training\\nexamples needed to adapt the model to a particular application. To further our understanding of the\\nimpact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer\\nlanguage model, which we call Pathways Language Model (PaLM).\\nWe trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient\\ntraining across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-\\nthe-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a\\nnumber of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-\\nof-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the\\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous\\nimprovements from model scale, meaning that performance steeply increased as we scaled to our largest\\nmodel. PaLM also has strong capabilities in multilingual tasks and source code generation, which we\\ndemonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias\\nand toxicity, and study the extent of training data memorization with respect to model scale. Finally,\\nwe discuss the ethical considerations related to large language models and discuss potential mitigation\\nstrategies.\\n∗Equal Contribution. Author contributions and ordering details are listed in Appendix A.\\nCorrespondence authors: chowdhery@google.com, sharannarang@google.com\\nIn addition to other contributions, the last five authors advised the overall project.\\n†Alphabet, X, the Moonshot Factory\\n‡Work done while at Google\\n\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7EJDodhVtqVMFdhqaqJqkZLn8QU4m at 0x7f7df05b5630> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"| ---------------------- | -------------------------- | ------ |\\n| 6144                   | TPU v4                    | N/A    |\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1683644836,\n",
              "  \"id\": \"cmpl-7EJDodhVtqVMFdhqaqJqkZLn8QU4m\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 27,\n",
              "    \"prompt_tokens\": 753,\n",
              "    \"total_tokens\": 780\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=f\"A table summarizing the training hardware from this paper:\\n\\n====\\n\\n{example_paper_text}\\n\\n====\\n\\n| Number of GPUs or TPUs | Hardware model (e.g. A100) | FLOP/s |\\n\",\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. N/A\n",
            "2. TPUv4\n",
            "3. N/A\n"
          ]
        }
      ],
      "source": [
        "prompt_text = f\"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "1. How many GPUs or TPUs were used to train the model? Just state the number. If the number of GPUs or TPUs is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "3. What FLOP/s (AKA: FLOP/second, FLOPS) was achieved during training? Include the same units as written in the paper. If FLOP/s is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "3. 21 TFLOP/s\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "3. 21 petaflops\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "3. 127e12 FLOPS\n",
        "\n",
        "====\n",
        "\n",
        "{example_paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt_text,\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7EJDz8p5dDiFFRchOLuCpfY9eaZsz at 0x7f7d909ec450> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"1. 6144\\n2. TPU v4\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1683644847,\n",
              "  \"id\": \"chatcmpl-7EJDz8p5dDiFFRchOLuCpfY9eaZsz\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 12,\n",
              "    \"prompt_tokens\": 901,\n",
              "    \"total_tokens\": 913\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_text = \"\"\"\n",
        "Read the following excerpt of a Machine Learning research paper and answer the questions below. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "1. How many GPUs or TPUs were used to train the model? Just state the number. If the number of GPUs or TPUs is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "\"\"\"\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant who is an expert in the field of Machine Learning.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_text},\n",
        "        {\"role\": \"assistant\", \"content\": \"Understood.\"},\n",
        "        {\"role\": \"user\", \"content\": example_paper_text,}\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_message_template = \"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def parse_text_gpt_chat(text):\n",
        "    prompt_text = chat_message_template.format(paper_text=text)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt_text},\n",
        "        ]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def parse_gpt_chat_response(response):\n",
        "    # E.g. \"1. 6144 TPUs\\n2. TPU v4\\n3. N/A\\n\"\n",
        "    answers = response[\"choices\"][0][\"message\"][\"content\"].strip().split(\"\\n\")\n",
        "    answers = [a.split(\".\")[-1].strip() for a in answers]\n",
        "    return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "3. What FLOP/s (AKA: FLOP/second, FLOPS) was achieved during training? Include the same units as written in the paper. If FLOP/s is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "3. 21 TFLOP/s\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "3. 21 petaflops\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "3. 127e12 FLOPS\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def parse_text_gpt(text):\n",
        "    prompt_text = prompt_template.format(paper_text=text)\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=prompt_text,\n",
        "        temperature=0,\n",
        "        max_tokens=100,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def parse_gpt_response(response):\n",
        "    # E.g. \"1. 6144 TPUs\\n2. TPU v4\\n3. N/A\\n\"\n",
        "    answers = response[\"choices\"][0][\"text\"].strip().split(\"\\n\")\n",
        "    answers = [a.split(\".\")[-1].strip() for a in answers]\n",
        "    return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I've heard that English has about 4 chars per token on average.\n",
        "# `text-davinci-003` token limit (including output) is 4097.\n",
        "# So 4097 * 3 should be pretty safe.\n",
        "CHAR_LIMIT = 4097*3\n",
        "\n",
        "def parse_paper(df, i, row, keys):\n",
        "    url = row['Link']\n",
        "\n",
        "    # replace \"abs\" with \"pdf\" in arxiv url links\n",
        "    url = url.replace('abs', 'pdf')\n",
        "    print(f\"Looking into \\\"{row['Reference']}\\\"\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "    except Exception as e:\n",
        "        print(f\"There's something wrong with downloading: {e}\")\n",
        "        raise e\n",
        "\n",
        "    file = open(\"download.pdf\", \"wb\")\n",
        "    file.seek(0) # overwrite previous file\n",
        "    file.write(response.content)\n",
        "    file.close()\n",
        "\n",
        "    try:\n",
        "        text = extract_text('download.pdf')\n",
        "\n",
        "        answers = parse_gpt_chat_response(parse_text_gpt_chat(text[:CHAR_LIMIT]))\n",
        "\n",
        "        for key, answer in zip(keys, answers):\n",
        "            df.loc[i,key]  = answer if answer else \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"There's something wrong with extracting the text: {e}\")\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkwpFPzmWyG6",
        "outputId": "057026f6-0b56-49cd-a84e-2ba4773b4eae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/g6/w_cfz8c507j6mbt5mdv7wt_h0000gn/T/ipykernel_1162/93879255.py:16: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking into \"GPT-4 Technical Report\"\n",
            "---\n",
            "Looking into \"Phenaki: Variable Length Video Generation From Open Domain Textual Description\"\n",
            "---\n",
            "Looking into \"Solving Quantitative Reasoning Problems with Language Models\"\n",
            "---\n",
            "Looking into \"PaLM: Scaling Language Modeling with Pathways\"\n",
            "---\n",
            "Looking into \"Training Compute-Optimal Large Language Models\"\n",
            "---\n",
            "Looking into \"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\"\n",
            "---\n",
            "Looking into \"LaMDA: Language Models for Dialog Applications\"\n",
            "---\n",
            "Looking into \"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\"\n",
            "---\n",
            "Looking into \"High-Resolution Image Synthesis with Latent Diffusion Models\"\n",
            "---\n",
            "Looking into \"Robust Speech Recognition via Large-Scale Weak Supervision\"\n",
            "---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author(s)</th>\n",
              "      <th>Publication date</th>\n",
              "      <th>Reference</th>\n",
              "      <th>Link</th>\n",
              "      <th>Number of hardware units</th>\n",
              "      <th>Hardware model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OpenAI</td>\n",
              "      <td>2023-03-15</td>\n",
              "      <td>GPT-4 Technical Report</td>\n",
              "      <td>https://arxiv.org/abs/2303.08774</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ruben Villegas, Mohammad Babaeizadeh, Pieter-J...</td>\n",
              "      <td>2022-10-05</td>\n",
              "      <td>Phenaki: Variable Length Video Generation From...</td>\n",
              "      <td>https://arxiv.org/abs/2210.02399</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aitor Lewkowycz, Anders Andreassen, David Doha...</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>Solving Quantitative Reasoning Problems with L...</td>\n",
              "      <td>https://arxiv.org/abs/2206.14858</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aakanksha Chowdhery, Sharan Narang, Jacob Devl...</td>\n",
              "      <td>2022-04-04</td>\n",
              "      <td>PaLM: Scaling Language Modeling with Pathways</td>\n",
              "      <td>https://arxiv.org/abs/2204.02311</td>\n",
              "      <td>6144</td>\n",
              "      <td>TPU v4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
              "      <td>2022-03-29</td>\n",
              "      <td>Training Compute-Optimal Large Language Models</td>\n",
              "      <td>https://arxiv.org/abs/2203.15556</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Titan V</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...</td>\n",
              "      <td>2022-06-22</td>\n",
              "      <td>Scaling Autoregressive Models for Content-Rich...</td>\n",
              "      <td>https://arxiv.org/abs/2206.10789v1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Romal Thoppilan, Daniel De Freitas, Jamie Hall...</td>\n",
              "      <td>2022-02-10</td>\n",
              "      <td>LaMDA: Language Models for Dialog Applications</td>\n",
              "      <td>https://arxiv.org/abs/2201.08239</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Saleh Soltan, Shankar Ananthakrishnan, Jack Fi...</td>\n",
              "      <td>2022-08-02</td>\n",
              "      <td>AlexaTM 20B: Few-Shot Learning Using a Large-S...</td>\n",
              "      <td>https://arxiv.org/abs/2208.01448</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Titan V</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Robin Rombach, Andreas Blattmann, Dominik Lore...</td>\n",
              "      <td>2022-04-13</td>\n",
              "      <td>High-Resolution Image Synthesis with Latent Di...</td>\n",
              "      <td>https://arxiv.org/abs/2112.10752</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Alec Radford, Jong Wook Kim, Tao Xu, Greg Broc...</td>\n",
              "      <td>2022-09-21</td>\n",
              "      <td>Robust Speech Recognition via Large-Scale Weak...</td>\n",
              "      <td>https://cdn.openai.com/papers/whisper.pdf</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Author(s) Publication date  \\\n",
              "0                                              OpenAI       2023-03-15   \n",
              "1   Ruben Villegas, Mohammad Babaeizadeh, Pieter-J...       2022-10-05   \n",
              "2   Aitor Lewkowycz, Anders Andreassen, David Doha...       2022-06-29   \n",
              "3   Aakanksha Chowdhery, Sharan Narang, Jacob Devl...       2022-04-04   \n",
              "5   Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...       2022-03-29   \n",
              "6   Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...       2022-06-22   \n",
              "7   Romal Thoppilan, Daniel De Freitas, Jamie Hall...       2022-02-10   \n",
              "9   Saleh Soltan, Shankar Ananthakrishnan, Jack Fi...       2022-08-02   \n",
              "12  Robin Rombach, Andreas Blattmann, Dominik Lore...       2022-04-13   \n",
              "13  Alec Radford, Jong Wook Kim, Tao Xu, Greg Broc...       2022-09-21   \n",
              "\n",
              "                                            Reference  \\\n",
              "0                              GPT-4 Technical Report   \n",
              "1   Phenaki: Variable Length Video Generation From...   \n",
              "2   Solving Quantitative Reasoning Problems with L...   \n",
              "3       PaLM: Scaling Language Modeling with Pathways   \n",
              "5      Training Compute-Optimal Large Language Models   \n",
              "6   Scaling Autoregressive Models for Content-Rich...   \n",
              "7      LaMDA: Language Models for Dialog Applications   \n",
              "9   AlexaTM 20B: Few-Shot Learning Using a Large-S...   \n",
              "12  High-Resolution Image Synthesis with Latent Di...   \n",
              "13  Robust Speech Recognition via Large-Scale Weak...   \n",
              "\n",
              "                                         Link Number of hardware units  \\\n",
              "0            https://arxiv.org/abs/2303.08774                      N/A   \n",
              "1            https://arxiv.org/abs/2210.02399                      N/A   \n",
              "2            https://arxiv.org/abs/2206.14858                      N/A   \n",
              "3            https://arxiv.org/abs/2204.02311                     6144   \n",
              "5            https://arxiv.org/abs/2203.15556                      N/A   \n",
              "6          https://arxiv.org/abs/2206.10789v1                      N/A   \n",
              "7            https://arxiv.org/abs/2201.08239                      N/A   \n",
              "9            https://arxiv.org/abs/2208.01448                      N/A   \n",
              "12           https://arxiv.org/abs/2112.10752                      N/A   \n",
              "13  https://cdn.openai.com/papers/whisper.pdf                      N/A   \n",
              "\n",
              "   Hardware model  \n",
              "0             N/A  \n",
              "1             N/A  \n",
              "2             N/A  \n",
              "3          TPU v4  \n",
              "5         Titan V  \n",
              "6             N/A  \n",
              "7             N/A  \n",
              "9         Titan V  \n",
              "12            N/A  \n",
              "13            N/A  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download dataset from the Parameters, Compute and Data Trends in ML sheet\n",
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/export?format=csv#gid=0')\n",
        "\n",
        "year_start = 2017\n",
        "\n",
        "# Recode columns\n",
        "df['Publication date'] = pd.to_datetime(df['Publication date'], errors='coerce', dayfirst=True)\n",
        "\n",
        "# Filter for papers of only the last 5 years\n",
        "df = df[df['Publication date'] > f'{year_start}-01-01']\n",
        "\n",
        "# Keep only bibliographical data\n",
        "df = df.filter(['Author(s)', 'Publication date', 'Reference', 'Link'])\n",
        "df = df[df['Link'].notna()]\n",
        "# Keep only links which forward to a pdf or an arxiv link\n",
        "df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n",
        "\n",
        "keys = ['Number of hardware units', 'Hardware model', 'Training FLOP/s']\n",
        "\n",
        "# Enable for test running with the first ten papers\n",
        "df = df[:10]\n",
        "# Or a specific paper\n",
        "# idx = 4\n",
        "# df = df[idx:idx+1]\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    try:\n",
        "        parse_paper(df, i, row, keys)\n",
        "        print(\"---\")\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "display(df)\n",
        "\n",
        "timestamp = datetime.datetime.now()\n",
        "df.to_csv(f'output_data/parsed_paper_data_{timestamp.strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b7c6d073edd95e87cfa6524fac94a63a54fb86c88ec13e975ab85409bf024d2f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
