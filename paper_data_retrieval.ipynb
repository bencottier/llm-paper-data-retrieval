{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieving data from papers using GPT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /Users/bencottier/miniconda3/envs/nlp\n",
            "\n",
            "  added / updated specs:\n",
            "    - tenacity\n",
            "\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2023.5.7~ --> pkgs/main::ca-certificates-2023.01.10-hecd8cb5_0\n",
            "  certifi            conda-forge/noarch::certifi-2023.5.7-~ --> pkgs/main/osx-64::certifi-2023.5.7-py39hecd8cb5_0\n",
            "  openssl            conda-forge::openssl-1.1.1t-hfd90126_0 --> pkgs/main::openssl-1.1.1t-hca72f7f_0\n",
            "\n",
            "\n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda install -y tenacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!conda install -y openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.11.0\n",
            "  latest version: 23.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /Users/bencottier/miniconda3/envs/nlp\n",
            "\n",
            "  added / updated specs:\n",
            "    - tiktoken\n",
            "\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2023.5.7-h8857fd0_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            pkgs/main/osx-64::certifi-2023.5.7-py~ --> conda-forge/noarch::certifi-2023.5.7-pyhd8ed1ab_0\n",
            "  openssl              pkgs/main::openssl-1.1.1t-hca72f7f_0 --> conda-forge::openssl-1.1.1t-hfd90126_0\n",
            "\n",
            "\n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda install -y -c conda-forge tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ee3887XnrDa",
        "outputId": "5cde1173-289a-44e4-d6a1-c911b058c4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.11.0\n",
            "  latest version: 23.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!conda install -y -c conda-forge pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "from pdfminer.high_level import extract_text\n",
        "import re\n",
        "import requests\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff on API calls\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('output_data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Base character limit for the model.\n",
        "May be adjusted by prompt length.\n",
        "I've heard that English has about 4 chars per token on average.\n",
        "But papers may have parts with a lot of digits, which I think are one token each.\n",
        "GPT-3 token limit (including output) is 4096.\n",
        "So a base value of 4096 is safe.\n",
        "This should be subtracted by the prompt length later.\n",
        "\"\"\"\n",
        "BASE_CHAR_LIMIT = 4096\n",
        "\n",
        "\"\"\"\n",
        "The number of questions the model is asked.\n",
        "This should be updated along with the prompt.\n",
        "\"\"\"\n",
        "NUM_QUESTIONS = 2\n",
        "\n",
        "# Token limit for each model\n",
        "MAX_TOKENS = {\n",
        "    \"gpt-4\": 8192,\n",
        "    \"gpt-4-32k\": 32768,\n",
        "    \"gpt-3.5-turbo\": 4096,\n",
        "    \"text-davinci-003\": 4097,\n",
        "}\n",
        "\n",
        "# Token limit for model output\n",
        "MAX_RESPONSE_TOKENS = 100"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_paper_text = \"\"\"\n",
        "2 Related work\n",
        "\n",
        "Language models and dialog models: Language models have attracted much attention recently thanks to their\n",
        "successes in NLP applications (e.g., [19, 20, 21, 2, 1, 22, 23, 5, 12, 24]). Our study of scaling laws with respect to\n",
        "model sizes is inspired by recent work on the scaling laws of neural language models [12, 13]. Similar to their ﬁndings,\n",
        "our results show that model scaling improves our quality (sensibleness, speciﬁcity, and interestingness), safety and\n",
        "groundedness metrics to some extent. However, ﬁne-tuning combined with scaling signiﬁcantly improves performance\n",
        "on all metrics.\n",
        "\n",
        "Our work is also closely related to recent successes in applying language models to dialog modeling (e.g., [25, 26,\n",
        "17, 18]), which built on earlier research in neural dialog modeling (e.g., [14, 15, 16, 27, 28]). One of our ﬁne-tuning\n",
        "stages requires training on dialog-only data, which is related to Wolf et al. [29], Dinan et al. [25] and Zhang et al. [30].\n",
        "Our use of ﬁne-tuning on crowdworker-annotated data to improve interestingness is comparable to Roller et al. [18].\n",
        "However, we aim to maximize the interestingness of the model’s output distinctly from its ability to engage the user in\n",
        "further interaction.\n",
        "\n",
        "Our ﬁnding that pure scaling has a limited effect on key measures of open-domain dialog model performance echoes\n",
        "that of Shuster et al. [31], who also focus on the problem of groundedness. Recent studies on scaling have found that\n",
        "performance on question-answering tasks improves with model size [32, 33], similar to our ﬁndings on pre-trained\n",
        "LaMDA prior to ﬁne-tuning.\n",
        "\n",
        "Our approach to improving model groundedness is broadly consistent with a growing literature on augmenting neural\n",
        "language models with retrieval systems. Most of the existing literature focuses on the problem of open-domain\n",
        "question-answering rather than dialog generation, and the models themselves are used to index and rank knowledge\n",
        "sources, rather than trained to use an intermediate tool. Given these differences, we note that the range of existing\n",
        "approaches to this problem include the RNNLM [34], RAG [35], REALM [36], and FiD [37] architectures. Zhu et\n",
        "al. [38] provide a survey of further recent work. See Karpukhin et al. [39] for details on the ‘dense passage retriever’\n",
        "used in RAG. Recent work in this direction has expanded and elaborated on neural models’ ability to retrieve and rank\n",
        "passages [40]. The RETRO architecture demonstrates that language models can be primed with results retrieved from\n",
        "a database as large as two trillion tokens [41]. At a broad level, our approach is also comparable to that of Byrne et\n",
        "al. [42], which ﬁne-tunes the model to use external APIs for movie ticketing dialog.\n",
        "\n",
        "Parts of our ﬁndings are similar to recent studies on dialog groundedness. Granting access to external knowledge\n",
        "bases has been shown to reduce the rate at which models hallucinate unsourced statements in dialog across a variety of\n",
        "retrieval systems and model architectures [31]. Another study ﬁnds that a question-answering system’s accuracy is\n",
        "improved by separating it into a reasoning unit and a response generator, analogous to our separation of ‘Base’ and\n",
        "‘Research’ models in our study [43]. Meanwhile, the WebGPT framework includes a language system that can interact\n",
        "with the open web via a text-only interface, and learns to imitate humans in answering questions by citing external\n",
        "sources [44]. Komeili et al. [45] compare different types of pre-trained models and retrieval methods, and reach a\n",
        "similar conclusion that augmenting language models with a search engine provides more factually grounded responses.\n",
        "They encode the input context with grounded information from search to generate the next response, while we augment\n",
        "the generated responses with information from known sources in our method. This allows us to ﬁne-tune the model for\n",
        "groundedness without sacriﬁcing gains in safety or quality from other ﬁne-tuning treatments.\n",
        "\n",
        "Dialog metrics: Deﬁning effective metrics for dialog models remains an open research topic. Our approach is\n",
        "inspired by Adiwardana et al. [17], who argued for human-like metrics, such as sensibleness and speciﬁcity. Many\n",
        "automated metrics for dialog models have been studied, including perplexity [16, 17], F1, Hits@1/N [25], USR [46],\n",
        "or BLEU/ROUGE [47, 15, 27]. However, such automated metrics may not correlate well with human judgment [48].\n",
        "More reliable metrics for dialog modeling require human evaluation [49, 50, 18, 25, 17, 51], as used in this paper.\n",
        "\n",
        "Earlier research attempted to combine multifaceted evaluations of dialog quality into a single headline metric [52]. We\n",
        "follow the pattern established in Adiwardana et al. [17] and Roller et al. [18] by considering the different components\n",
        "of our evaluations separately. In addition to sensibleness and speciﬁcity per Adiwardana et al. [17], we add new metrics:\n",
        "interestingness, safety, and groundedness. An advantage of using several different metrics is their debuggability: by\n",
        "exploring responses with low safety or groundedness scores, we have been able to develop targeted methods to improve\n",
        "them.\n",
        "\n",
        "Safety and safety of dialog models:\n",
        "Inappropriate and unsafe risks and behaviors of language models have been\n",
        "extensively discussed and studied in previous works (e.g., [53, 54]). Issues encountered include toxicity (e.g., [55, 56,\n",
        "57]), bias (e.g., [58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]), and inappropriately revealing personally\n",
        "identifying information (PII) from training data [73]. Weidinger et al. [54] identify 21 risks associated with large-scale\n",
        "\n",
        "3\n",
        "\n",
        "\flanguage models and discuss the points of origin for these risks. While many mitigation strategies have also been\n",
        "suggested (e.g., [74, 75, 76, 77, 78, 79, 80, 81, 82]), meaningfully addressing these issues remains an active research\n",
        "area.\n",
        "\n",
        "Similar issues have also been discussed speciﬁcally for dialog models [53]. For instance, examples of bias, offensiveness,\n",
        "and hate speech have been found both in training data drawn from social media, and consequently in the output of dialog\n",
        "models trained on such data [83]. Dialog models [84] can learn, and even amplify, biases in the training data. Echoing\n",
        "Gehman et al. [85], we ﬁnd ﬁne-tuning effective to augment language models for safety. The method we use in this\n",
        "paper follows previous attempts to tackle these issues by training separate layers to detect unsafe output [17, 86, 18, 79].\n",
        "Our strategy is similar to recent work that also uses ﬁne-tuning [87]. While their safety guidelines were derived from\n",
        "human rights principles, they similarly ﬁnd that increasing scale has no impact on toxicity metrics, while ﬁne-tuning on\n",
        "safety evaluations does.\n",
        "\n",
        "Groundedness metrics: Similar to other recent research into groundedness cited above, we assess groundedness\n",
        "by asking crowdworkers to judge whether the model’s output is in accordance with authoritative external sources.\n",
        "The recently-proposed Attributable to Identiﬁed Sources (AIS) framework [88] articulates a more precise approach\n",
        "to assess output of language models that pertains to the external world. It splits evaluation into two stages, where\n",
        "crowdworkers are asked: (1) if they can understand and identify the information shared in a dialog turn, and (2) if all\n",
        "of this information can be attributed to a source. Meanwhile, a recent study has reopened the question of automatic\n",
        "evaluation, with the Q2 metric showing performance comparable to human annotation [89].\n",
        "\n",
        "3 LaMDA pre-training\n",
        "\n",
        "LaMDA was pre-trained to predict the next token in a text corpus. Unlike previous dialog models trained on dialog data\n",
        "alone [17, 18], we pre-trained LaMDA on a dataset created from public dialog data and other public web documents.\n",
        "Therefore, LaMDA can be used as a general language model prior to ﬁne-tuning.\n",
        "\n",
        "The pre-training dataset consists of 2.97B documents, 1.12B dialogs, and 13.39B dialog utterances, for a total of 1.56T\n",
        "words (Appendix E). Over 90% of the pre-training dataset is in the English language. We used the SentencePiece\n",
        "library [90] to tokenize the dataset into 2.81T byte pair encoding (BPE) tokens [91], with a vocabulary of 32K tokens.\n",
        "For comparison, the total number of words in the training set for Meena [17] was 40B words, which is nearly 40x\n",
        "smaller.\n",
        "\n",
        "The largest LaMDA model has 137B non-embedding parameters, which is ~50x more parameters than Meena [17].\n",
        "We use a decoder-only Transformer [92] language model as the model architecture for LaMDA. The Transformer has\n",
        "64 layers, dmodel = 8192, df f = 65536, h = 128, dk = dv = 128, relative attention as described in T5 [11], and\n",
        "gated-GELU activation as described in Raffel et al. [93].\n",
        "\n",
        "We pre-trained LaMDA on 1024 TPU-v3 chips for a total of about 57.7 days, and 256K tokens per batch. We used\n",
        "the Lingvo framework [94] for training and achieved 123 TFLOPS/sec with 56.5% FLOPS utilization with the 2D\n",
        "sharding algorithm, as described in GSPMD [95] (see Section 10 for carbon footprint estimates). We also trained\n",
        "smaller 2B-parameter and 8B-parameter models to measure the effects of model scaling on our metrics. Hyperparameter\n",
        "details for the models of different sizes can be found in Table 27, Appendix D.\n",
        "\n",
        "Figure 2 gives an overview of the pre-training stage. We call the model before any ﬁne-tuning \"PT\", for PreTrained.\n",
        "\n",
        "Figure 2: LaMDA pre-training as a language model.\n",
        "\n",
        "4\n",
        "\n",
        "\fPT uses the same sample-and-rank strategy as Meena [17] for decoding. We ﬁrst sample 16 independent candidate\n",
        "responses using top-k (k = 40) sampling (no temperature). The ﬁnal output is the highest-scoring candidate, where the\n",
        "score is based on the candidate’s log-likelihood and its length.\n",
        "\n",
        "4 Metrics\n",
        "\n",
        "Evaluating generative models in general, and open-ended dialog models in particular, is difﬁcult. See the Related\n",
        "Work section for a general review of recent work in this area. In this section, we describe the metrics that we use for\n",
        "evaluation.\n",
        "\n",
        "4.1 Foundation metrics: Quality, Safety and Groundedness\n",
        "\n",
        "Sensibleness, Speciﬁcity, Interestingness (SSI): Our overall quality score is an average of sensibleness, speciﬁcity,\n",
        "and interestingness (SSI).\n",
        "\n",
        "Adiwardana et al. [17] propose the sensibleness and speciﬁcity average (SSA) metric to measure the quality of Meena.\n",
        "This metric is a simple average of two scores: sensibleness and speciﬁcity.\n",
        "\n",
        "The ﬁrst score, sensibleness, measures whether a model’s responses make sense in context and do not contradict\n",
        "anything that was said earlier. Humans tend to take this basic aspect of communication for granted, but generative\n",
        "models often struggle to meet this requirement. However, if sensibleness alone is used to evaluate models, we could\n",
        "inadvertently reward models for playing it safe by always producing short, generic, and boring responses. The\n",
        "GenericBot algorithm [17], which answers every question with “I don’t know” and every statement with “Ok,” scores\n",
        "70% on sensibleness, which even surpasses some large dialog models [17].\n",
        "\n",
        "The second score, speciﬁcity, is used to measure whether a response is speciﬁc to a given context. For example, if a user\n",
        "says “I love Eurovision” and the model responds “Me too,” then it would score 0 on speciﬁcity, since this response could\n",
        "be used in many different contexts. If it answers “Me too. I love Eurovision songs,” then it would score 1. Adiwardana\n",
        "et al. [17] report that Meena narrows the gap to average human performance in the SSA metric.\n",
        "\n",
        "As the model’s performance increases, however, we ﬁnd that sensibleness and speciﬁcity are not sufﬁcient to measure\n",
        "the quality of a dialog model. For example, a response to “How do I throw a ball?” could be “You can throw a ball by\n",
        "ﬁrst picking it up and then throwing it”, which makes sense and is speciﬁc to the question. An alternative deeper and\n",
        "more satisfying answer could be “One way to toss a ball is to hold it ﬁrmly in both hands and then swing your arm\n",
        "down and up again, extending your elbow and then releasing the ball upwards.”\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7GpBPwe7HuuZAPWQKPeXJzqfxQtaX at 0x7f9689162220> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"| ---------------------- | --------------------------- | ------ |\\n| 1024                  | TPU-v3                      | 123 TFLOPS/sec |\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1684244351,\n",
              "  \"id\": \"cmpl-7GpBPwe7HuuZAPWQKPeXJzqfxQtaX\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 29,\n",
              "    \"prompt_tokens\": 3117,\n",
              "    \"total_tokens\": 3146\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=f\"A table summarizing the training hardware from this paper:\\n\\n====\\n\\n{example_paper_text}\\n\\n====\\n\\n| Number of GPUs or TPUs | Hardware model (e.g. A100) | FLOP/s |\\n\",\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. N/A\n",
            "2. N/A\n",
            "3. 123 TFLOP/s\n"
          ]
        }
      ],
      "source": [
        "prompt_text = f\"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "1. How many GPUs or TPUs were used to train the model? Just state the number. If the number of GPUs or TPUs is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "3. What FLOP/s (AKA: FLOP/second, FLOPS) was achieved during training? Include the same units as written in the paper. If FLOP/s is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "3. 21 TFLOP/s\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "3. 21 petaflops\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "3. 127e12 FLOPS\n",
        "\n",
        "====\n",
        "\n",
        "{example_paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt_text,\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7GpBW3GmfzX2pXup6Dao78vgc85QC at 0x7f96885c3950> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"1. 1024 TPUs were used to pre-train the LaMDA model.\\n2. The TPU model used to pre-train LaMDA is TPU-v3.\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1684244358,\n",
              "  \"id\": \"chatcmpl-7GpBW3GmfzX2pXup6Dao78vgc85QC\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 39,\n",
              "    \"prompt_tokens\": 3074,\n",
              "    \"total_tokens\": 3113\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_text = \"\"\"\n",
        "Read the excerpt of a Machine Learning research paper below and answer the following questions.\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\"\"\"\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert in Machine Learning.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"{prompt_text}\\n\\n====\\n\\n{example_paper_text}\\n\\n====\\n\\n\"},\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_message_template = \"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some made-up example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "\n",
        "1. N/A\n",
        "2. TPUv3\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def parse_text_gpt_chat(text):\n",
        "    prompt_text = chat_message_template.format(paper_text=text)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            # {\"role\": \"system\", \"content\": \"You are an expert in Machine Learning.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt_text},\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=MAX_RESPONSE_TOKENS,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def parse_gpt_chat_response(text):\n",
        "    response = parse_text_gpt_chat(text)\n",
        "    # E.g. \"1. 6144 TPUs\\n2. TPU v4\\n3. N/A\\n\"\n",
        "    # print(f\"Response: {response['choices'][0]['message']['content']}\")\n",
        "    answers = response[\"choices\"][0][\"message\"][\"content\"].strip().split(\"\\n\")\n",
        "    answers = [a.split(\".\")[-1].strip() for a in answers]\n",
        "    return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "completion_prompt_template = \"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are made-up some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "\n",
        "1. N/A\n",
        "2. TPUv3\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def parse_text_gpt(text):\n",
        "    prompt_text = completion_prompt_template.format(paper_text=text)\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=prompt_text,\n",
        "        temperature=0,\n",
        "        max_tokens=MAX_RESPONSE_TOKENS,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def parse_gpt_response(text):\n",
        "    response = parse_text_gpt(text)\n",
        "    # E.g. \"1. 6144 TPUs\\n2. TPU v4\\n3. N/A\\n\"\n",
        "    # print(response[\"choices\"][0][\"text\"])\n",
        "    answers = response[\"choices\"][0][\"text\"].strip().split(\"\\n\")\n",
        "    answers = [a.split(\".\")[-1].strip() for a in answers]\n",
        "    return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_answers(text, num_questions=1, mode=\"chat\"):\n",
        "    answer_fcn = parse_gpt_chat_response if mode == \"chat\" else parse_gpt_response\n",
        "    model_name = \"gpt-3.5-turbo\" if mode == \"chat\" else \"text-davinci-003\"\n",
        "    prompt_template = chat_message_template if mode == \"chat\" else completion_prompt_template\n",
        "\n",
        "    tokenizer = tiktoken.encoding_for_model(model_name)\n",
        "    encoded_text = tokenizer.encode(text, disallowed_special=())\n",
        "    # print(f\"Full #tokens: {len(encoded_text)}\")\n",
        "    # The prompt text will be added to the paper text later\n",
        "    # Add constant extra tokens for a little buffer when using the chat model\n",
        "    max_tokens = MAX_TOKENS[model_name] - (len(tokenizer.encode(prompt_template)) + MAX_RESPONSE_TOKENS + 20)\n",
        "    # print(f\"Max #tokens: {max_tokens}\")\n",
        "\n",
        "    text_pos = 0\n",
        "    final_answers = [None] * num_questions\n",
        "    while text_pos < len(encoded_text):\n",
        "        # Get model answers for the next chunk of the text\n",
        "        encoded_text_chunk = encoded_text[text_pos : text_pos + max_tokens]\n",
        "        # print(f\"Chunk #tokens: {len(encoded_text_chunk)}\")\n",
        "        text_chunk = tokenizer.decode(encoded_text_chunk)  # the model will encode again\n",
        "        answers = answer_fcn(text_chunk)\n",
        "        # Process each answer\n",
        "        for i in range(num_questions):\n",
        "            try:\n",
        "                if final_answers[i] is None and answers[i] is not None:\n",
        "                    # Take the first answer as the final answer initially\n",
        "                    final_answers[i] = answers[i]\n",
        "                elif \"N/A\" in final_answers[i] and not (\"N/A\" in answers[i]):\n",
        "                    \"\"\"\n",
        "                    If the answer was \"N/A\" previously but there's at least \n",
        "                    one non-\"N/A\" answer for a later chunk, then use the \n",
        "                    first non-\"N/A\" answer as the final answer\n",
        "                    \"\"\"\n",
        "                    final_answers[i] = answers[i]\n",
        "            except IndexError:\n",
        "                print(f\"IndexError: index={i}, answers={answers}, final_answers={final_answers}\")\n",
        "        # Move to the next chunk of text\n",
        "        text_pos += max_tokens\n",
        "    return final_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_paper(df, i, row, keys):\n",
        "    url = row['Link']\n",
        "\n",
        "    # replace \"abs\" with \"pdf\" in arxiv url links\n",
        "    url = url.replace('abs', 'pdf')\n",
        "    print(f\"Looking into \\\"{row['Reference']}\\\"\")\n",
        "\n",
        "    # try:\n",
        "    #     response = requests.get(url)\n",
        "    # except Exception as e:\n",
        "    #     print(f\"There's something wrong with downloading: {e}\")\n",
        "    #     raise e\n",
        "\n",
        "    # file = open(\"download.pdf\", \"wb\")\n",
        "    # file.seek(0) # overwrite previous file\n",
        "    # file.write(response.content)\n",
        "    # file.close()\n",
        "\n",
        "    # try:\n",
        "    # text = extract_text('download.pdf')\n",
        "    paper_title = row['Reference'].replace(' ', '_').replace(':', '').replace('\"', '').lower()\n",
        "    with open('input_data/' + paper_title + '.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    answers = get_model_answers(text, num_questions=NUM_QUESTIONS, mode='chat')\n",
        "    print(f\"Answers: {answers}\")\n",
        "    \n",
        "    for key, answer in zip(keys, answers):\n",
        "        df.loc[i,key] = answer if answer else \"none\"\n",
        "    # except Exception as e:\n",
        "    #     print(f\"There's something wrong with extracting the text: {e}\")\n",
        "    #     raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_answers(df, answer_keys, correct_answers):\n",
        "    correct_dict = dict()\n",
        "    for key in answer_keys:\n",
        "        print(key)\n",
        "        correct = 0\n",
        "        for i, row in df.iterrows():\n",
        "            ref = row['Reference']\n",
        "            answer = row[key]\n",
        "            correct_answer = correct_answers[key][ref]\n",
        "            if answer == correct_answer:\n",
        "                correct += 1\n",
        "            else:\n",
        "                print(f\"{answer} != {correct_answer}\")\n",
        "        correct_dict[key] = correct\n",
        "    for k, v in correct_dict.items():\n",
        "        print(f\"{k}: {v}/{len(df)}\")\n",
        "    return correct_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "CORRECT_ANSWERS = {\n",
        "    'Number of hardware units': {\n",
        "        'No Language Left Behind: Scaling Human-Centered Machine Translation': 'N/A',\n",
        "        'Solving Quantitative Reasoning Problems with Language Models': '1024',\n",
        "        'Scaling Autoregressive Models for Content-Rich Text-to-Image Generation': 'N/A',\n",
        "        'A Generalist Agent': '256',\n",
        "        'OPT: Open Pre-trained Transformer Language Models': '992',\n",
        "        'PaLM: Scaling Language Modeling with Pathways': '6144',\n",
        "        'Training Compute-Optimal Large Language Models': 'N/A',\n",
        "        'Efficient Language Modeling with Sparse all-MLP': '32',\n",
        "        'Announcing GPT- NeoX- 20B': '96',\n",
        "        'LaMDA: Language Models for Dialog Applications': '1024',\n",
        "        'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale': 'N/A',\n",
        "        'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding': '1024',\n",
        "        'Generative Pretraining from Pixels': 'N/A',\n",
        "        'Once for all: Train one network and specialize it for efficient deployment.': '32',\n",
        "        'Language models are Few- Shot Learners': 'N/A',\n",
        "        'ProGen: Language Modeling for Protein Generation': '128',\n",
        "        'Turing-NLG: A 17-billion-parameter language model by Microsoft': '256',\n",
        "        'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.': '512',\n",
        "        'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures': '1',\n",
        "        'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm': '64',\n",
        "        'Progressive Neural Architecture Search': '100',\n",
        "        'Mastering the game of Go without human knowledge': '64',\n",
        "        'Dota 2 ': 'N/A',\n",
        "        'Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.': '50',\n",
        "        'Attention Is All You Need': '8',\n",
        "        'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer': '64',\n",
        "        'DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker': '20'\n",
        "    },\n",
        "    'Hardware model': {\n",
        "        'No Language Left Behind: Scaling Human-Centered Machine Translation': 'A100',\n",
        "        'Solving Quantitative Reasoning Problems with Language Models': 'TPUv4',\n",
        "        'Scaling Autoregressive Models for Content-Rich Text-to-Image Generation': 'TPUv4',\n",
        "        'A Generalist Agent': 'TPUv3',\n",
        "        'OPT: Open Pre-trained Transformer Language Models': 'A100',\n",
        "        'PaLM: Scaling Language Modeling with Pathways': 'TPUv4',\n",
        "        'Training Compute-Optimal Large Language Models': 'TPUv3, TPUv4',\n",
        "        'Efficient Language Modeling with Sparse all-MLP': 'V100',\n",
        "        'Announcing GPT- NeoX- 20B': 'A100',\n",
        "        'LaMDA: Language Models for Dialog Applications': 'TPUv3',\n",
        "        'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale': 'TPUv3',\n",
        "        'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding': 'TPUv3',\n",
        "        'Generative Pretraining from Pixels': 'V100',\n",
        "        'Once for all: Train one network and specialize it for efficient deployment.': 'V100',\n",
        "        'Language models are Few- Shot Learners': 'V100',\n",
        "        'ProGen: Language Modeling for Protein Generation': 'TPUv3',\n",
        "        'Turing-NLG: A 17-billion-parameter language model by Microsoft': 'V100',\n",
        "        'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.': 'TPUv3',\n",
        "        'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures': 'P100',\n",
        "        'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm': 'TPUv2',\n",
        "        'Progressive Neural Architecture Search': 'P100',\n",
        "        'Mastering the game of Go without human knowledge': 'N/A',\n",
        "        'Dota 2 ': 'N/A',\n",
        "        'Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.': 'K80',\n",
        "        'Attention Is All You Need': 'P100',\n",
        "        'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer': 'K40',\n",
        "        'DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker': 'N/A'\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['No Language Left Behind: Scaling Human-Centered Machine Translation',\n",
              " 'Solving Quantitative Reasoning Problems with Language Models',\n",
              " 'Scaling Autoregressive Models for Content-Rich Text-to-Image Generation',\n",
              " 'A Generalist Agent',\n",
              " 'OPT: Open Pre-trained Transformer Language Models',\n",
              " 'PaLM: Scaling Language Modeling with Pathways',\n",
              " 'Training Compute-Optimal Large Language Models',\n",
              " 'Efficient Language Modeling with Sparse all-MLP',\n",
              " 'Announcing GPT- NeoX- 20B',\n",
              " 'LaMDA: Language Models for Dialog Applications',\n",
              " 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',\n",
              " 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding',\n",
              " 'Generative Pretraining from Pixels',\n",
              " 'Once for all: Train one network and specialize it for efficient deployment.',\n",
              " 'Language models are Few- Shot Learners',\n",
              " 'ProGen: Language Modeling for Protein Generation',\n",
              " 'Turing-NLG: A 17-billion-parameter language model by Microsoft',\n",
              " 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.',\n",
              " 'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures',\n",
              " 'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm',\n",
              " 'Progressive Neural Architecture Search',\n",
              " 'Mastering the game of Go without human knowledge',\n",
              " 'Dota 2 ',\n",
              " 'Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.',\n",
              " 'Attention Is All You Need',\n",
              " 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer',\n",
              " 'DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker']"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paper_titles = list(CORRECT_ANSWERS['Hardware model'].keys())\n",
        "print(len(paper_titles))\n",
        "paper_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkwpFPzmWyG6",
        "outputId": "057026f6-0b56-49cd-a84e-2ba4773b4eae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/g6/w_cfz8c507j6mbt5mdv7wt_h0000gn/T/ipykernel_14503/3471291030.py:16: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking into \"Solving Quantitative Reasoning Problems with Language Models\"\n",
            "Answers: ['N/A', 'N/A']\n",
            "---\n",
            "Looking into \"PaLM: Scaling Language Modeling with Pathways\"\n",
            "Answers: ['6144', 'TPU v4']\n",
            "---\n",
            "Looking into \"Training Compute-Optimal Large Language Models\"\n",
            "Answers: [')', 'TPUv3/TPUv4']\n",
            "---\n",
            "Looking into \"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\"\n",
            "Answers: ['N/A', 'ViT-VQGAN']\n",
            "---\n",
            "Looking into \"LaMDA: Language Models for Dialog Applications\"\n",
            "Answers: ['1024', 'TPU-v3']\n",
            "---\n",
            "Looking into \"Efficient Language Modeling with Sparse all-MLP\"\n",
            "Answers: ['32', 'V100']\n",
            "---\n",
            "Looking into \"Language models are Few- Shot Learners\"\n",
            "Answers: [')', 'V100 (used to train all models)']\n",
            "---\n",
            "Looking into \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\"\n",
            "Answers: ['2048', 'TPUv3']\n",
            "---\n",
            "Looking into \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\"\n",
            "Answers: ['N/A', 'Cloud TPU V3']\n",
            "---\n",
            "Looking into \"Once for all: Train one network and specialize it for efficient deployment.\"\n",
            "Answers: ['N/A', 'V100 (for training the full network)']\n",
            "---\n",
            "Looking into \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\n",
            "Answers: [')', 'TPUv3']\n",
            "---\n",
            "Looking into \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\"\n",
            "Answers: ['1 GPU', 'N/A']\n",
            "---\n",
            "Looking into \"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\"\n",
            "Answers: ['', '']\n",
            "---\n",
            "Looking into \"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.\"\n",
            "Answers: ['50', 'K80']\n",
            "---\n",
            "Looking into \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\"\n",
            "Answers: ['N/A', 'K40, Tesla']\n",
            "---\n",
            "Looking into \"Progressive Neural Architecture Search\"\n",
            "Answers: ['256', 'N/A']\n",
            "---\n",
            "Looking into \"Attention Is All You Need\"\n",
            "Answers: ['8 GPUs', 'P100']\n",
            "---\n",
            "Looking into \"DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker\"\n",
            "Answers: ['1', 'NVIDIA GeForce GTX 1080']\n",
            "---\n",
            "18\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author(s)</th>\n",
              "      <th>Publication date</th>\n",
              "      <th>Reference</th>\n",
              "      <th>Link</th>\n",
              "      <th>Number of hardware units</th>\n",
              "      <th>Hardware model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aitor Lewkowycz, Anders Andreassen, David Doha...</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>Solving Quantitative Reasoning Problems with L...</td>\n",
              "      <td>https://arxiv.org/abs/2206.14858</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aakanksha Chowdhery, Sharan Narang, Jacob Devl...</td>\n",
              "      <td>2022-04-04</td>\n",
              "      <td>PaLM: Scaling Language Modeling with Pathways</td>\n",
              "      <td>https://arxiv.org/abs/2204.02311</td>\n",
              "      <td>6144</td>\n",
              "      <td>TPU v4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
              "      <td>2022-03-29</td>\n",
              "      <td>Training Compute-Optimal Large Language Models</td>\n",
              "      <td>https://arxiv.org/abs/2203.15556</td>\n",
              "      <td>)</td>\n",
              "      <td>TPUv3/TPUv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...</td>\n",
              "      <td>2022-06-22</td>\n",
              "      <td>Scaling Autoregressive Models for Content-Rich...</td>\n",
              "      <td>https://arxiv.org/abs/2206.10789v1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>ViT-VQGAN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Romal Thoppilan, Daniel De Freitas, Jamie Hall...</td>\n",
              "      <td>2022-02-10</td>\n",
              "      <td>LaMDA: Language Models for Dialog Applications</td>\n",
              "      <td>https://arxiv.org/abs/2201.08239</td>\n",
              "      <td>1024</td>\n",
              "      <td>TPU-v3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...</td>\n",
              "      <td>2022-04-14</td>\n",
              "      <td>Efficient Language Modeling with Sparse all-MLP</td>\n",
              "      <td>https://arxiv.org/abs/2203.06850</td>\n",
              "      <td>32</td>\n",
              "      <td>V100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...</td>\n",
              "      <td>2020-05-28</td>\n",
              "      <td>Language models are Few- Shot Learners</td>\n",
              "      <td>https://arxiv.org/abs/2005.14165</td>\n",
              "      <td>)</td>\n",
              "      <td>V100 (used to train all models)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...</td>\n",
              "      <td>2020-06-30</td>\n",
              "      <td>GShard: Scaling Giant Models with Conditional ...</td>\n",
              "      <td>https://arxiv.org/abs/2006.16668</td>\n",
              "      <td>2048</td>\n",
              "      <td>TPUv3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...</td>\n",
              "      <td>2020-02-09</td>\n",
              "      <td>ALBERT: A Lite BERT for Self-supervised Learni...</td>\n",
              "      <td>https://arxiv.org/abs/1909.11942</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Cloud TPU V3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...</td>\n",
              "      <td>2020-04-29</td>\n",
              "      <td>Once for all: Train one network and specialize...</td>\n",
              "      <td>https://arxiv.org/abs/1908.09791</td>\n",
              "      <td>N/A</td>\n",
              "      <td>V100 (for training the full network)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...</td>\n",
              "      <td>2020-10-22</td>\n",
              "      <td>An Image is Worth 16x16 Words: Transformers fo...</td>\n",
              "      <td>https://arxiv.org/abs/2010.11929</td>\n",
              "      <td>)</td>\n",
              "      <td>TPUv3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...</td>\n",
              "      <td>2018-02-05</td>\n",
              "      <td>IMPALA: Scalable Distributed Deep-RL with Impo...</td>\n",
              "      <td>https://arxiv.org/abs/1802.01561</td>\n",
              "      <td>1 GPU</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>D Silver, T Hubert, J Schrittwieser, I Antonoglou</td>\n",
              "      <td>2017-12-05</td>\n",
              "      <td>Mastering Chess and Shogi by Self-Play with a ...</td>\n",
              "      <td>https://arxiv.org/abs/1712.01815</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...</td>\n",
              "      <td>2017-08-04</td>\n",
              "      <td>Revisiting Unreasonable Effectiveness of Data ...</td>\n",
              "      <td>https://arxiv.org/abs/1707.02968</td>\n",
              "      <td>50</td>\n",
              "      <td>K80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>N Shazeer, A Mirhoseini, K Maziarz, A Davis</td>\n",
              "      <td>2017-01-23</td>\n",
              "      <td>Outrageously Large Neural Networks: The Sparse...</td>\n",
              "      <td>https://arxiv.org/abs/1701.06538</td>\n",
              "      <td>N/A</td>\n",
              "      <td>K40, Tesla</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>C Liu, B Zoph, M Neumann, J Shlens</td>\n",
              "      <td>2017-12-02</td>\n",
              "      <td>Progressive Neural Architecture Search</td>\n",
              "      <td>https://arxiv.org/abs/1712.00559</td>\n",
              "      <td>256</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...</td>\n",
              "      <td>2017-06-12</td>\n",
              "      <td>Attention Is All You Need</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2017/file...</td>\n",
              "      <td>8 GPUs</td>\n",
              "      <td>P100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>Matej Moravčík, Martin Schmid, Neil Burch, Vil...</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>DeepStack: Expert-Level Artificial Intelligenc...</td>\n",
              "      <td>https://arxiv.org/abs/1701.01724</td>\n",
              "      <td>1</td>\n",
              "      <td>NVIDIA GeForce GTX 1080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Author(s) Publication date  \\\n",
              "3    Aitor Lewkowycz, Anders Andreassen, David Doha...       2022-06-29   \n",
              "4    Aakanksha Chowdhery, Sharan Narang, Jacob Devl...       2022-04-04   \n",
              "6    Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...       2022-03-29   \n",
              "7    Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...       2022-06-22   \n",
              "8    Romal Thoppilan, Daniel De Freitas, Jamie Hall...       2022-02-10   \n",
              "21   Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...       2022-04-14   \n",
              "100  Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...       2020-05-28   \n",
              "103  Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...       2020-06-30   \n",
              "108  Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...       2020-02-09   \n",
              "111  Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...       2020-04-29   \n",
              "132  Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...       2020-10-22   \n",
              "185  Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...       2018-02-05   \n",
              "208  D Silver, T Hubert, J Schrittwieser, I Antonoglou       2017-12-05   \n",
              "211  ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...       2017-08-04   \n",
              "212        N Shazeer, A Mirhoseini, K Maziarz, A Davis       2017-01-23   \n",
              "213                 C Liu, B Zoph, M Neumann, J Shlens       2017-12-02   \n",
              "214  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...       2017-06-12   \n",
              "215  Matej Moravčík, Martin Schmid, Neil Burch, Vil...       2017-01-06   \n",
              "\n",
              "                                             Reference  \\\n",
              "3    Solving Quantitative Reasoning Problems with L...   \n",
              "4        PaLM: Scaling Language Modeling with Pathways   \n",
              "6       Training Compute-Optimal Large Language Models   \n",
              "7    Scaling Autoregressive Models for Content-Rich...   \n",
              "8       LaMDA: Language Models for Dialog Applications   \n",
              "21     Efficient Language Modeling with Sparse all-MLP   \n",
              "100             Language models are Few- Shot Learners   \n",
              "103  GShard: Scaling Giant Models with Conditional ...   \n",
              "108  ALBERT: A Lite BERT for Self-supervised Learni...   \n",
              "111  Once for all: Train one network and specialize...   \n",
              "132  An Image is Worth 16x16 Words: Transformers fo...   \n",
              "185  IMPALA: Scalable Distributed Deep-RL with Impo...   \n",
              "208  Mastering Chess and Shogi by Self-Play with a ...   \n",
              "211  Revisiting Unreasonable Effectiveness of Data ...   \n",
              "212  Outrageously Large Neural Networks: The Sparse...   \n",
              "213             Progressive Neural Architecture Search   \n",
              "214                          Attention Is All You Need   \n",
              "215  DeepStack: Expert-Level Artificial Intelligenc...   \n",
              "\n",
              "                                                  Link  \\\n",
              "3                     https://arxiv.org/abs/2206.14858   \n",
              "4                     https://arxiv.org/abs/2204.02311   \n",
              "6                     https://arxiv.org/abs/2203.15556   \n",
              "7                   https://arxiv.org/abs/2206.10789v1   \n",
              "8                     https://arxiv.org/abs/2201.08239   \n",
              "21                    https://arxiv.org/abs/2203.06850   \n",
              "100                   https://arxiv.org/abs/2005.14165   \n",
              "103                   https://arxiv.org/abs/2006.16668   \n",
              "108                   https://arxiv.org/abs/1909.11942   \n",
              "111                   https://arxiv.org/abs/1908.09791   \n",
              "132                   https://arxiv.org/abs/2010.11929   \n",
              "185                   https://arxiv.org/abs/1802.01561   \n",
              "208                   https://arxiv.org/abs/1712.01815   \n",
              "211                   https://arxiv.org/abs/1707.02968   \n",
              "212                   https://arxiv.org/abs/1701.06538   \n",
              "213                   https://arxiv.org/abs/1712.00559   \n",
              "214  https://proceedings.neurips.cc/paper/2017/file...   \n",
              "215                   https://arxiv.org/abs/1701.01724   \n",
              "\n",
              "    Number of hardware units                        Hardware model  \n",
              "3                        N/A                                   N/A  \n",
              "4                       6144                                TPU v4  \n",
              "6                          )                           TPUv3/TPUv4  \n",
              "7                        N/A                             ViT-VQGAN  \n",
              "8                       1024                                TPU-v3  \n",
              "21                        32                                  V100  \n",
              "100                        )       V100 (used to train all models)  \n",
              "103                     2048                                 TPUv3  \n",
              "108                      N/A                          Cloud TPU V3  \n",
              "111                      N/A  V100 (for training the full network)  \n",
              "132                        )                                 TPUv3  \n",
              "185                    1 GPU                                   N/A  \n",
              "208                     none                                  none  \n",
              "211                       50                                   K80  \n",
              "212                      N/A                            K40, Tesla  \n",
              "213                      256                                   N/A  \n",
              "214                   8 GPUs                                  P100  \n",
              "215                        1               NVIDIA GeForce GTX 1080  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download dataset from the Parameters, Compute and Data Trends in ML sheet\n",
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/export?format=csv#gid=0')\n",
        "\n",
        "year_start = 2017\n",
        "\n",
        "# Recode columns\n",
        "df['Publication date'] = pd.to_datetime(df['Publication date'], errors='coerce', dayfirst=True)\n",
        "\n",
        "# Filter for papers of only the last 5 years\n",
        "df = df[df['Publication date'] > f'{year_start}-01-01']\n",
        "\n",
        "# Keep only bibliographical data\n",
        "df = df.filter(['Author(s)', 'Publication date', 'Reference', 'Link'])\n",
        "df = df[df['Link'].notna()]\n",
        "# Keep only links which forward to a pdf or an arxiv link\n",
        "df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n",
        "\n",
        "keys = ['Number of hardware units', 'Hardware model']\n",
        "\n",
        "# Enable for test running with the first ten papers\n",
        "# df = df[1:11]\n",
        "# Or a specific paper\n",
        "# idx = 4\n",
        "# df = df[idx:idx+1]\n",
        "# Or by title\n",
        "df = df[df['Reference'].isin(paper_titles)]\n",
        "df = df.drop_duplicates(subset=['Reference'], keep='first')\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    # try:\n",
        "    parse_paper(df, i, row, keys)\n",
        "    print(\"---\")\n",
        "    # except:\n",
        "    #     continue\n",
        "print(len(df))\n",
        "display(df)\n",
        "\n",
        "timestamp = datetime.datetime.now()\n",
        "df.to_csv(f'output_data/parsed_paper_data_{timestamp.strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author(s)</th>\n",
              "      <th>Publication date</th>\n",
              "      <th>Reference</th>\n",
              "      <th>Link</th>\n",
              "      <th>Number of hardware units</th>\n",
              "      <th>Hardware model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aitor Lewkowycz, Anders Andreassen, David Doha...</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>Solving Quantitative Reasoning Problems with L...</td>\n",
              "      <td>https://arxiv.org/abs/2206.14858</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aakanksha Chowdhery, Sharan Narang, Jacob Devl...</td>\n",
              "      <td>2022-04-04</td>\n",
              "      <td>PaLM: Scaling Language Modeling with Pathways</td>\n",
              "      <td>https://arxiv.org/abs/2204.02311</td>\n",
              "      <td>6144</td>\n",
              "      <td>TPU v4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
              "      <td>2022-03-29</td>\n",
              "      <td>Training Compute-Optimal Large Language Models</td>\n",
              "      <td>https://arxiv.org/abs/2203.15556</td>\n",
              "      <td>)</td>\n",
              "      <td>TPUv3/TPUv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...</td>\n",
              "      <td>2022-06-22</td>\n",
              "      <td>Scaling Autoregressive Models for Content-Rich...</td>\n",
              "      <td>https://arxiv.org/abs/2206.10789v1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>ViT-VQGAN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Romal Thoppilan, Daniel De Freitas, Jamie Hall...</td>\n",
              "      <td>2022-02-10</td>\n",
              "      <td>LaMDA: Language Models for Dialog Applications</td>\n",
              "      <td>https://arxiv.org/abs/2201.08239</td>\n",
              "      <td>1024</td>\n",
              "      <td>TPU-v3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...</td>\n",
              "      <td>2022-04-14</td>\n",
              "      <td>Efficient Language Modeling with Sparse all-MLP</td>\n",
              "      <td>https://arxiv.org/abs/2203.06850</td>\n",
              "      <td>32</td>\n",
              "      <td>V100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...</td>\n",
              "      <td>2020-05-28</td>\n",
              "      <td>Language models are Few- Shot Learners</td>\n",
              "      <td>https://arxiv.org/abs/2005.14165</td>\n",
              "      <td>)</td>\n",
              "      <td>V100 (used to train all models)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...</td>\n",
              "      <td>2020-06-30</td>\n",
              "      <td>GShard: Scaling Giant Models with Conditional ...</td>\n",
              "      <td>https://arxiv.org/abs/2006.16668</td>\n",
              "      <td>2048</td>\n",
              "      <td>TPUv3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...</td>\n",
              "      <td>2020-02-09</td>\n",
              "      <td>ALBERT: A Lite BERT for Self-supervised Learni...</td>\n",
              "      <td>https://arxiv.org/abs/1909.11942</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Cloud TPU V3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...</td>\n",
              "      <td>2020-04-29</td>\n",
              "      <td>Once for all: Train one network and specialize...</td>\n",
              "      <td>https://arxiv.org/abs/1908.09791</td>\n",
              "      <td>N/A</td>\n",
              "      <td>V100 (for training the full network)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...</td>\n",
              "      <td>2020-10-22</td>\n",
              "      <td>An Image is Worth 16x16 Words: Transformers fo...</td>\n",
              "      <td>https://arxiv.org/abs/2010.11929</td>\n",
              "      <td>)</td>\n",
              "      <td>TPUv3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...</td>\n",
              "      <td>2018-02-05</td>\n",
              "      <td>IMPALA: Scalable Distributed Deep-RL with Impo...</td>\n",
              "      <td>https://arxiv.org/abs/1802.01561</td>\n",
              "      <td>1 GPU</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>D Silver, T Hubert, J Schrittwieser, I Antonoglou</td>\n",
              "      <td>2017-12-05</td>\n",
              "      <td>Mastering Chess and Shogi by Self-Play with a ...</td>\n",
              "      <td>https://arxiv.org/abs/1712.01815</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...</td>\n",
              "      <td>2017-08-04</td>\n",
              "      <td>Revisiting Unreasonable Effectiveness of Data ...</td>\n",
              "      <td>https://arxiv.org/abs/1707.02968</td>\n",
              "      <td>50</td>\n",
              "      <td>K80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>N Shazeer, A Mirhoseini, K Maziarz, A Davis</td>\n",
              "      <td>2017-01-23</td>\n",
              "      <td>Outrageously Large Neural Networks: The Sparse...</td>\n",
              "      <td>https://arxiv.org/abs/1701.06538</td>\n",
              "      <td>N/A</td>\n",
              "      <td>K40, Tesla</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>C Liu, B Zoph, M Neumann, J Shlens</td>\n",
              "      <td>2017-12-02</td>\n",
              "      <td>Progressive Neural Architecture Search</td>\n",
              "      <td>https://arxiv.org/abs/1712.00559</td>\n",
              "      <td>256</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...</td>\n",
              "      <td>2017-06-12</td>\n",
              "      <td>Attention Is All You Need</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2017/file...</td>\n",
              "      <td>8 GPUs</td>\n",
              "      <td>P100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>Matej Moravčík, Martin Schmid, Neil Burch, Vil...</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>DeepStack: Expert-Level Artificial Intelligenc...</td>\n",
              "      <td>https://arxiv.org/abs/1701.01724</td>\n",
              "      <td>1</td>\n",
              "      <td>NVIDIA GeForce GTX 1080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Author(s) Publication date  \\\n",
              "3    Aitor Lewkowycz, Anders Andreassen, David Doha...       2022-06-29   \n",
              "4    Aakanksha Chowdhery, Sharan Narang, Jacob Devl...       2022-04-04   \n",
              "6    Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...       2022-03-29   \n",
              "7    Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...       2022-06-22   \n",
              "8    Romal Thoppilan, Daniel De Freitas, Jamie Hall...       2022-02-10   \n",
              "21   Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...       2022-04-14   \n",
              "100  Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...       2020-05-28   \n",
              "103  Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...       2020-06-30   \n",
              "108  Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...       2020-02-09   \n",
              "111  Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...       2020-04-29   \n",
              "132  Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...       2020-10-22   \n",
              "185  Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...       2018-02-05   \n",
              "208  D Silver, T Hubert, J Schrittwieser, I Antonoglou       2017-12-05   \n",
              "211  ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...       2017-08-04   \n",
              "212        N Shazeer, A Mirhoseini, K Maziarz, A Davis       2017-01-23   \n",
              "213                 C Liu, B Zoph, M Neumann, J Shlens       2017-12-02   \n",
              "214  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...       2017-06-12   \n",
              "215  Matej Moravčík, Martin Schmid, Neil Burch, Vil...       2017-01-06   \n",
              "\n",
              "                                             Reference  \\\n",
              "3    Solving Quantitative Reasoning Problems with L...   \n",
              "4        PaLM: Scaling Language Modeling with Pathways   \n",
              "6       Training Compute-Optimal Large Language Models   \n",
              "7    Scaling Autoregressive Models for Content-Rich...   \n",
              "8       LaMDA: Language Models for Dialog Applications   \n",
              "21     Efficient Language Modeling with Sparse all-MLP   \n",
              "100             Language models are Few- Shot Learners   \n",
              "103  GShard: Scaling Giant Models with Conditional ...   \n",
              "108  ALBERT: A Lite BERT for Self-supervised Learni...   \n",
              "111  Once for all: Train one network and specialize...   \n",
              "132  An Image is Worth 16x16 Words: Transformers fo...   \n",
              "185  IMPALA: Scalable Distributed Deep-RL with Impo...   \n",
              "208  Mastering Chess and Shogi by Self-Play with a ...   \n",
              "211  Revisiting Unreasonable Effectiveness of Data ...   \n",
              "212  Outrageously Large Neural Networks: The Sparse...   \n",
              "213             Progressive Neural Architecture Search   \n",
              "214                          Attention Is All You Need   \n",
              "215  DeepStack: Expert-Level Artificial Intelligenc...   \n",
              "\n",
              "                                                  Link  \\\n",
              "3                     https://arxiv.org/abs/2206.14858   \n",
              "4                     https://arxiv.org/abs/2204.02311   \n",
              "6                     https://arxiv.org/abs/2203.15556   \n",
              "7                   https://arxiv.org/abs/2206.10789v1   \n",
              "8                     https://arxiv.org/abs/2201.08239   \n",
              "21                    https://arxiv.org/abs/2203.06850   \n",
              "100                   https://arxiv.org/abs/2005.14165   \n",
              "103                   https://arxiv.org/abs/2006.16668   \n",
              "108                   https://arxiv.org/abs/1909.11942   \n",
              "111                   https://arxiv.org/abs/1908.09791   \n",
              "132                   https://arxiv.org/abs/2010.11929   \n",
              "185                   https://arxiv.org/abs/1802.01561   \n",
              "208                   https://arxiv.org/abs/1712.01815   \n",
              "211                   https://arxiv.org/abs/1707.02968   \n",
              "212                   https://arxiv.org/abs/1701.06538   \n",
              "213                   https://arxiv.org/abs/1712.00559   \n",
              "214  https://proceedings.neurips.cc/paper/2017/file...   \n",
              "215                   https://arxiv.org/abs/1701.01724   \n",
              "\n",
              "    Number of hardware units                        Hardware model  \n",
              "3                        N/A                                   N/A  \n",
              "4                       6144                                TPU v4  \n",
              "6                          )                           TPUv3/TPUv4  \n",
              "7                        N/A                             ViT-VQGAN  \n",
              "8                       1024                                TPU-v3  \n",
              "21                        32                                  V100  \n",
              "100                        )       V100 (used to train all models)  \n",
              "103                     2048                                 TPUv3  \n",
              "108                      N/A                          Cloud TPU V3  \n",
              "111                      N/A  V100 (for training the full network)  \n",
              "132                        )                                 TPUv3  \n",
              "185                    1 GPU                                   N/A  \n",
              "208                     none                                  none  \n",
              "211                       50                                   K80  \n",
              "212                      N/A                            K40, Tesla  \n",
              "213                      256                                   N/A  \n",
              "214                   8 GPUs                                  P100  \n",
              "215                        1               NVIDIA GeForce GTX 1080  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of hardware units\n",
            "N/A != 1024\n",
            ") != N/A\n",
            ") != N/A\n",
            "2048 != 1024\n",
            "N/A != 512\n",
            "N/A != 32\n",
            ") != N/A\n",
            "1 GPU != 1\n",
            "none != 64\n",
            "N/A != 64\n",
            "256 != 100\n",
            "8 GPUs != 8\n",
            "1 != 20\n",
            "Hardware model\n",
            "N/A != TPUv4\n",
            "TPU v4 != TPUv4\n",
            "TPUv3/TPUv4 != TPUv3, TPUv4\n",
            "ViT-VQGAN != TPUv4\n",
            "TPU-v3 != TPUv3\n",
            "V100 (used to train all models) != V100\n",
            "Cloud TPU V3 != TPUv3\n",
            "V100 (for training the full network) != V100\n",
            "N/A != P100\n",
            "none != TPUv2\n",
            "K40, Tesla != K40\n",
            "N/A != P100\n",
            "NVIDIA GeForce GTX 1080 != N/A\n",
            "Number of hardware units: 5/18\n",
            "Hardware model: 5/18\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Number of hardware units': 5, 'Hardware model': 5}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_answers(df, keys, CORRECT_ANSWERS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Count tokens in papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-4 Technical Report: 84644 tokens for davinci, 75304 tokens for gpt-4\n",
            "Phenaki: Variable Length Video Generation From Open Domain Textual Description: 15106 tokens for davinci, 14130 tokens for gpt-4\n",
            "Solving Quantitative Reasoning Problems with Language Models: 41815 tokens for davinci, 38377 tokens for gpt-4\n",
            "PaLM: Scaling Language Modeling with Pathways: 87181 tokens for davinci, 82875 tokens for gpt-4\n",
            "Training Compute-Optimal Large Language Models: 31998 tokens for davinci, 30151 tokens for gpt-4\n",
            "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation: 42391 tokens for davinci, 39185 tokens for gpt-4\n",
            "LaMDA: Language Models for Dialog Applications: 42530 tokens for davinci, 38884 tokens for gpt-4\n",
            "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model: 32768 tokens for davinci, 31059 tokens for gpt-4\n",
            "High-Resolution Image Synthesis with Latent Diffusion Models: 34114 tokens for davinci, 31745 tokens for gpt-4\n",
            "Robust Speech Recognition via Large-Scale Weak Supervision: 41375 tokens for davinci, 38852 tokens for gpt-4\n",
            "Total tokens: 453922 for davinci, 420562 for gpt-4\n"
          ]
        }
      ],
      "source": [
        "total_davinci_tokens = 0\n",
        "total_gpt_tokens = 0\n",
        "davinci_tokenizer = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
        "gpt4_tokenizer = tiktoken.encoding_for_model(\"gpt-4\")  # same for gpt-3.5-turbo\n",
        "for i, row in df.iterrows():\n",
        "    url = row['Link']\n",
        "    # replace \"abs\" with \"pdf\" in arxiv url links\n",
        "    url = url.replace('abs', 'pdf')\n",
        "    paper_title = row['Reference'].replace(' ', '_').replace(':', '').replace('\"', '').lower()\n",
        "    with open('input_data/' + paper_title + '.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    davinci_tokens = davinci_tokenizer.encode(text, disallowed_special=())\n",
        "    gpt_tokens = gpt4_tokenizer.encode(text, disallowed_special=())\n",
        "\n",
        "    total_davinci_tokens += len(davinci_tokens)\n",
        "    total_gpt_tokens += len(gpt_tokens)\n",
        "\n",
        "    print(f\"{row['Reference']}: {len(davinci_tokens)} tokens for davinci, {len(gpt_tokens)} tokens for gpt-4\")\n",
        "\n",
        "print(f\"Total tokens: {total_davinci_tokens} for davinci, {total_gpt_tokens} for gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "185"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(gpt4_tokenizer.encode(chat_message_template))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b7c6d073edd95e87cfa6524fac94a63a54fb86c88ec13e975ab85409bf024d2f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
