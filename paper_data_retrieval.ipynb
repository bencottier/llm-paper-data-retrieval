{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEPHl7N7YJWn"
      },
      "source": [
        "# TODO\n",
        "If we're already at it, let's also look for keywords, such as:\n",
        "- related to GPU/TPU Hardware\n",
        "- training time\n",
        "- inference compute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ee3887XnrDa",
        "outputId": "5cde1173-289a-44e4-d6a1-c911b058c4ff"
      },
      "source": [
        "!pip install pdfminer.six"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 3.7 MB/s \n",
            "\u001b[?25hCollecting cryptography\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 30.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-36.0.1 pdfminer.six-20211012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkwpFPzmWyG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057026f6-0b56-49cd-a84e-2ba4773b4eae"
      },
      "source": [
        "# Download dataset from the Parameters, Compute and Data Trends in ML sheet\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/export?format=csv#gid=0')\n",
        "\n",
        "year_start = 2017\n",
        "\n",
        "# Recode columns\n",
        "df['Publication date'] = pd.to_datetime(df['Publication date'], \n",
        "                                        errors='coerce', \n",
        "                                        dayfirst=True)\n",
        "\n",
        "# Filter for papers of only the last 5 years\n",
        "df = df[df['Publication date'] > f'{year_start}-01-01']\n",
        "\n",
        "\n",
        "# Keep only bibliographical data\n",
        "df = df.filter(['Author(s)', 'Publication date', 'Reference', 'Link'])\n",
        "df = df[df['Link'].notna()]\n",
        "# Keep only links which forward to a pdf or an arxiv link\n",
        "df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n",
        "\n",
        "\n",
        "import requests\n",
        "from pdfminer.high_level import extract_text\n",
        "import re\n",
        "\n",
        "\n",
        "# Set patterns and keys\n",
        "patterns = [r\".*@.*\\..*\", \n",
        "            r\"[a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+\", \n",
        "            r\"[a-zA-Z0-9._-]+.at.[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+\",\n",
        "            r\"([^.]*?GPU[^.]*\\.)\",\n",
        "            r\"([^.]*?TPU[^.]*\\.)\",\n",
        "            r\"([^.]*?NVIDIA[^.]*\\.)\"\n",
        "            ]\n",
        "keys = ['email_1', 'email_2', 'email_3', 'GPU', 'TPU', 'NVIDIA']\n",
        "\n",
        "tuples = (patterns, keys)\n",
        "\n",
        "# Enable for test running with the first ten papers\n",
        "# df = df[:10]\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "\n",
        "  url = row['Link']\n",
        "\n",
        "  # replace \"abs\" with \"pdf\" in arxiv url links\n",
        "  url = url.replace('abs', 'pdf')\n",
        "  print(f\"Looking into {row['Reference']}\")\n",
        "\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "  except Exception as e:\n",
        "    print(f\"There's something wrong with downloading: {e}\")\n",
        "    continue\n",
        "\n",
        "  file = open(\"download.pdf\", \"wb\")\n",
        "  file.seek(0) # overwrite previous file\n",
        "  file.write(response.content)\n",
        "  file.close()\n",
        "\n",
        "  try:\n",
        "    text = extract_text('download.pdf')\n",
        "\n",
        "\n",
        "\n",
        "    for pattern, key in zip(*tuples):\n",
        "      matches = re.findall(pattern, text)\n",
        "      print(matches)\n",
        "\n",
        "      matches = ';'.join(matches)\n",
        "      df.loc[i,key]  = matches if matches else \"\"\n",
        "  except Exception as e:\n",
        "    print(f\"There's something wrong with extracting the text: {e}\")\n",
        "    continue\n",
        "\n",
        "\n",
        "    # print(\"There's something wrong with downloading the paper.\")\n",
        "\n",
        "  print(\"---\")\n",
        "\n",
        "  df.loc[i, 'email_subject'] = f\"Trends in Machine Learning - Report your data from {row['Reference']}\"\n",
        "  df.loc[i, 'email_body'] = f\"\"\"\n",
        "Dear all,\n",
        "\n",
        "We are writing to you about your paper: “{row['Reference']}”. We’ve enjoyed reading it and we would like to include information about it in our public dataset (https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0) of milestone systems in AI.\n",
        "\n",
        "Could you share some more information about the primary system developed in the paper with us? We’re looking for estimates of:\n",
        "(A) Parameter count\n",
        "(B) Training compute (any type of metric is fine: GPU days and the hardware, number of operations, number of FLOPs, etc.)\n",
        "    (B.1) The used number representation during the training (e.g., float16, float32, bloat16).\n",
        "(C) Inference compute (number of operations/FLOPs per forward pass)\n",
        "(D) The size of the training dataset\n",
        "\n",
        "This information will help us in our investigation of trends in parameters, compute, and data usage in Machine Learning. \n",
        "\n",
        "We understand that some of this information is already available in your paper - your answer will help us guarantee we didn’t misinterpret the results in the paper.\n",
        "\n",
        "Feel free to just answer this email or fill out this minimal form (https://forms.gle/kPs8xoPif2H56DCz5). We would be grateful for any kind of information.\n",
        "If you have any questions, feel free to get back to us.\n",
        "\n",
        "This is a joint project by Jaime Sevilla, Lennart Heim, and others.\n",
        "\n",
        "Best regards,\n",
        "Lennart Heim\n",
        "\n",
        "\n",
        "Stanford's Existential Risk Initiative (SERI)\n",
        "Center for International Security and Cooperation, Stanford University\n",
        "mail: \tlennart@heim.xyz\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(df)\n",
        "df.to_csv('emails.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  return func(self, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking into DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Mask R-CNN\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Thinking Fast and Slow with Deep Learning and Tree Search\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Attention Is All You Need\n",
            "['avaswani@google.com', 'noam@google.com', 'nikip@google.com', 'usz@google.com', 'llion@google.com', 'aidan@cs.toronto.edu', 'lukaszkaiser@google.com', 'illia.polosukhin@gmail.com']\n",
            "['avaswani@google.com', 'noam@google.com', 'nikip@google.com', 'usz@google.com', 'llion@google.com', 'aidan@cs.toronto.edu', 'lukaszkaiser@google.com', 'illia.polosukhin@gmail.com']\n",
            "[]\n",
            "['5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.', '\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.', '\\n\\n2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions.', '2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs.', '5 days on 8 P100 GPUs.', ' We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.', ' Neural GPUs learn algorithms.']\n",
            "[]\n",
            "['2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs.']\n",
            "---\n",
            "Looking into Hybrid Reward Architecture for Reinforcement Learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Noisy Networks for Exploration\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Learning Transferable Architectures for Scalable Image Recognition\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Focal loss for dense object detection\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Focal loss for dense object detection\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Neural Collaborative Filtering\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Squeeze-and-Excitation Networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Dynamic Routing Between Capsules\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Dynamic Routing Between Capsules\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Progressive Neural Architecture Search\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Universal Language Model Fine-tuning for Text Classification\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Regularized Evolution for Image Classifier Architecture Search\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Unsupervised Representation Learning by Predicting Image Rotations\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into YOLOv3: An Incremental Improvement\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Human-level performance in first-person multiplayer games with population-based deep reinforcement learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Large Scale GAN Training for High Fidelity Natural Image Synthesis\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Multi-Task Deep Neural Networks for Natural Language Understanding\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into The Hanabi Challenge: A New Frontier for AI Research\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into ProxylessNAS: Direct neural architecture search on target task and hardware\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Language Models are Unsupervised Multitask Learners\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Billion-scale semi-supervised learning for image classification\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Data-Efficient Image Recognition with Contrastive Predictive Coding\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Defending Against Neural Fake News\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into MnasNet: Platform-Aware Neural Architecture Search for Mobile\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into MnasNet: Platform-Aware Neural Architecture Search for Mobile\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Deep Learning Recommendation Model for Personalization and Recommendation Systems\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Cross-lingual Language Model Pretraining\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into XLNet: Generalized Autoregressive Pretraining for Language Understanding\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Learning Representations by Maximizing Mutual Information Across Views\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Fixing the train-test resolution discrepancy\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Large Scale Adversarial Representation Learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models\n",
            "['∗Equal contribution. Website https://objectnet.dev. Corresponding author abarbu@csail.mit.edu', 'Toyota Research Institute, and the SystemsThatLearn@CSAIL initiative. We would like to thank the']\n",
            "['abarbu@csail.mit.edu']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Grandmaster level in StarCraft II using multi-agent reinforcement learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Momentum Contrast for Unsupervised Visual Representation Learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Mastering Atari Go Chess and Shogi by Planning with a Learned Model\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into StarGAN v2: Diverse Image Synthesis for Multiple Domains\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Dota 2 with Large Scale Deep Reinforcement Learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Dota 2 with Large Scale Deep Reinforcement Learning\n",
            "[]\n",
            "[]\n",
            "['citation.cfm', 'citation.cfm', 'x at 0.01']\n",
            "['\\n\\nThe key ingredient in solving this complex environment was to scale existing reinforcement\\nlearning systems to unprecedented levels, utilizing thousands of GPUs over multiple months.', ' A central pool of optimizer GPUs receives game\\ndata and stores it asynchronously in local buﬀers called experience buﬀers.', ' Each optimizer GPU\\ncomputes gradients using minibatches sampled randomly from its experience buﬀer.', ' In this way the eﬀective batch size is the batch size on each GPU (120 samples, each\\nwith 16 timesteps) multiplied by the number of GPUs (up to 1536 at the peak), for a total batch\\nsize of 2,949,120 time steps (each with ﬁve hero policy replicas).', ' The rollout machines\\nrun the game engine but not the policy; they communicate with a separate pool of GPU machines\\nwhich run forward passes in larger batches of approximately 60.', ' They communicate in a tight loop with Forward Pass GPUs,\\nwhich sample actions from the policy given the current observation.', ' Rollouts send their data to\\nOptimizer GPUs, which perform gradient updates.', ' The Optimizers publish the parameter versions\\nto storage in the Controller, and the Forward Pass GPUs occasionally pull the latest parameter\\nversion.', ' We use ops from the blocksparse library for fast GPU training[22].', '\\nIncreasing the batch size in our case means two things: ﬁrst, using twice as many optimizer GPUs\\nto optimize over the larger batch, and second, using twice as many rollout machines and forward\\npass GPUs to produce twice as many samples to feed the increased optimizer pool.', '\\nIf this scaling property\\nholds, it is possible to use the same total amount of GPU-days (and thus dollars) to reach a given\\nresult[28].', ' GPU Kernels for Block-Sparse Weights 2017.', ' We estimate the compute per step per\\nGPU using TensorFlow’s tf.', ' It does not count non-GPU compute on the optimizer\\nmachines such as exporting parameter versions to the rollouts.', '\\n\\nThis computation concludes that OpenAI Five used 770±50 PFlops/s·days of total optimization\\ncompute on GPUs at the time of playing the world champions (April 13, 2019), and 820±50 total\\noptimization compute when it was ﬁnally turned oﬀ on April 22nd, 2019.', ' In addition to the GPU\\nmachines doing optimization (roughly 30% of the cost by dollars spent) there are approximately\\nthe same number of GPUs running forward passes for the rollout workers (30%), as well as the\\nactual rollouts CPUs running the selfplay games (30%) and the overhead of controllers, TrueSkill\\nevaluators, CPUs on the GPU machines, etc (10%).', '\\nBecause the rollout GPUs will be running the newest code, all of these past versions must be updated\\nin the same way as the current version to ensure compatibility.', '01\\n\\nParam\\nFrameskipe\\nLSTM Unroll lengthe\\nSamples Per Segmente\\nNumber of optimizer GPUs\\nBatch Size/optimizer GPU (samples)\\nTotal Batch Size (samples)a\\nTotal Batch Size (timesteps)a\\nNumber of rollout GPUs\\nNumber of rollout CPUs\\nSteps per Iteration\\nLSTM Size\\nSample Reuse\\nTeam Spirit\\nGAE Horizon\\nGAE λ\\nPPO clipping\\nValue loss weightc\\nEntropy coeﬃcient\\nLearning rate\\nAdam β1\\nAdam β2\\nPast opponentsb\\nPast Opponents Learning Rated\\na Batch size can be measured in samples (each an unrolled LSTM of 16 frames) or in individual\\ntimesteps.', '\\nBecause we average gradients across the pool of optimizer machines, the eﬀective total batch\\nsize is given by the product of the number of GPU optimizers with the batch size on each optimizer.', '\\nWe always use the maximum batch size on each optimizer which will ﬁt within the GPU’s memory\\nconstraints (120 for our setup).', ' Thus in order to change the overall batch size we increase the number\\nof optimizer GPUs.', ' We increase the size of the other machine pools in the experiment (rollout CPU\\nworkers, forward pass GPUs, etc), such that the larger batch size experiment is truly optimizing\\nover more data, not simply reusing the same data more.', '\\nworker stays relatively stable, so we vary this rate by changing the number of rollout CPU workers\\nand forward pass GPUs while keeping the number of optimizers and everything else ﬁxed.', ' CPUs are often easier and cheaper to scale up than GPUs and this can be a\\nsigniﬁcant performance boost in some setups.', ' Of course the magnitude of improvement\\nis relatively small and the cost (doubling the number of rollout workers and forward pass GPUs) is\\nsigniﬁcant.']\n",
            "[]\n",
            "[' NVIDIA.', ' NVIDIA Collective Communications Library (NCCL) https : / / developer .']\n",
            "---\n",
            "Looking into Large scale learning of general visual representations for transfer\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into BERT-of-Theseus: Compressing BERT by Progressive Module Replacing\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into A Simple Framework for Contrastive Learning of Visual Representations\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Electra: pre-training text encoders as discriminators rather than generators\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into MetNet: A Neural Weather Model for Precipitation Forecasting\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Agent57: Outperforming the Atari Human Benchmark\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into CURL: Contrastive Unsupervised Representations for Reinforcement Learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into First return, then explore\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Language Models are Few-Shot Learners\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Once for all: Train one network and specialize it for efficient deployment.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Hopfield Networks is All You Need\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into CPM: A Large-scale Generative Chinese Pre-trained Language Model\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into AraGPT2: Pre-Trained Transformer for Arabic Language Generation\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Primer: Searching for Efficient Transformers for Language Modeling\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Learning Transferable Visual Models From Natural Language Supervision\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Learning Transferable Visual Models From Natural Language Supervision\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Finetuned Language Models Are Zero-Shot Learners\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Recurrent Rational Networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into M6: A Chinese Multimodal Pretrainer\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into M6: A Chinese Multimodal Pretrainer\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Recipes for building an open-domain chatbot\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into M6-T: Exploring Sparse Expert Models and Beyond\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Efficient Large-Scale Language Model Training on GPU Clusters\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into CogView: Mastering Text-to-Image Generation via Transformers\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Scaling Vision Transformers\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Denoising Diffusion Probabilistic Models\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Self-supervised Pretraining of Visual Features in the Wild\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Perceiver IO: A General Architecture for Structured Inputs & Outputs\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Jurassic-1: Technical Details and Evaluation\n",
            "['opherl@ai21.com', 'ors@ai21.com', 'barakl@ai21.com', 'yoavs@ai21.com', 'interested in conducting research on or otherwise promoting AI ethics and safety to contact us at safety@ai21.com and']\n",
            "['opherl@ai21.com', 'ors@ai21.com', 'barakl@ai21.com', 'yoavs@ai21.com', 'safety@ai21.com']\n",
            "[]\n",
            "['\\n\\n1Training such a large model, on over 800 GPUs over many months, is a non-trivial engineering feat, and raises many issues not\\npresent in smaller models: Overﬂows, null attention heads, model and data parallelism that require solutions on top of packages such\\nas DeepSpeed (Rasley et al.', ' For example, the hidden\\ndimension is constrained by the number of heads and their size, while dhead should be a multiple of 8 for optimal\\nmatrix-multiplication operations on GPUs and nheads should be divisible by the model parallelization factor.', ' This is especially relevant to text generation where tokens are\\nprocessed one at a time, and so there is less opportunity for parallelization, resulting in sub-optimal GPU utilization.', ' Simply storing 178B\\nparameters requires more than 356GB of memory in half-precision, whereas even the largest GPUs available today have\\na maximum memory of 80GB, and this is before taking into account the optimizer’s state or the intermediate calculations\\nused by backward simulations.', ' Therefore, training must be distributed across tens or hundreds of nodes, each with\\nmultiple GPUs, which presents its own set of challenges, e.']\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Larger-Scale Transformers for Multilingual Masked Language Modeling\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Fine-tuned Language Models Are Zero-Shot Learners\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Multitask Prompted Training Enables Zero-Shot Task Generalization\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Mastering Atari Games with Limited Data\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Report on the 11th iwslt evaluation campaign, iwslt 2014\n",
            "['(EAMT),Trento,Italy,May2012.[Online].Available:http://hltshare.fbk.eu/EAMT2012/html/Papers/59.pdf[12]O.Bojar,C.Buck,C.Federmann,B.Haddow,P.Koehn,J.Leveling,C.Monz,P.Pecina,M.Post,H.Saint-Amand,R.Soricut,L.Specia,andA.Tam-chyna,“Findingsofthe2014WorkshoponStatisti-calMachineTranslation,”inProceedingsoftheNinthWorkshoponStatisticalMachineTranslation,Balti-more,MD,USA,2014.[13]M.Freitag,J.Wuebker,S.Peitz,H.Ney,M.Huck,A.Birch,N.Durrani,P.Koehn,M.Mediani,I.Slawik,J.Niehues,E.Cho,A.Waibel,N.Bertoldi,M.Cettolo,andM.Federico,“CombinedSpokenLanguageTrans-lation,”inProceedingsofthe11thInternationalWork-shoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[14]B.Babaali,R.Serizel,S.Jalalvand,D.Falavigna,R.Gretter,andD.Giuliani,“FBK@IWSLT2014-ASRtrack,”inProceedingsofthe11thInternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[15]N.Bertoldi,P.Mathur,N.Ruiz,andM.Federico,“FBK’sMachineTranslationandSpeechTranslationSystemsfortheIWSLT2014EvaluationCampaign,”inProceedingsofthe11thInternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[16]M.Beloucif,C.-K.Lo,andD.Wu,“Improvingtun-ingagainstMEANT,”inProceedingsofthe11thIn-ternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[17]Q.B.Nguyen,T.T.Vu,andC.M.Luong,“TheSpeechRecognitionSystemsofIOITforIWSLT2014,”inPro-ceedingsofthe11thInternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[18]K.Kilgour,M.Heck,M.M¨uller,M.Sperber,S.St¨uker,andA.Waibel,“The2014KITIWSLTSpeech-to-TextSystemsforEnglish,GermanandItalian,”inProceed-ingsofthe11thInternationalWorkshoponSpokenLan-guageTranslation(IWSLT),LakeTahoe,CA,2014.[19]I.Slawik,M.Mediani,J.Niehues,Y.Zhang,E.Cho,T.Herrmann,T.-L.Ha,andA.Waibel,“TheKITTrans-lationSystemsforIWSLT2014,”inProceedingsofthe11thInternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[20]M.Morchid,S.Huet,andR.Dufour,“ATopic-basedApproachforPost-processingCorrectionofAutomaticTranslations,”inProceedingsofthe11thInternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[21]N.Segal,H.Bonneau-Maynard,Q.K.Do,A.Allauzen,J.-L.Gauvain,L.Lamel,andF.Yvon,“LIMSIEnglish-FrenchSpeechTranslationSystem,”inProceedingsofthe11thInternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[22]A.Rousseau,L.Barrault,P.Del´eglise,Y.Est`eve,H.Schwenk,S.Bennacef,A.Muscariello,andS.Vanni,“TheLIUMEnglish-to-FrenchSpokenLan-guageTranslationSystemandtheVecsys/LIUMAu-tomaticSpeechRecognitionSystemforItalianLan-guageforIWSLT2014,”inProceedingsofthe11thIn-ternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[23]A.B.Romdhane,S.Jamoussi,A.B.Hamadou,andK.Smaili,“Phrase-basedLanguageModellingforSta-tisticalMachineTranslation,”inProceedingsofthe11thInternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[24]P.Shen,X.Lu,X.Hu,N.Kanda,M.Saiko,andC.Hori,“TheNICTASRSystemforIWSLT2014,”inProceed-ingsofthe11thInternationalWorkshoponSpokenLan-guageTranslation(IWSLT),LakeTahoe,CA,2014.[25]X.Wang,A.Finch,M.Utiyama,T.Watanabe,andE.Sumita,“TheNICTTranslationSystemforIWSLT2014,”inProceedingsofthe11thInternationalWork-shoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[26]K.Sudoh,G.Neubig,K.Duh,andK.Hayashi,“NTT-NAISTSyntax-basedSMTSystemsforIWSLT2014,”inProceedingsofthe11thInternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[27]K.WolkandK.Marasek,“Polish-EnglishSpeechStatisticalMachineTranslationSystemsfortheIWSLT2014,”inProceedingsofthe11thInternationalWork-shoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[28]J.Wuebker,S.Peitz,A.Guta,andH.Ney,“TheRWTHAachenMachineTranslationSystemsforIWSLT2014,”inProceedingsofthe11thInternationalWork-shoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[29]P.Bell,P.Swietojanski,J.Driesen,M.Sinclair,F.McInnes,andS.Renals,“TheUEDINASRSystemsfortheIWSLT2014Evaluation,”inProceedingsofthe11thInternationalWorkshoponSpokenLanguageTranslation(IWSLT),LakeTahoe,CA,2014.[30]A.Birch,M.Huck,N.Durrani,N.Bogoychev,andP.Koehn,“EdinburghSLTandMTSystemDescrip-tionfortheIWSLT2014Evaluation,”inProceedingsof\\x0c11']\n",
            "[]\n",
            "['GermanyAbstractThepaperoverviewsthe11thevaluationcampaignorganizedbytheIWSLTworkshop.The2014evaluationofferedmul-tipletracksonlecturetranscriptionandtranslationbasedontheTEDTalkscorpus.Inparticular', 'foratotalof76primaryrunssubmitted.Participantswerealsoaskedtosubmitrunsonthe2013testset', 'cialtexttranslationtrackswerealsoevaluatedwithhumanpost-editing.1.IntroductionThispaperoverviewstheresultsofthe2014evaluationcam-paignorganizedbytheInternationalWorkshopofSpokenLanguageTranslation.TheIWSLTevaluationhasbeenrun-ningnowforoveradecadeandhasofferedalongtheseyearsavarietyofspeechtranslationtasks', 'i.e.thetranslationofapol-ishedtranscriptintoanotherlanguage.However', 'andItalian-Englishtranslationdirections.Besidestheof', 'manyotheroptionaltranslationdirectionswerealsooffered.OptionalSLTdirectionswereEnglish-ArabicandEnglish-Chinese.OptionalMTtranslationdi-rectionswere', 'thatincludesbothfreelyavailablecorporaandcorporaavailablefromLDC.Testdatawerereleasedatthebeginningofeachtestperiod', 'MTandSLTtestperiodofalloptionaldirections.Allrunssubmittedbyparticipantswereevaluatedwithautomaticmetrics.Inaddition', 'systemswereevaluatedbycalculatingHTERvaluesonpost-editscreatedbyprofessionaltranslators.TherationalbehindthisevaluationistoassesstheutilityofanMToutputbymeasuringthepost-editingeffortneededbyaprofessionaltranslatorto', 'rsttimeatIWSLT2010.TEDisanonpro', '.Itswebsite1makesthevideorecordingsofthebestTEDtalksavailableundertheCreativeCommonslicense.AlltalkshaveEnglishcaptions', 'whichhavealsobeentranslatedintomanylanguagesbyvolunteersworldwide.Inadditiontotheof', 'whichsharethesameformatoftheoriginalTEDtalksbutareholdinthelanguageofthehostingcountry.Recently', 'readytousefortrainingandevaluatingMTsystems.Besidesrepresentingapopularbenchmarkforspokenlanguagetechnology', 'theTEDTalkstaskembedsinterestingresearchchallengeswhichareuniqueamongtheavailablespeechrecognitionandmachinetranslationbenchmarks.TEDTalksisacollectionofrathershortspeeches', '2aloweramountofrephrasingandre-orderingisexpectedthaninordinarytranslationofwrittendocuments.Fromanapplicationperspective', 'requiringatightintegrationofMTwithASRpossiblyhandlingstream-basedprocessing.1http', 'translations.ted.org', 'andtheratherinformalspeakingstyle.FortheTEDxtalkstherecordingconditionsarealittlebitmoredif', 'andrecordingisoftendonebyamateursresultinginoftenpoorerrecordingqualitythanfortheTEDlectures.Theresultoftherecognitionofthetalksisusedfortwopurposes.ItisusedtomeasuretheperformanceofASRsys-temsonthetalksanditisusedasinputforthespokenlan-guagetranslationevaluation', 'seeSection4.3.2.EvaluationParticipantshadtosubmittheresultsoftherecognitionofthetst2014setinCTMformat.Theworderrorratewasmeasuredcase-insensitive.Aftertheendoftheevaluationapreliminaryscoringwasperformedwiththe', 'rstsetofreferences.Thiswasfollowedbyanadjudicationphaseinwhichparticipantscouldpointouterrorsinthereferencetranscripts.Theadjudicationresultswerecollectedandcom-binedintothe', 'cialscoreswerecalculated.InordertomeasuretheprogressofthesystemsovertheyearsonEnglishandGerman', 'sevaluationwereceivedprimarysubmissionsfromeightsitesaswellasonecombinedsubmissionbytheEU-BRIDGEproject.SevensitesparticipatedintheEnglishevaluation', 'threesitesintheGermanevaluationandfoursitesintheItalianone.ForEnglishwefurtherreceivedatotalofsevencontrastivesubmissionsfrom', 'outputsandforwhichnosepa-ratesystemdescriptionwassubmitted.3.4.ResultsThedetailedresultsoftheprimarysubmissionsoftheeval-uationintermsofworderrorrate', 'canbefoundinAppendixA.1.Theworderrorrateofthesubmittedsystemsinintherangeof8.4', 'USTCNationalEngineeringLaboratoryofSpeechandLang.Inform.Proc', 'participantshadtotranslateintoEnglish.ForEnglishassourcelanguage', 'participantshadtotranslateintoFrench.Inaddition', 'participantshadtosegmenttheaudioaccordingtotheman-ualreferencesegmentationprovidedbytheorganizersoftheevaluation.ForEnglish', 'asROVERcombinationwithothersys-temsdidnotgiveanyperformancegains.Theresultsofthetranslationhadtobesubmittedinthesameformatasforthemachinetranslationtrack', 'EnglishtoFrenchreceivingthemostsub-missions.4.4.ResultsThedetailedresultsoftheautomaticevaluationintermsofBLEUandTERcanbefoundinAppendixA.1', 'ciallanguagepairsdatasetlangsenttokenvoctrainDe183k3.36M124.7kEn188k3.81M63.4kFr186k4.00M77.0kIt185k3.49M90.2k5.MTTrack5.1.De', 'nitionTheMTTEDtrackbasicallycorrespondstoasubtitlingtranslationtask.ThenaturaltranslationunitconsideredbythehumantranslatorsvolunteeringforTEDisindeedthesin-glecaption', 'buttofragmentsofitthatﬁtthecaptionspace.Whiletranslatorscanlookatthecontextofthesinglecaptions', 'thereforenoevaluationsetisavailableforassessingprogress.AsingleTEDxbaseddevelopmentsetwasreleasedforeachpair', '5-gramLMswithimprovedKneser-NeysmoothingwereestimatedonthetargetsideofthetrainingparalleldatawiththeIRSTLMtoolkit.Theweightsofthelog-linearinterpolationmodelwereoptimizedwiththeMERTprocedureprovidedwithMoses', 'wheredev2012wereem-ployed.5.2.EvaluationTheparticipantstotheMTtrackhadtoprovidetheresultsofthetranslationofthetestsetsinNISTXMLformat.Theout-puthadtobecase-sensitiveandhadtocontainpunctuation3QCRI-normalizerwasspeci', 'callydevelopedforIWSLTEvaluationCampaignsbyP.NakovandF.Al-ObaidliatQatarComputingResearchInstitute', '.Tokenizationscriptswereappliedautomaticallytoallrunsubmissionspriortoevaluation.EvaluationscoreswerecalculatedforthetwoautomaticstandardmetricsBLEUandTER', 'wewereaskedtoevaluatealso64contrastiveruns.Concerningtheoptionalpairs', 'forreferencepurposesTable4showsBLEUandTERscoresonthetst2014evaluationsetsofthebaselinesystemswedevelopedasdescribedinSection5.1.Theresultsontheof', 'cialevaluationmetrictorankthesys-tems.Post-Editing', 'haslongbeeninvestigatedbythetranslationindustryasaformofmachineassistancetoreducethecostsofhumantranslation.Nowadays', 'productivity.TheMTTEDtaskofferedinIWSLTcanbeseenasaninterestingapplicationscenariototesttheutilityofMTsystemsinarealsubtitlingtask.Fromthepointofviewoftheevaluationcampaign', 'bothintermsofinformationaboutMTsystemsanddataandre-sourcestobereused.Withrespecttoothertypesofhumanassessment', 'asetofadditionalref-erencetranslations.BoththesebyproductsareveryusefulforMTsystemdevelopmentandevaluation.Furthermore', '-whichconsistsofmeasuringtheminimumeditdistancebetweenthemachinetranslationanditsmanuallypost-editedversion-hasbeenshowntocorrelatequitewellwithhumanjudgmentsofMTquality.Thehumanevaluationsetupandthecollectionofpost-editingdataarepresentedinSection5.5.1', 'whereasthere-sultsoftheevaluationarepresentedinSection5.5.2.5.5.1.EvaluationSetupandDataCollectionThehumanevaluation', 'ofeachtalk.Thischoiceofselectingaconsecu-tiveblockofsentencesforeachtalkwasdeterminedbytheneedofrealisticallysimulatingacaptionpost-editingtaskonseveralTEDtalks.TheresultingHEsetsarecomposedof628segmentsforEnDeand622segmentsforEnFr', 'whereprofessionaltranslatorsarerequiredtopost-edittheMToutputdirectlyaccordingtothesourcesentence.Bilingualpost-editingisexpectedtogivemoreaccurateresultsthanmonolingualpost-editingaspost-editorsdonotdependonangiven-andpossiblyimprecise-translation.Then', 'HTERscoreswerecalculatedonthecreatedpost-edits.HTER', '.TERmeasurestheamountofeditingthatahumanwouldhavetoperformtochangeamachinetranslationsothatitexactlymatchesagivenreferencetranslation.HTERisavariantofTERwhereanewreferencetranslationisgeneratedbyapplyingthemin-imumnumberofpost-editstothegivenMToutput.Thisnewtargetedreferenceisthenusedastheonlyreferencetransla-tiontocalculatetheTERoftheMToutput.Aninterestingoutcomeoflastyear', 'wasthatthemostinformativeandreliableHTERwasnotobtainedbyusingonlythetargetedreferencebutbyex-ploitingallthepost-editsoftheevaluatedMToutputs.Ac-cordingtotheseresults', 'ciallyrankedaccordingtoHTERcalculatedonmultiplereferences.Asforthesystemstobeevaluated', 'eachMTsystemmustbeequallypost-editedbyalltranslators.Furthermore', 'sevaluation.Table5', 'Post-editinginformationforeachPost-editorPEditorPEEffortstd-devSysTERstd-devPE132.1718.8056.0520.23PE219.6913.5656.3220.34PE340.9117.2356.1819.58PE427.5614.7155.9320.02PE524.9915.6255.6319.88Table6', 'Post-editinginformationforeachPost-editorPEditorPEEffortstd-devSysTERstd-devPE134.9620.2142.6017.61PE217.4714.7642.8117.98PE323.6814.1743.0217.74PE439.6520.4742.2717.78PE519.7314.0742.8617.72devisedaschemethatdispatchesMToutputstotranslatorsbothrandomlyandsatisfyingtheuniformassignmentcon-straints.Foreachtask', 'andthepost-editingtaskswererunusinganenterprise-levelCATtooldevelopedundertheMateCatproject7.Boththepost-editinginterfaceandtheguidelinesgiventotranslatorsarepresentedinAppendixB.Foreachtask', 'venewreferencetranslationsforeachofthesentencesoftheHEset.Eachoneofthese', 'vereferencesrepresentsthetargetedtranslationofthesystemoutputfromwhichitwasderived.Fromthepointofviewofthesystemoutput', 'onetargetedtranslationandotherfourtranslationsareavailable.Themaincharacteristicsoftheworkcarriedoutbypost-editorsarepresentedinTable5fortheEnDetaskandinTable6fortheEnFrtask', 'thepost-editingeffortforeachtrans-latorisgiven.Post-editingeffortistobeinterpretedasthenumberofactualeditoperationsperformedtoproducethepost-editedversionand-consequently-itiscalculatedastheHTERofallthesystemsentencespost-editedbyeachsingletranslator.ItisinterestingtoseethatthePEeffortissimilarforbothlanguagepairs', 'vetranslatorsproducedquitedifferentpost-editingeffortdistributions.7www.matecat.com', 'aswellastherelatedstandarddeviation.Aswecanseefromthetables', 'rmsthattheprocedurefollowedindatapreparationwasef-fective.Thevariabilityobservedinpost-editingeffort-despitethesimilarityoftheinputdocuments-ismostprobablyduetotranslators', 'post-editorvariabilityisanissuetobeaddressedtoensureasoundevaluationofthesystems.5.5.2.EvaluationResultsAsanticipatedabove', 'shumanevaluationresultsdemonstratedthatHTERcomputedagainstallthereferencesproducedbyallpost-editorsallowedamorereliableandconsistentevaluationofMTsystemswithrespecttoHTERcalculatedagainstthetargetedreferenceonly.Indeed', 'ciallyrankedaccordingtoHTERcalculatedonmultiplereferences.FortheEnDetask', 'i.e.foreachsystemthetargetedtranslationandtheadditionalfourreferenceswereused.FortheEnFrtask', 'sevaluationitstargetedtranslation.Theof', 'arethoseobtainedonthecombinationofthefourpost-editswhichgavethebestresults.Ingeneral', 'theverylowHTERresultsobtainedinbothtasksdemonstratethattheoverallqualityofthesystemsisveryhigh.Moreover', 'cialhumanevaluationresultsSystemHTERTERTERRankingHESetHESetTestSet5PErefsrefrefEU-BRIDGE19.2254.5553.62UEDIN19.9356.3255.12KIT20.8854.8853.83NTT-NAIST21.3254.6853.86KLE28.7559.6758.27RankCorr.0.600.70Table8', 'cialhumanevaluationresultsSystemHTERHTERTERTERRankingHESetHESetHESetTestSet4PErefs5PErefsrefrefEU-BRIDGE19.21UEDIN16.4842.6443.27RWTH19.27UEDIN16.5541.8242.58KIT20.89MIRACL17.6442.3343.09UEDIN21.52MIRACL17.2343.2843.80MITLL-AFRL22.64MIRACL18.6943.4844.05FBK22.90MIRACL22.2944.2844.83MIRACL33.6132.9052.1951.96RankCorr.0.960.900.90hasbeenshown', 'indicatingthatsystemstranslatingintoFrenchperformbetterthansystemstranslatingintoGerman.Anumberofadditionalobservationscanbedrawnbycomparingtheof', '0.0ifnocorre-lationexists.WecanseefromthetablesthatTERrankingscorrelatewellwiththeof', 'whonaturallytendtodivergefromthepost-editingguidelinesandpersonalizetheirtranslations.Moreover', 'anumberofadditionalreferencetranslationswillbeavailableforfurtherdevelopmentandevaluationofMTsystems.6.ConclusionsWehavereportedontheevaluationcampaignorganizedfortheeleventheditionoftheIWSLTworkshop.Theevaluationhasaddressedthreetracks', 'andaprogresstestsetcreatedandusedforthe2013evaluation.Thisyear', 'whichwereallscoredwithautomaticmet-rics.WealsomanuallyevaluatedrunsoftheEnglish-GermanandEnglish-Frenchtexttranslationtracks.Inparticular', 'wecouldobtainmorereliablescoresbyusingalltheproducedpost-editsasreferencetranslations.ByusingtheHTERmetric', 'thispercentageofpost-editingseemstobeanotherstrongargumentsupportingtheutilityofmachinetranslationforhumantranslators.7.AcknowledgementsResearchGroup3-01', 'orSummarization.AnnArbor', 'withpunctuationsremovedA.1.Of', 'AppendixB.HumanEvaluationInterfaceusedforthebilingualpost-editingtaskPost-editinginstructionsgiventoprofessionaltranslatorsInthistaskyouarepresentedwithautomatictranslationsofTEDTalkscaptions.Youareaskedtopost-editthegivenautomatictranslationbyapplyingtheminimaleditsrequiredtotransformthesystemoutputintoa', 'rememberthatthepost-editedsentenceistobeintendedasatranscriptionofspokenlanguage.Notealsothatthefocusisthecorrectnessofthesinglesentencewithinthegivencontext', 'differentbutcorrecttranslationsoftermsacrosssegmentsshouldnotbecorrected.Examples', 'ThisnextonetakesalittleexplanationbeforeIshareitwithyou.Automatictranslation', 'Etlaformedelatableestimportante.Post-editing1']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Listen, attend and spell: A neural network for large vocabulary conversational speech recognition\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into BIG-LITTLE NET: AN EFFICIENT MULTI-SCALE FEATURE REPRESENTATION FOR VISUAL AND SPEECH RECOGNITION\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into A survey of model compression and acceleration for deep neural networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Mono3d+: Monocular 3d vehicle detection with two-scale 3d hypotheses and task priors\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Computer Architecture : A Quantitative Approach\n",
            "['(ca5bugs@mkp.com).', 'separate email address at ca5comments@mkp.com.']\n",
            "['ca5bugs@mkp.com', 'ca5comments@mkp.com']\n",
            "['website at www.mkp.com', 'register at textbooks.elsevier.com', 'found at www.spec.org', 'described at www.tpc.org', 'runs at 2.20', 'that ADD.D', 'SX-9 at 102.4', 'i7 at 42.66', 'faster at 40.5', 'nested at 12.5', 'run at 0.97', 'that 0.2', 'that 1.2', 'PerspectivesDataCenterCostAndPower.xls', 'running at 2.93', 'running at 2.2', 'that 0.230', 'running at 2.2', 'respond at 0.5', 'runs at 1.1', 'that MUL.D', 'runs at 28.8', 'transmitted at 869.04', 'transmitted at 824.04', 'messages at 1.25', 'that inr-111-cs2.Berkeley.edu', 'that route.What', '2 at 1.5', 'Edition at 3.8', 'Power5 at 1.9', 'clock at 1.9', 'seattlepi.nwsource.com', 'climatesaverscomputing.org', 'JamesHamilton_CleanSlateCTO2009.pdf', 'online at java.sun.com']\n",
            "['  A  highlight  of  the  new  edition  is  the significantly\\nrevised chapter on data-level parallelism, which demystifies GPU architectures\\nwith clear explanations using traditional computer architecture terminology.', '”\\n\\n—James Larus, Microsoft Research\\n\\n“This new edition adds a superb new chapter on data-level parallelism in vector,\\nSIMD, and GPU architectures.', ' It explains key architecture concepts inside mass-\\nmarket  GPUs,  maps  them  to  traditional  terms,  and  compares  them  with  vector\\nand  SIMD  architectures.', '  It’s  timely  and  relevant  with  the  widespread  shift  to\\nGPU parallel computing.', ' The chapter on data parallelism is particularly illuminating: the comparison\\nand  contrast  between  Vector  SIMD,  instruction  level  SIMD,  and  GPU  cuts\\nthrough the jargon associated with each architecture and exposes the similarities\\nand differences between these architectures.', ' Colwell\\n\\nIntroduction\\nVector Architecture\\nSIMD Instruction Set Extensions for Multimedia\\n\\nData-Level Parallelism in Vector, SIMD, and GPU Architectures \\n4.', '7\\n\\nCrosscutting Issues\\nPutting It All Together: Mobile versus Server GPUs\\nand Tesla versus Core i7\\nFallacies and Pitfalls\\nConcluding Remarks\\n\\n4.', ' Like the\\nfirst edition, this edition has a sharp focus on new platforms—personal mobile\\ndevices and warehouse-scale computers—and new architectures—multicore and\\nGPUs.', ' We then explain the four\\narchitectural styles that exploit DLP and TLP: instruction-level parallelism (ILP)\\nin Chapter 3; vector architectures and graphic processor units (GPUs) in Chapter\\n4,  which  is  a  brand-new  chapter  for  this  edition;  thread-level  parallelism  in\\nChapter 5; and request-level parallelism (RLP) via warehouse-scale computers in\\nChapter 6, which is also a brand-new chapter for this edition.', ' We are particularly proud about Chapter 4, which con-\\ntains  the  most  detailed  and  clearest  explanation  of  GPUs  yet,  and  Chapter  6,\\nwhich is the first publication of the most recent details of a Google Warehouse-\\nscale computer.', '  The\\n“Putting It All Together” sections of this edition include the pipeline organiza-\\ntions and memory hierarchies of the ARM Cortex A8 processor, the Intel core i7\\nprocessor,  the  NVIDIA GTX-280 and GTX-480 GPUs, and one of the Google\\nwarehouse-scale computers.', ' In particular, there is a rising interest in computing using graphi-\\ncal processing units (GPUs), yet few architects understand how GPUs really work.', '  Chapter  4  starts  with  an  introduction  to  vector  architectures,\\nwhich acts as a foundation on which to build explanations of multimedia SIMD\\ninstrution set extensions and GPUs.', ') The section on GPUs was the most difficult to write in this\\nbook, in that it took many iterations to get an accurate description that was also\\neasy to understand.', ') This chapter introduces the Roofline performance model and then uses it\\nto compare the Intel Core i7 and the NVIDIA GTX 280 and GTX 480 GPUs.', ' The\\nchapter also describes the Tegra 2 GPU for PMDs.', '  Ziavras,  New  Jersey\\nInstitute of Technology\\n\\nMembers of the University of California–Berkeley Par Lab and RAD Lab who\\ngave  frequent  reviews  of  Chapter  1,  4,  and  6  and  shaped  the  explanation  of\\nGPUs and WSCs: Krste  Asanovic´,  Michael Armbrust, Scott Beamer, Sarah Bird,\\nBryan Catanzaro, Jike Chong, Henry Cook, Derrick Coetzee, Randy Katz, Yun-\\nsup Lee, Leo Meyervich, Mark Murphy, Zhangxi Tan, Vasily Volkov, and Andrew\\nWaterman\\n\\nAdvisory Panel\\n\\nLuiz  André  Barroso,  Google  Inc.', ' \\n\\nAdditional Material\\n\\nJohn  Nickolls,  Steve  Keckler,  and  Michael  Toksvig  of  NVIDIA  (Chapter  4\\nNVIDIA GPUs); Victor Lee, Intel (Chapter 4 comparison of Core i7 and GPU);\\nJohn Shalf, LBNL (Chapter 4 recent vector architectures); Sam Williams, LBNL\\n(Roofline  model  for  computers  in  Chapter  4);  Steve  Blackburn  of  Australian\\nNational  University  and  Kathryn  McKinley  of  University  of  Texas  at  Austin\\n(Intel  performance  and  power  measurements  in  Chapter  5);  Luiz  Barroso,  Urs\\nHölzle,  Jimmy  Clidaris,  Bob  Felderman,  and  Chris  Johnson  of  Google  (the\\nGoogle WSC in Chapter 6); James  Hamilton of Amazon Web Services (power\\ndistribution and cost model in Chapter 6)\\n\\nJason  D.', ' Vector Architectures and Graphic Processor Units (GPUs) exploit data-level\\nparallelism by applying a single instruction to a collection of data in parallel.', '\\nChapter  4  covers  DLP  and  three  different  architectures  that  exploit  it:\\nvector  architectures,  multimedia  extensions  to  standard  instruction  sets,\\nand GPUs.', ' The third piece is an in-\\ndepth explanation of how modern graphics processing units (GPUs) work.', ' Most\\nGPU descriptions are written from the programmer’s perspective, which usually\\nhides  how  the  computer  really  works.', '  This  section  explains  GPUs  from  an\\ninsider’s perspective, including a mapping between GPU jargon and more tradi-\\ntional architecture terms.', '  Since  Graphics  Processor  Units\\n(GPUs;  see  Chapter  4)  require  more  bandwidth  per  DRAM  chip  than  CPUs,\\nGDDRs have several important differences:\\n\\n1.', '  To  allow  a\\nhigher  transfer  rate  without  incurring  signaling  problems,  GDRAMS\\nnormally connect directly to the GPU and are attached by soldering them to\\nthe  board,  unlike  DRAMs,  which  are  normally  arranged  in  an  expandable\\narray of DIMMs.', '\\n\\nAltogether,  these  characteristics  let  GDDRs  run  at  two  to  five  times  the  band-\\nwidth per DRAM versus DDR3 DRAMs, a significant advantage in supporting\\nGPUs.', ' Because of the lower locality of memory requests in a GPU, burst mode\\ngenerally is less useful for a GPU, but keeping open multiple memory banks and\\nmanaging their use improves effective bandwidth.', ' \\n\\nOne idea that periodically arises is the use of programmer-controlled scratch-\\npad or other  high-speed  memories, which we will see  are  used in GPUs.', '  In\\nGPUs  (see  Chapter  4),  where  local  scratchpad  memories  are  heavily  used,  the\\nburden for managing them currently falls on the programmer.', '\\n\\nAn important alternative method for exploiting loop-level parallelism is the\\nuse of SIMD in both vector processors and Graphics Processing Units (GPUs),\\nboth of which are covered in Chapter 4.', ' The following example illustrates the challenges, and in the next chapter\\nwe will see an alternative approach to exploiting fine-grained parallelism in the\\nform of GPUs.', ' In the next chapter, we\\nwill see how multithreading provides the same advantages in GPUs, and finally,\\nChapter 5  will  explore the combination  of  multithreading and multiprocessing.', ' In a strict sense, multithreading\\nuses thread-level parallelism, and thus is properly the subject of Chapter 5, but its\\nrole in both improving pipeline utilization and in GPUs motivates us to introduce\\nthe concept here.', '  The  Sun  Niagara  processor,  which  we  examine\\nshortly, uses simple fine-grained multithreading, as do the Nvidia GPUs, which\\nwe look at in the next chapter.', ' More importantly, graphics process-\\ning units (GPUs) pursued aggressive use of SIMD, achieving significant perfor-\\nmance  advantages  for  applications  with  extensive  data-level  parallelism.', '10\\n\\nIntroduction\\nVector Architecture\\nSIMD Instruction Set Extensions for Multimedia\\nGraphics Processing Units\\nDetecting and Enhancing Loop-Level Parallelism\\nCrosscutting Issues\\nPutting It All Together: Mobile versus Server GPUs\\n\\nand Tesla versus Core i7\\n\\nFallacies and Pitfalls\\nConcluding Remarks\\nHistorical Perspective and References\\nCase Study and Exercises by Jason D.', ' Bakos\\n\\n262\\n264\\n282\\n288\\n315\\n322\\n\\n323\\n330\\n332\\n334\\n334\\n\\n\\x0c4\\n\\nData-Level Parallelism in \\nVector, SIMD, and GPU \\nArchitectures\\n\\n1\\n\\nWe call these algorithms data parallel algorithms because their parallelism \\ncomes from simultaneous operations across large sets of data, rather than \\nfrom multiple threads of control.', '\\n\\n\\x0c262 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\n \\n\\n4.', ' \\n\\nThis chapter covers three variations of SIMD: vector architectures, multime-\\n\\ndia SIMD instruction set extensions, and graphics processing units (GPUs).', '\\n\\nThe  third  variation  on  SIMD  comes  from  the  GPU  community,  offering\\nhigher  potential  performance  than  is  found  in  traditional  multicore  computers\\ntoday.', ' While GPUs share features with vector architectures, they have their own\\ndistinguishing characteristics, in part due to the ecosystem in which they evolved.', '\\nThis environment has a system processor and system memory in addition to the\\nGPU and its graphics memory.', ' In fact, to recognize those distinctions, the GPU\\ncommunity refers to this type of architecture as heterogeneous.', ' \\n\\n1 This chapter is based on material in Appendix F, “Vector Processors,” by Krste Asanovic, and Appendix G, “Hardware\\nand Software for VLIW and EPIC” from the 4th edition of this book; on material in Appendix A, “Graphics and Com-\\nputing GPUs,” by John Nickolls and David Kirk, from the 4th edition of Computer Organization and Design; and to a\\nlesser extent on material in “Embracing and Extending 20th-Century Instruction Set Architectures,” by Joe Gebis and\\nDavid Patterson, IEEE Computer, April 2007.', ' \\nThe  goal of this  chapter is  for  architects  to  understand why  vector is  more\\ngeneral  than  multimedia  SIMD,  as  well  as  the  similarities  and  differences\\nbetween vector and GPU architectures.', ' Since vector architectures are supersets\\nof the multimedia SIMD instructions, including a better model for compilation,\\nand since GPUs share several similarities with vector architectures, we start with\\n\\n\\x0c264 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nvector architectures to set the foundation for the following two sections.', ' \\n\\n\\x0c266 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nInstruction\\n\\nOperands\\n\\nFunction\\n\\nADDVV.', ' \\n\\n\\x0c268 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nLoop:\\n\\n L.', ' If only\\none vector instruction can be initiated in a clock cycle (the reality in most vector\\nprocessors),  the  chime  count  will  underestimate  the  actual  execution  time  of  a\\n\\n\\x0c270 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nconvoy.', '5 shows the structure of a four-lane vector\\n\\n\\x0c272 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\n(a)\\n\\nElement group\\n\\n(b)\\n\\nFigure 4.', '\\n\\n\\x0c274 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nVector-Length Registers: Handling Loops Not Equal to 64\\n\\nA vector register processor has a natural vector length determined by the number\\nof  elements  in  each  vector  register.', ' When the\\nvector-mask register is enabled, any vector instructions executed operate only on\\n\\n\\x0c276 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nthe vector elements whose corresponding entries in the vector-mask register are\\none.', '4, one difference between vector processors and\\nGPUs is the way they handle conditional statements.', ' In contrast, GPUs get the same effect using hardware to\\nmanipulate  internal  mask  registers  that  are  invisible  to  GPU  software.', '\\n\\n\\x0c278 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nStride: Handling Multidimensional Arrays in Vector \\nArchitectures\\n\\nThe position in memory of adjacent elements in a vector may not be sequential.', ' A gather operation\\n\\n\\x0c280 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\ntakes an index vector and fetches the vector whose elements are at the addresses\\ngiven by adding a base address to the offsets given in the index vector.', '4, all loads are gathers and all stores are scatters\\nin GPUs.', ' To avoid running slowly in the frequent case of unit strides, it is up to\\nthe GPU programmer to ensure that all the addresses in a gather or scatter are to\\nadjacent locations.', ' In addition, the GPU hardware must recognize the sequence\\nof these addresses during execution to turn the gathers and scatters into the more\\nefficient unit stride accesses to memory.', '\\n\\n\\x0c282 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nThe hint-rich versions show significant gains in vectorization level for codes\\nthe compiler could not vectorize well by itself, with all codes now above 50%\\nvectorization.', ' Fourth, SIMD  does not  have  to  deal with problems in\\n\\n\\x0c284 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nAVX Instruction\\n\\nDescription\\n\\nAdd four packed double-precision operands\\n\\nSubtract four packed double-precision operands\\n\\nMultiply four packed double-precision operands\\n\\nDivide four packed double-precision operands\\n\\nMultiply and add four packed double-precision operands\\n\\nVADDPD\\n\\nVSUBPD\\n\\nVMULPD\\n\\nVDIVPD\\n\\nVFMADDPD\\n\\nVFMSUBPD\\n\\nVCMPxx\\n\\nVMOVAPD\\n\\nMultiply and subtract four packed double-precision operands\\nCompare four packed double-precision operands for EQ, NEQ, LT, LE, GT, GE, …\\n\\nMove aligned four packed double-precision operands\\n\\nVBROADCASTSD\\n\\nBroadcast one double-precision operand to four locations in a 256-bit register\\n\\nFigure  4.', '\\n\\n\\x0c286 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nO(1) \\n\\nO(log(N)) \\n\\nO(N) \\n\\nA r i t h m e t i c   I n t e n s i t y \\n\\nSparse\\nmatrix\\n(SpMV)\\n\\nSpectral\\nmethods\\n(FFTs)\\n\\nStructured\\ngrids\\n(Stencils,\\nPDEs)\\n\\nStructured\\ngrids\\n(Lattice\\nmethods)\\n\\nDense\\nmatrix\\n(BLAS3)\\n\\nN-body\\n(Particle\\nmethods)\\n\\nFigure  4.', ' For programs\\n\\n\\x0c288 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\n \\n\\n4.', ' \\n\\nFor a few hundred dollars, anyone can buy a GPU with hundreds of parallel float-\\ning-point units, which makes high-performance computing more accessible.', ' The\\ninterest in GPU computing blossomed when this potential was combined with a\\nprogramming  language  that  made  GPUs  easier  to  program.', '  Hence,  many  pro-\\ngrammers of scientific and multimedia applications today are pondering whether\\nto use GPUs or CPUs.', '\\n\\nGPUs  and  CPUs  do  not  go  back  in  computer  architecture  genealogy  to  a\\ncommon ancestor; there is no Missing Link that explains both.', '10\\ndescribes,  the  primary  ancestors  of  GPUs  are  graphics  accelerators,  as  doing\\ngraphics  well  is  the  reason  why  GPUs  exist.', '  While  GPUs  are  moving  toward\\nmainstream  computing,  they  can’t  abandon  their  responsibility  to  continue  to\\nexcel at graphics.', ' Thus, the design of GPUs may make more sense when archi-\\ntects ask, given the hardware invested to do graphics well, how can we supple-\\nment it to improve the performance of a wider range of applications?\\n\\nNote that this section concentrates on using GPUs for computing.', ' To see how\\nGPU computing combines with the traditional role of graphics acceleration, see\\n“Graphics and Computing GPUs,” by John Nickolls and David Kirk (Appendix A\\nin the 4th edition of Computer Organization and Design by the same authors as\\nthis book).', '\\n\\nSince the  terminology and  some hardware  features  are  quite  different from\\nvector and SIMD architectures, we believe it will be easier if we start with the\\nsimplified programming model for GPUs before we describe the architecture.', '\\n\\nProgramming the GPU\\n\\nCUDA  is  an  elegant  solution  to  the  problem  of  representing  parallelism  in\\nalgorithms, not all algorithms, but enough to matter.', '\\n\\nVincent Natol\\n“Kudos for CUDA,” HPC Wire (2010)\\n\\nThe challenge for the GPU programmer is not simply getting good performance\\non the GPU, but also in coordinating the scheduling of computation on the sys-\\ntem processor and the GPU and the transfer of data between system memory and\\nGPU memory.', ' Moreover, as we see shall see later in this section, GPUs have vir-\\ntually every type of parallelism that can be captured by the programming envi-\\nronment: multithreading, MIMD, SIMD, and even instruction-level.', '4 Graphics Processing Units\\n\\n■ 289\\n\\nNVIDIA  decided  to  develop  a  C-like  language  and  programming  environ-\\nment  that  would  improve  the  productivity  of  GPU  programmers  by  attacking\\nboth the challenges of heterogeneous computing and of multifaceted parallelism.', '\\nCUDA produces C/C++ for the system processor (host) and a C and C++ dialect\\nfor the GPU (device, hence the D in CUDA).', '  Using  this  lowest  level  of  parallelism  as  the  programming\\nprimitive, the compiler and the hardware can gang thousands of CUDA Threads\\ntogether to utilize the various styles of parallelism within a GPU: multithreading,\\nMIMD, SIMD, and instruction-level parallelism.', '\\n\\nWe need just a few details before we can give an example of a CUDA program:\\n\\n■ To distinguish between functions for the GPU (device) and functions for the\\nsystem processor (host), CUDA uses __device__or __global__for the for-\\nmer and __host__for the latter.', '\\n\\n■ CUDA variables declared as in the__device__or__global__functions are\\nallocated to the GPU Memory (see below), which is accessible by all multi-\\nthreaded SIMD processors.', '\\n\\n■ The extended function call syntax for the function name that runs on the GPU is\\nname<<<dimGrid, dimBlock>>>(.', '0, x, y);\\n// DAXPY in C\\nvoid daxpy(int n, double a, double *x, double *y)\\n{\\n\\nfor (int i = 0; i < n; ++i)\\n\\ny[i] = a*x[i] + y[i];\\n\\n}\\n\\n\\x0c290 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nBelow is the CUDA version.', ' The\\nGPU function starts by calculating the corresponding element index i based on\\nthe block ID, the number of threads per block, and the thread ID.', ' \\n\\nThe GPU hardware handles parallel execution and thread management; it is\\nnot done by applications or by the operating system.', ' \\n\\nAs  we  shall  soon  see,  many  GPU  hardware  concepts  are  not  obvious  in\\nCUDA.', '  That  is  a  good  thing  from  a  programmer  productivity  perspective,  but\\nmost  programmers  are  using  GPUs  instead  of  CPUs  to  get  performance.', '\\nPerformance programmers must keep the GPU hardware in mind when writing in\\nCUDA.', '  It  will  be  interesting  to  see  how  the  lan-\\nguage evolves in this classic productivity–performance battle as well as to see if\\nCUDA becomes popular for other GPUs or even other architectural styles.', '\\n\\nNVIDIA GPU Computational Structures\\n\\nThe  uncommon  heritage  mentioned  above  helps  explain  why  GPUs  have  their\\nown architectural style and their own terminology independent from CPUs.', ' One\\nobstacle to understanding GPUs has been the jargon, with some terms even hav-\\ning misleading names.', ' To try to bridge the twin goals of\\nmaking  the  architecture  of  GPUs  understandable  and  learning  the  many  GPU\\nterms with non traditional definitions, our final solution is to use the CUDA ter-\\nminology for software but initially use more descriptive terms for the hardware,\\nsometimes borrowing terms used by OpenCL.', ' Once we explain the GPU archi-\\ntecture in our terms, we’ll map them into the official jargon of NVIDIA GPUs.', '12 lists the more descriptive term used in this sec-\\ntion,  the  closest  term  from  mainstream  computing,  the  official  NVIDIA  GPU\\nterm in case you are interested, and then a short description of the term.', ' The rest\\nof  this  section  explains  the  microarchitetural  features  of  GPUs  using  these\\ndescriptive terms from the left of the figure.', ' \\n\\nWe use NVIDIA systems as our example as they are representative of GPU\\narchitectures.', '\\n\\nLike vector architectures, GPUs work well only with data-level parallel prob-\\nlems.', ' Both styles have gather-scatter data transfers and mask registers, and GPU\\nprocessors have even more registers than do vector processors.', ' Since they do not\\nhave a close-by scalar processor, GPUs sometimes implement a feature at runtime\\nin hardware that vector computers implement at compiler time in software.', ' Unlike\\nmost vector architectures, GPUs also rely on multithreading within a single multi-\\nthreaded SIMD processor to hide memory latency (see Chapters 2 and 3).', ' How-\\never, efficient code for both vector architectures and GPUs requires programmers\\nto think in groups of SIMD operations.', '\\n\\nA Grid is the code that runs on a GPU that consists of a set of Thread Blocks.', '13 shows the relationship between this\\nexample and these first two GPU terms.', ' The GPU code that works on the whole\\n8192  element  multiply  is  called  a  Grid  (or  vectorized  loop).', ' The Grid and Thread Block\\n\\n\\x0c292 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nMore descrip-\\ntive name\\n\\nClosest old term \\noutside of GPUs\\n\\nOfficial CUDA/\\nNVIDIA GPU term Book definition\\n\\nType\\n\\ns\\nn\\no\\ni\\nt\\nc\\na\\nr\\nt\\ns\\nb\\na\\nm\\na\\nr\\ng\\no\\nr\\nP\\n\\n \\n\\n \\n\\nt\\nc\\ne\\nj\\nb\\no\\ne\\nn\\nh\\nc\\na\\nM\\n\\ni\\n\\n \\n\\ne\\nr\\na\\nw\\nd\\nr\\na\\nh\\ng\\nn\\ni\\ns\\ns\\ne\\nc\\no\\nr\\nP\\n\\ne\\nr\\na\\nw\\nd\\nr\\na\\nh\\ny\\nr\\no\\nm\\ne\\nM\\n\\n \\n\\nVectorizable \\nLoop\\n\\nVectorizable Loop Grid\\n\\nBody of \\nVectorized Loop\\n\\n Body of a \\n(Strip-Mined) \\nVectorized Loop\\n\\nThread Block\\n\\nSequence of\\nSIMD Lane \\nOperations\\n\\nA Thread of \\nSIMD \\nInstructions\\n\\nSIMD \\nInstruction\\n\\nMultithreaded \\nSIMD \\nProcessor\\n\\nThread Block \\nScheduler\\n\\nSIMD Thread\\nScheduler\\n\\nOne iteration of \\na Scalar Loop\\n\\nCUDA Thread\\n\\nThread of Vector \\nInstructions\\n\\nWarp\\n\\nVector Instruction\\n\\nPTX Instruction\\n\\n(Multithreaded) \\nVector Processor\\n\\nStreaming \\nMultiprocessor\\n\\nScalar Processor\\n\\nGiga Thread \\nEngine\\n\\nThread scheduler \\nin a Multithreaded \\nCPU\\n\\nWarp Scheduler\\n\\nSIMD Lane\\n\\nVector Lane\\n\\nThread Processor\\n\\nGPU Memory\\n\\nMain Memory\\n\\nGlobal Memory\\n\\nA vectorizable loop, executed on the GPU, made \\nup of one or more Thread Blocks (bodies of \\nvectorized loop) that can execute in parallel.', '\\n\\nDRAM memory accessible by all multithreaded \\nSIMD Processors in a GPU.', '12 Quick  guide  to  GPU  terms  used  in  this  chapter.', ' (The maximum number of SIMD Threads that can execute simultaneously per Thread Block is 16 for Tesla-\\ngeneration GPUs and 32 for the later Fermi-generation GPUs.', ')\\n\\n\\x0c294 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nInstruction\\ncache\\n\\nWarp scheduler\\n\\nWarp No.', '\\n\\nare programming abstractions implemented in GPU hardware that help program-\\nmers  organize  their  CUDA  code.', '15 Floor  plan  of  the  Fermi  GTX  480  GPU.', ' \\n\\nThe GPU hardware then contains a collection of multithreaded SIMD Proces-\\nsors that execute a Grid of Thread Blocks (bodies of vectorized loop); that is, a\\nGPU is a multiprocessor composed of multithreaded SIMD Processors.', ' To provide\\ntransparent  scalability  across  models  of  GPUs  with  differing  number  of  multi-\\nthreaded SIMD Processors, the Thread Block Scheduler assigns Thread Blocks\\n(bodies  of  a  vectorized  loop)  to  multithreaded  SIMD  Processors.', '  These\\n\\n\\x0c296 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nthreads of SIMD instructions have their own PCs and they run on a multithreaded\\nSIMD Processor.', '  Thus,\\nGPU  hardware  has  two  levels  of  hardware  schedulers:  (1)  the  Thread  Block\\nScheduler  that  assigns  Thread  Blocks  (bodies  of  vectorized  loops)  to  multi-\\nthreaded SIMD Processors, which ensures that thread blocks are assigned to the\\nprocessors whose local memories have the corresponding data, and (2) the SIMD\\nThread  Scheduler  within  a  SIMD  Processor,  which  schedules  when  threads  of\\nSIMD instructions should run.', ' \\n\\nThe number of lanes per SIMD processor varies across GPU generations.', ' The assumption of\\nGPU architects is that GPU applications have so many threads of SIMD instruc-\\ntions that multithreading can both hide the latency to DRAM and increase utiliza-\\ntion of multithreaded SIMD Processors.', ' However, to hedge their bets, the recent\\nNVIDIA Fermi GPU includes an L2 cache (see Section 4.', '\\n(Rather than trying to design hardware registers with many read ports and write\\nports  per  bit,  GPUs  will  use  simpler  memory  structures  but  divide  them  into\\nbanks  to  get  sufficient  bandwidth,  just  as  vector  processors  do.', '\\n\\nWe’re now ready to see what GPU instructions look like.', '\\n\\n\\x0c298 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nNVIDA GPU Instruction Set Architecture\\n\\nUnlike most system processors, the instruction set target of the NVIDIA compil-\\ners is an abstraction of the hardware instruction set.', ' PTX (Parallel Thread Execu-\\ntion)  provides  a  stable  instruction  set  for  compilers  as  well  as  compatibility\\nacross generations of GPUs.', '\\n\\nWhile there is some similarity between the x86 microarchitectures and PTX,\\nin that both translate to an internal form (microinstructions for x86), the differ-\\nence is that this translation happens in hardware at runtime during execution on\\nthe x86 versus in software and load time on a GPU.', '17 Basic PTX GPU thread instructions.', '\\n\\nd = 1/a;\\n\\nd = sqrt(a);\\n\\nd = 1/sqrt(a);\\n\\nd = sin(a);\\n\\nd = cos(a);\\n\\nreciprocal\\n\\nsquare root\\n\\nsine\\n\\ncosine\\n\\nreciprocal square root\\n\\nd = log(a)/log(2)\\n\\nbinary logarithm\\n\\nd = 2 ** a;\\n\\nbinary exponential\\n\\nd = a & b;\\n\\nd = a | b;\\n\\nd = a ^ b;\\n\\nd = ~a;\\n\\nd = a << b;\\n\\nd = a >> b;\\n\\nd = (a==0)? 1:0;\\n\\nC logical not\\n\\none’s complement\\n\\nshift left\\n\\nshift right\\n\\n\\x0c300 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nThe  following  sequence  of  PTX  instructions  is  for  one  iteration  of  our\\n\\nDAXPY loop on page 289:\\n\\nshl.', ')\\nNote that unlike vector architectures, GPUs don’t have separate instructions\\nfor sequential data transfers, strided data transfers, and gather-scatter data trans-\\nfers.', ' All data transfers are gather-scatter! To regain the efficiency of sequential\\n(unit-stride) data transfers, GPUs include special Address Coalescing hardware\\nto recognize when the SIMD Lanes within a thread of SIMD instructions are col-\\nlectively  issuing  sequential  addresses.', ' To get\\nthis important performance improvement, the GPU programmer must ensure that\\nadjacent  CUDA  Threads  access  nearby  addresses  at  the  same  time  that  can  be\\ncoalesced into one or a few memory or cache blocks, which our example does.', ' \\n\\nConditional Branching in GPUs\\n\\nJust  like  the  case  with  unit-stride  data  transfers,  there  are  strong  similarities\\nbetween how vector architectures and GPUs handle IF statements, with the for-\\nmer implementing the mechanism largely in software with limited hardware sup-\\nport  and  the  latter  making  use  of  even  more  hardware.', '  As  we  shall  see,  in\\naddition  to  explicit  predicate  registers,  GPU  branch  hardware  uses  internal\\nmasks, a branch synchronization stack, and instruction markers to manage when\\na branch diverges into multiple execution paths and when the paths converge.', ' The PTX assembler analyzes the PTX branch graph and\\noptimizes it to the fastest GPU hardware instruction sequence.', '4 Graphics Processing Units\\n\\n■ 301\\n\\nAt the GPU hardware instruction level, control flow includes branch, jump,\\njump indexed, call, call indexed, return, exit, and special instructions that manage\\nthe  branch  synchronization  stack.', '  GPU  hardware  provides  each  SIMD  thread\\nwith its own stack; a stack entry contains an identifier token, a target instruction\\naddress, and a target thread-active mask.', ' There are GPU special instructions that\\npush  stack  entries  for  a  SIMD  thread  and  special  instructions  and  instruction\\nmarkers that pop a stack entry or unwind the stack to a specified entry and branch\\nto the  target  instruction  address  with the  target  thread-active mask.', ' GPU hard-\\nware  instructions  also  have  individual  per-lane  predication  (enable/disable),\\nspecified with a 1-bit predicate register for each lane.', ' \\n\\nThe PTX assembler typically optimizes a simple outer-level IF/THEN/ELSE\\nstatement  coded  with  PTX  branch  instructions  to  just  predicated  GPU  instruc-\\ntions, without any GPU branch instructions.', ' A more complex control flow typi-\\ncally results in a mixture of predication and GPU branch instructions with special\\ninstructions and markers that use the branch synchronization stack to push a stack\\nentry  when  some  lanes  branch  to  the  target  address,  while  others  fall  through.', ' \\n\\nThe  PTX  assembler  identifies  loop  branches  and  generates  GPU  branch\\ninstructions that branch to the top of the loop, along with special stack instruc-\\ntions  to  handle  individual  lanes  breaking  out  of  the  loop  and  converging  the\\nSIMD  Lanes  when  all  lanes  have  completed  the  loop.', '  GPU  indexed  jump  and\\nindexed call instructions push entries on the stack so that when all lanes complete\\nthe switch statement or function call the SIMD thread converges.', '\\n\\nA GPU set predicate instruction (setp in the figure above) evaluates the con-\\nditional  part  of  the  IF  statement.', '  If  the  PTX  assembler  generates  predicated  instructions  with  no\\nGPU branch instructions, it uses a per-lane predicate register to enable or disable\\neach  SIMD  Lane  for  each  instruction.', ' \\n\\nIF statements can be nested, hence the use of a stack, and the PTX assembler\\ntypically generates a mix of predicated instructions and GPU branch and special\\nsynchronization instructions for complex control flow.', ' \\n\\n\\x0c302 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nDropping down a level of detail, the PTX assembler sets a “branch synchro-\\nnization” marker on appropriate conditional branch instructions that pushes the\\ncurrent active mask on a stack inside each SIMD thread.', '  This  flexibility  can  lead\\nnaive GPU programmers to poor performance, but it can be helpful in the early\\nstages of program development.', ' A naive programmer may think that this thread abstraction means GPUs\\nhandle conditional branches more gracefully.', '\\n\\nVector  compilers  could do the same tricks with mask  registers as GPUs\\ndo in hardware, but it would involve scalar instructions to save, complement,\\nand restore mask registers.', ' Conditional execution is a case where GPUs do in\\nruntime hardware what vector architectures do at compile time.', ' One optimi-\\nzation  available  at  runtime  for  GPUs  but  not  at  compile  time  for  vector\\narchitectures is to skip the THEN or ELSE parts when mask bits are all zeros\\nor all ones.', '\\n\\nThus, the efficiency with which GPUs execute conditional statements comes\\ndown to how frequently the branches would diverge.', ' For example, one calcula-\\ntion of eigenvalues has deep conditional nesting, but measurements of the code\\nshow that around 82% of clock cycle issues have between 29 and 32 out of the 32\\nmask bits set to one, so GPUs execute this code more efficiently than one might\\nexpect.', '\\n\\n\\x0c304 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nNVIDIA GPU Memory Structures\\n\\nFigure 4.', '18 shows the memory structures of an NVIDIA GPU.', ' Recent GPUs cache this Private Memory in the L1\\nand L2 caches to aid register spilling and to speed up function calls.', ' \\n\\nFinally, we call the off-chip DRAM shared by the whole GPU and all thread\\n\\nblocks GPU Memory.', ' Our vector multiply example only used GPU Memory.', ' \\n\\nSequence\\n\\nInter-Grid Synchronization\\n\\nGPU Memory\\n\\nFigure 4.', '18 GPU Memory structures.', ' GPU Memory is shared by all Grids (vectorized\\nloops), Local Memory is shared by all threads of SIMD instructions within a thread block\\n(body of a vectorized loop), and Private Memory is private to a single CUDA Thread.', '4 Graphics Processing Units\\n\\n■ 305\\n\\nThe system processor, called the host, can read or write GPU Memory.', '\\n\\nRather  than  rely  on  large  caches  to  contain  the  whole  working  sets  of  an\\napplication,  GPUs  traditionally  use  smaller  streaming  caches  and  rely  on\\nextensive multithreading of threads of SIMD instructions to hide the long latency\\nto DRAM, since their working sets can be hundreds of megabytes.', ' \\n\\nWhile hiding memory latency is the underlying philosophy, note that the lat-\\nest  GPUs  and  vector  processors  have  added  caches.', '  For  example,  the  recent\\nFermi architecture has added caches, but they are thought of as either bandwidth\\nfilters to reduce demands on GPU Memory or as accelerators for the few vari-\\nables whose latency cannot be hidden by multithreading.', ' These restrictions are placed on the GPU pro-\\ngram, somewhat  analogous to  the guidelines for  system  processor  programs  to\\nengage hardware prefetching (see Chapter 2).', ' The GPU memory controller will\\nalso  hold  requests  and  send  ones  to  the  same  open  page  together  to  improve\\nmemory bandwidth (see Section 4.', '\\n\\nInnovations in the Fermi GPU Architecture\\n\\nThe multithreaded SIMD Processor of Fermi is more complicated than the sim-\\nplified version in Figure 4.', '20\\nshows the block diagram of the multithreaded SIMD Processor of a Fermi GPU.', '\\n\\n\\x0c306 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nSIMD thread scheduler\\n\\nSIMD thread scheduler\\n\\nInstruction dispatch unit\\n\\nInstruction dispatch unit\\n\\ne\\nm\\nT\\n\\ni\\n\\nSIMD thread 8 instruction 11\\n\\nSIMD thread 9 instruction 11\\n\\nSIMD thread 2 instruction 42\\n\\nSIMD thread 3 instruction 33\\n\\nSIMD thread 14 instruction 95\\n\\nSIMD thread 15 instruction 95\\n\\nSIMD thread 8 instruction 12\\n\\nSIMD thread 9 instruction 12\\n\\nSIMD thread 14 instruction 96\\n\\nSIMD thread 3 instruction 34\\n\\nSIMD thread 2 instruction 43\\n\\nSIMD thread 15 instruction 96\\n\\nFigure  4.', ' \\n\\nFermi introduces several innovations to bring GPUs much closer to mainstream\\n\\nsystem processors than Tesla and previous generations of GPU architectures:\\n\\n■ Fast  Double-Precision  Floating-Point Arithmetic—Fermi  matches  the rela-\\ntive  double-precision  speed  of  conventional  processors  of  roughly  half  the\\nspeed of single precision versus a tenth the speed of single precision in the\\nprior Tesla generation.', ' The peak\\ndouble-precision performance grew from 78 GFLOP/sec in the predecessor\\nGPU to 515 GFLOP/sec when using multiply-add instructions.', ' \\n\\n■ Caches  for  GPU  Memory—While  the  GPU  philosophy  is  to  have  enough\\nthreads  to  hide  DRAM  latency,  there  are  variables  that  are  needed  across\\nthreads, such as local variables mentioned above.', ' Fermi includes both an L1\\nData Cache and L1 Instruction Cache for each multithreaded SIMD Processor\\nand a single 768 KB L2 cache shared by all multithreaded SIMD Processors in\\nthe GPU.', ' As mentioned above, in addition to reducing bandwidth pressure on\\nGPU Memory, caches can save energy by staying on-chip rather than going\\noff-chip to DRAM.', ' It will be interesting to\\nsee the impact of this inverted ratio on GPU applications.', '\\n\\n■\\n\\n64-Bit Addressing and a Unified Address Space for All GPU Memories—This\\ninnovation makes it much easier to provide the pointers needed for C and C++.', '20 Block  diagram  of  the  multithreaded  SIMD  Processor  of  a  Fermi  GPU.', '\\n\\n\\x0c308 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\n■ Faster  Atomic  Instructions—First  included  in  the  Tesla  architecture,  Fermi\\nimproves performance of Atomic instructions by 5 to 20×, to a few microsec-\\nonds.', '\\n\\nSimilarities and Differences between Vector \\nArchitectures and GPUs\\n\\nAs we have seen, there really are many similarities between vector architectures\\nand GPUs.', ' Along with the quirky jargon of GPUs, these similarities have con-\\ntributed to the confusion in architecture circles about how novel GPUs really are.', '\\nNow that you’ve seen what is under the covers of vector computers and GPUs,\\nyou can appreciate both the similarities and the differences.', '21 shows the vector term first and then the\\nclosest equivalent in a GPU.', ' The multiple SIMD Processors\\nin GPUs act as independent MIMD cores, just as many vector computers have\\nmultiple vector processors.', ' The biggest difference is multithreading, which is fundamental to GPUs\\nand missing from most vector processors.', ' In contrast, a sin-\\ngle vector in a GPU would be distributed across the registers of all SIMD Lanes.', ' A GPU thread of SIMD instructions has up to 64 registers with 32 elements\\neach, or 2048 elements.', ' These extra GPU registers support multithreading.', '22 is a block diagram of the execution units of a vector processor on\\nthe left and a multithreaded SIMD Processor of a GPU on the right.', '\\n\\nIn reality, there are many more lanes in GPUs, so GPU “chimes” are shorter.', ' A SIMD thread is 32 elements wide, so a GPU chime\\nwould just be 2 or 4 clock cycles.', '\\n\\nThe closest GPU term to a vectorized loop is Grid, and a PTX instruction is\\nthe closest to a vector instruction since a SIMD Thread broadcasts a PTX instruc-\\ntion to all SIMD Lanes.', ' \\n\\n\\x0cClosest CUDA/NVIDIA \\nGPU term\\n\\nComment\\n\\nType\\n\\nVector term\\n\\n \\n\\nm\\na\\nr\\ng\\no\\nr\\nP\\n\\ns Vectorized Loop\\nn\\no\\ni\\nt\\nc\\na\\nr\\nt\\ns\\nb\\na\\n\\nChime\\n\\nGrid\\n\\n--\\n\\nVector Instruction\\n\\nPTX Instruction\\n\\nGather/Scatter\\n\\nGlobal load/store\\n(ld.', 'global)\\n\\n \\n\\ns\\nt\\nc\\ne\\nj\\nb\\no\\ne\\nn\\nh\\nc\\na\\nM\\n\\ni\\n\\n \\n\\ne\\nr\\na\\nw\\nd\\nr\\na\\nh\\ny\\nr\\no\\nm\\ne\\nm\\nd\\nn\\na\\ng\\nn\\ni\\ns\\ns\\ne\\nc\\no\\nr\\nP\\n\\n \\n\\n \\n\\nMask Registers\\n\\nPredicate Registers and\\nInternal Mask Registers\\n\\nVector Processor\\n\\nMultithreaded SIMD\\nProcessor\\n\\nControl Processor \\n\\nThread Block Scheduler\\n\\nScalar Processor \\n\\nSystem Processor\\n\\nVector Lane\\n\\nSIMD Lane\\n\\nVector Registers\\n\\nSIMD Lane Registers\\n\\nMain Memory\\n\\nGPU Memory\\n\\nFigure 4.', '21 GPU equivalent to vector terms.', '4 Graphics Processing Units\\n\\n■ 309\\n\\nConcepts are similar, with the GPU using the less \\ndescriptive term.', '\\n\\nSince a vector instruction (PTX Instruction) takes \\njust two cycles on Fermi and four cycles on Tesla \\nto complete, a chime is short in GPUs.', '\\n\\nAll GPU loads and stores are gather and scatter, in \\nthat each SIMD Lane sends a unique address.', ' It’s \\nup to the GPU Coalescing Unit to get unit-stride \\nperformance when addresses from the SIMD \\nLanes allow it.', '\\n\\nVector mask registers are explicitly part of the \\narchitectural state, while GPU mask registers are \\ninternal to the hardware.', ' The GPU conditional \\nhardware adds a new feature beyond predicate \\nregisters to manage masks dynamically.', ' But GPUs have no scalar-vector \\noperations and no unit-stride or strided data \\ntransfer instructions, which Control Processors \\noften provide.', '\\n\\nBecause of the lack of shared memory and the \\nhigh latency to communicate over a PCI bus \\n(1000s of clock cycles), the system processor in a \\nGPU rarely takes on the same tasks that a scalar \\nprocessor does in a vector architecture.', '\\n\\nMemory for GPU versus System memory in \\nvector case.', '\\n\\n\\x0c310 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nPC\\n\\nInstruction\\ncache\\n\\nPC\\n\\nPC\\n\\nPC\\n\\nPC\\n\\nSIMD Thread Scheduler\\n\\nInstruction\\ncache\\n\\nDispatch unit\\n\\nInstruction register\\n\\nInstruction register\\n\\nControl\\nprocesser\\n\\nMask\\n\\nMask\\n\\nMask\\n\\nMask\\n\\nMask\\n\\nMask\\n\\nMask\\n\\nMask\\n\\nFU\\n0\\n\\nFU\\n1\\n\\nFU\\n2\\n\\nFU\\n3\\n\\nFU\\n0\\n\\nFU\\n1\\n\\nFU\\n2\\n\\nFU\\n3\\n\\n0\\n4\\n\\n•\\n•\\n•\\n\\ni\\n\\ns\\nr\\ne\\nt\\ns\\ng\\ne\\nr\\n \\nr\\no\\nt\\nc\\ne\\nV\\n\\n1\\n5\\n\\n•\\n•\\n•\\n\\n2\\n6\\n\\n•\\n•\\n•\\n\\n3\\n7\\n\\n•\\n•\\n•\\n\\n0\\n1\\n\\n•\\n•\\n•\\n\\ns\\nr\\ne\\nt\\ns\\ng\\ne\\nR\\n\\ni\\n\\n0\\n1\\n\\n•\\n•\\n•\\n\\n0\\n1\\n\\n•\\n•\\n•\\n\\n0\\n1\\n\\n•\\n•\\n•\\n\\n60\\n\\n61\\n\\n62\\n\\n63\\n\\n1023\\n\\n1023\\n\\n1023\\n\\n1023\\n\\nVector load/store unit\\n\\nSIMD Load/store unit\\n\\nMemory interface\\nunit\\n\\nAddress coalescing unit\\n\\nMemory interface unit\\n\\nFigure 4.', '22 A vector processor with four lanes on the left and a multithreaded SIMD Processor of a GPU with four\\nSIMD Lanes on the right.', ' (GPUs typically have 8 to 16 SIMD Lanes.', ' Peak memory performance only occurs in a GPU when the Address Coalescing unit can\\ndiscover localized addressing.', '\\n\\nWith  respect to  memory access  instructions  in the  two architectures, all GPU\\nloads  are  gather  instructions  and  all  GPU  stores  are  scatter  instructions.', '  If  data\\naddresses of CUDA Threads refer to nearby addresses that fall in the same cache/\\nmemory  block  at  the  same  time,  the  Address  Coalescing  Unit  of  the  GPU  will\\nensure high memory bandwidth.', ' The explicit unit-stride load and store instructions\\nof vector architectures versus the implicit unit stride of GPU programming is why\\nwriting efficient GPU code requires that programmers think in terms of SIMD oper-\\nations, even though the CUDA programming model looks like MIMD.', ' As CUDA\\nThreads can generate their own addresses, strided as well as gather-scatter, address-\\ning vectors are found in both vector architectures and GPUs.', ' In contrast, GPUs hide\\nmemory latency using multithreading.', ' The difference is that the vector com-\\npiler manages mask registers explicitly in software while the GPU hardware and\\nassembler manages them implicitly using branch synchronization markers and an\\ninternal stack to save, complement, and restore masks.', '\\n\\nAs mentioned above, the conditional branch mechanism of GPUs gracefully\\nhandles the strip-mining problem of vector architectures.', ' This case is simpler with GPUs\\nsince they just iterate the loop until all the SIMD Lanes reach the loop bound.', ' It also does\\nimplicit  calculations  that  are  explicit  in  GPUs,  such  as  automatically  incre-\\nmenting memory addresses for unit-stride and non-unit-stride loads and stores.', '\\nThe control processor is missing in the GPU.', ' The runtime hardware mechanisms in a GPU that\\nboth generate addresses and then discover if they are adjacent, which is com-\\nmonplace in many DLP applications, are likely less power efficient than using\\na control processor.', ' Although the system processor that is associated with a GPU is the\\nclosest analogy to a scalar processor in a vector architecture, the separate address\\nspaces  plus  transferring  over  a  PCle  bus  means  thousands  of  clock  cycles  of\\noverhead to use them together.', '\\n\\nHence,  each “vector unit”  in  a  GPU  must  do  computations that  you  would\\nexpect to do on a scalar processor in a vector computer.', ' The relatively simple scalar processor\\nin a vector computer is likely to be faster and more power efficient than the GPU\\n\\n\\x0c312 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nsolution.', '  If  system processors and GPUs  become more  closely tied  together in\\nthe future, it will be interesting to see if system processors can play the same role\\nas scalar processors do for vector and Multimedia SIMD architectures.', ' \\n\\nSimilarities and Differences between Multimedia SIMD \\nComputers and GPUs\\n\\nAt a high level, multicore computers with Multimedia SIMD instruction exten-\\nsions  do  share  similarities  with  GPUs.', ' \\n\\nBoth  are  multiprocessors  whose  processors  use  multiple  SIMD  lanes,\\nalthough GPUs have more processors and many more lanes.', ' Both use hardware\\nmultithreading to improve processor utilization, although GPUs have hardware\\nsupport for many more threads.', ' Recent innovations in GPUs mean that now both\\nhave  similar  performance  ratios  between  single-precision  and  double-precision\\nfloating-point arithmetic.', ' Both use caches, although GPUs use smaller streaming\\ncaches and multicore computers  use  large  multilevel  caches that  try  to  contain\\nwhole  working  sets  completely.', '  Both  use  a  64-bit  address  space,  although  the\\nphysical main memory is much smaller in GPUs.', ' While GPUs support memory\\nprotection at the page level, they do not support demand paging.', ' The scalar processor and Multimedia SIMD instructions are tightly inte-\\ngrated in traditional computers; they are separated by an I/O bus in GPUs, and\\nthey  even  have  separate  main  memories.', '  The  multiple  SIMD  processors  in  a\\nGPU use a single address space, but the caches are not coherent as they are in tra-\\nditional  multicore  computers.', '  Unlike  GPUs,  multimedia  SIMD  instructions  do\\nnot support gather-scatter memory accesses, which Section 4.', '\\n\\nFeature\\n\\nSIMD processors\\n\\nSIMD lanes/processor\\n\\nMultithreading hardware support for SIMD threads\\n\\nTypical ratio of single-precision to double-precision performance\\n\\nLargest cache size\\n\\nSize of memory address\\n\\nSize of main memory\\n\\nMemory protection at level of page\\n\\nIntegrated scalar processor/SIMD processor\\n\\nDemand paging\\n\\nCache coherent\\n\\nMulticore with SIMD\\n\\nGPU\\n\\n4 to 8\\n\\n2 to 4\\n\\n2 to 4\\n\\n2:1\\n\\n8 MB\\n\\n64-bit\\n\\nYes\\n\\nYes\\n\\nYes\\n\\nYes\\n\\n8 GB to 256 GB\\n\\n8 to 16\\n\\n8 to 16\\n\\n16 to 32\\n\\n2:1\\n\\n0.', '23 Similarities and differences between multicore with Multimedia SIMD extensions and recent GPUs.', '4 Graphics Processing Units\\n\\n■ 313\\n\\nSummary\\n\\nNow  that  the  veil  has  been  lifted,  we  can  see  that  GPUs  are  really  just  multi-\\nthreaded SIMD processors, although they have more processors, more lanes per\\nprocessor, and more multithreading hardware than do traditional multicore com-\\nputers.', '\\n\\nType\\n\\nMore \\ndescriptive \\nname used in \\nthis book\\n\\nVectorizable \\nloop\\n\\nOfficial \\nCUDA/\\nNVIDIA \\nterm\\n\\nGrid\\n\\nBody of \\nVectorized \\nloop\\n\\nThread \\nBlock\\n\\nSequence of\\nSIMD Lane \\noperations\\n\\nCUDA \\nThread\\n\\nA Thread of \\nSIMD \\ninstructions\\n\\nWarp\\n\\ns\\nn\\no\\ni\\nt\\nc\\na\\nr\\nt\\ns\\nb\\na\\nm\\na\\nr\\ng\\no\\nr\\nP\\n\\n \\n\\n \\n\\nt\\nc\\ne\\nj\\nb\\no\\ne\\nn\\nh\\nc\\na\\nM\\n\\ni\\n\\nBook definition and\\nAMD and OpenCL terms\\n\\nOfficial CUDA/NVIDIA\\ndefinition\\n\\nA vectorizable loop, executed on the \\nGPU, made up of one or more “Thread \\nBlocks” (or bodies of vectorized loop) \\nthat can execute in parallel.', '\\n\\n\\x0c314 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nMore \\ndescriptive \\nname used in \\nthis book\\n\\nOfficial \\nCUDA/\\nNVIDIA \\nterm\\n\\nType\\n\\nMultithreaded \\nSIMD \\nprocessor\\n\\nStreaming \\nmulti-\\nprocessor\\n\\nThread \\nblock \\nscheduler\\n\\nSIMD \\nThread \\nscheduler\\n\\nGiga \\nthread \\nengine\\n\\nWarp \\nscheduler\\n\\nSIMD \\nLane\\n\\nThread \\nprocessor\\n\\nGPU \\nMemory\\n\\nGlobal \\nMemory\\n\\nBook definition and\\nAMD and OpenCL terms\\n\\nOfficial CUDA/NVIDIA\\ndefinition\\n\\nMultithreaded SIMD Processor that executes \\nthread of SIMD instructions, independent of \\nother SIMD Processors.', '\\n\\nDRAM memory accessible by all \\nmultithreaded SIMD Processors in a GPU.', '\\n\\nAlthough  we’ve  used  CUDA  and  the  NVIDIA  GPU  in  this  section,  rest\\nassured that the same ideas are found in the OpenCL programming language and\\nin GPUs from other companies.', '\\n\\nNow  that  you  understand  better  how  GPUs  work,  we  reveal  the  real  jargon.', ' We believe the GPU learning curve is steep in part because of\\nusing terms such as “Streaming Multiprocessor” for the SIMD Processor, “Thread\\nProcessor”  for  the  SIMD  Lane,  and  “Shared  Memory”  for  Local  Memory—\\nespecially since Local Memory is not shared between SIMD Processors! We hope\\nthat this two-step approach gets you up that curve quicker, even if it’s a bit indirect.', ' To see that a loop is parallel, let us first look at the\\nsource representation:\\n\\nfor (i=999; i>=0; i=i-1)\\n\\nx[i] = x[i] + s;\\n\\n\\x0c316 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nIn this loop, the two uses of x[i] are dependent, but this dependence is within a\\nsingle  iteration  and  is  not  loop  carried.', ' Consider the following example:\\n\\nfor (i=0;i<100;i=i+1) {\\nA[i] = B[i] + C[i]\\nD[i] = A[i] * E[i]\\n\\n}\\n\\n\\x0c318 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nThe second reference to A in this example need not be translated to a load instruc-\\ntion, since we know that the value is computed and stored by the previous state-\\nment; hence, the second reference to A can simply be a reference to the register\\ninto which A was computed.', ' \\n\\n\\x0c320 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nExample The following loop has multiple types of dependences.', ' \\n\\n\\x0c322 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\n \\n\\n4.', ' Since we assume ample data-level parallelism,\\nthe performance is the same if we halve the clock rate and double the execution\\nresources: twice the number of lanes for a vector computer, wider registers and\\nALUs for multimedia SIMD, and more SIMD lanes for GPUs.', '\\n\\nTo  achieve  their  high  performance,  GPUs  also  require  substantial  memory\\nbandwidth.', '  Special  DRAM  chips  designed  just  for  GPUs,  called  GDRAM  for\\ngraphics DRAM, help deliver this bandwidth.', '  To  deliver  this\\nbandwidth, GDRAM chips are often soldered directly onto the same board as the\\nGPU rather than being placed into DIMM modules that are inserted into slots on\\na  board,  as  is  the  case  for  system  memory.', ' This limited\\ncapacity—about  4  GB  in  2011—is  in  conflict  with  the  goal  of  running  bigger\\nproblems, which is a natural use of the increased computational power of GPUs.', '7 Putting It All Together: Mobile versus Server GPUs and Tesla versus Core i7\\n\\n■ 323\\n\\nTo deliver the best possible performance, GPUs try to take into account all\\nthe features of GDRAMs.', ' Chapter 2 describes the details of DRAM behavior\\nthat GPUs try to match.', '\\nTo  cope,  the  GPU’s  memory  controller  maintains  separate  queues  of  traffic\\nbound for different GDRAM banks, waiting until there is enough traffic to jus-\\ntify  opening  a  row  and  transferring  all  requested  data  at  once.', '\\n\\nStrided Accesses and TLB Misses\\n\\nOne  problem  with  strided  accesses  is  how  they  interact  with  the  translation\\nlookaside  buffer  (TLB)  for  virtual  memory  in  vector  architectures  or  GPUs.', '\\n(GPUs  use  TLBs  for  memory  mapping.', ')  Depending  on  how  the  TLB  is  orga-\\nnized and the size of the array being accessed in memory, it is even possible to\\nget one TLB miss for every access to an element in the array!\\n\\nGiven  the  popularity  of  graphics  applications,  GPUs  are  now  found  in  both\\nmobile  clients  as  well  as  traditional  servers  or  heavy-duty  desktop  computers.', '26 lists the key characteristics of the NVIDIA Tegra 2 for mobile cli-\\nents, which is used in the LG Optimus 2X and runs Android OS, and the Fermi\\nGPU  for  servers.', '  GPU  server  engineers  hope  to  be  able  to  do  live  animation\\nwithin five years after a movie is released.', ' GPU mobile engineers in turn want\\nwithin five more years that a mobile client can do what a server or game console\\ndoes today.', ' More concretely, the overarching goal is for the graphics quality of a\\nmovie such as Avatar to be achieved in real time on a server GPU in 2015 and on\\nyour mobile GPU in 2020.', '  \\n\\nThe NVIDIA Tegra 2 for mobile devices provides both the system processor\\nand the GPU in a single chip using a single physical memory.', '\\n\\nThe  GPU  has  hardware  acceleration  for  programmable  pixel  shading,  pro-\\ngrammable vertex and lighting, and 3D graphics, but it does not include the GPU\\ncomputing features needed to run CUDA or OpenCL programs.', '7\\n\\nPutting It All Together: Mobile versus Server GPUs \\nand Tesla versus Core i7\\n\\n\\x0c324 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nMarket\\n\\nSystem processor\\n\\nSystem interface\\n\\nSystem interface \\nbandwidth\\n\\nClock rate\\n\\nSIMD multiprocessors\\n\\nSIMD lanes/SIMD \\nmultiprocessor\\n\\nMemory interface\\n\\nMemory bandwidth\\n\\nMemory capacity\\n\\nTransistors\\n\\nProcess\\n\\nDie area\\n\\nPower\\n\\nNVIDIA Tegra 2\\n\\nNVIDIA Fermi GTX 480\\n\\nMobile client\\n\\nDesktop, server\\n\\nDual-Core ARM Cortex-A9\\n\\nNot applicable\\n\\nNot applicable\\n\\nUp to 1 GHz\\n\\nUnavailable\\n\\nUnavailable\\n\\n32-bit LP-DDR2/DDR2\\n\\n2.', '26 Key features of the GPUs for mobile clients and servers.', '  The  whole  module  is  250\\nwatts, which includes the GPU, GDRAMs, fans, power regulators, and so on.', ' \\n\\nComparison of a GPU and a MIMD with Multimedia SIMD \\n\\nA  group  of  Intel  researchers  published  a  paper  [Lee  et  al.', '  2010]  comparing  a\\nquad-core Intel i7 (see Chapter 3) with multimedia SIMD extensions to the pre-\\nvious generation GPU, the Tesla GTX 280.', '7 Putting It All Together: Mobile versus Server GPUs and Tesla versus Core i7\\n\\n■ 325\\n\\nCore i7-\\n960\\n\\nGTX 280\\n\\nGTX 480\\n\\n Ratio \\n280/i7 \\n\\n Ratio \\n480/i7 \\n\\nIntel 45 nm TSMC 65 nm TSMC 40 nm  1.', ' The Core i7 is\\nin Intel’s 45-nanometer semiconductor technology while the GPU is in TSMC’s\\n65-nanometer technology.', '\\nFor single-precision performance, the ridge point moves far to the right, as it’s\\nmuch  harder  to  hit  the  roof  of  single-precision  performance  because  it  is  so\\n\\n\\x0cc\\ne\\ns\\n/\\nP\\nO\\nL\\nF\\nG\\n \\nn\\no\\ns\\nc\\ne\\nr\\np\\n-\\ne\\nb\\nu\\no\\nD\\n\\ni\\n\\nl\\n\\ni\\n\\n128\\n\\n64\\n\\n32\\n\\n16\\n\\n8\\n\\n4\\n\\n2\\n\\n1\\n\\nc\\ne\\ns\\n/\\nP\\nO\\nL\\nF\\nG\\n \\nn\\no\\ns\\nc\\ne\\nr\\np\\n-\\ne\\ng\\nn\\nS\\n\\ni\\n\\nl\\n\\ni\\n\\ni\\n\\n1024\\n\\n512\\n\\n256\\n\\n128\\n\\n64\\n\\n32\\n\\n16\\n\\n8\\n\\nc\\ne\\ns\\n/\\nP\\nO\\nL\\nF\\nG\\n \\nn\\no\\ns\\nc\\ne\\nr\\np\\n-\\ne\\nb\\nu\\no\\nD\\n\\ni\\n\\nl\\n\\ni\\n\\nc\\ne\\ns\\n/\\nP\\nO\\nL\\nF\\nG\\n \\nn\\no\\ns\\nc\\ne\\nr\\np\\n-\\ne\\ng\\nn\\nS\\n\\ni\\n\\ni\\n\\nl\\n\\ni\\n\\n128\\n\\n64\\n\\n32\\n\\n16\\n\\n8\\n\\n4\\n\\n2\\n\\n1\\n\\n512\\n\\n256\\n\\n128\\n\\n64\\n\\n32\\n\\n16\\n\\n8\\n\\n326 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nCore i7 920\\n(Nehalem)\\n\\nPeak = 42.', '7 Putting It All Together: Mobile versus Server GPUs and Tesla versus Core i7\\n\\n■ 327\\n\\nmuch  higher.', '\\n\\n\\x0c328 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nKernel\\n\\nUnits\\n\\nCore i7-960\\n\\nGTX 280\\n\\nGTX 280/\\ni7-960\\n\\nSGEMM\\n\\nGFLOP/sec\\n\\nMC\\n\\nConv\\n\\nFFT\\n\\nSAXPY\\n\\nLBM\\n\\nSolv\\n\\nSpMV\\n\\nGJK\\n\\nSort\\n\\nRC\\n\\nSearch\\n\\nHist\\n\\nBilat\\n\\nBillion paths/sec\\n\\nMillion pixels/sec\\n\\nMillion lookups/sec\\n\\nGFLOP/sec\\n\\nGBytes/sec\\n\\nFrames/sec\\n\\nGFLOP/sec\\n\\nFrames/sec\\n\\nMillion elements/sec\\n\\nFrames/sec\\n\\nMillion queries/sec\\n\\nMillion pixels/sec\\n\\nMillion pixels/sec\\n\\n94\\n\\n0.', ' The GPU has 4.', '7 Putting It All Together: Mobile versus Server GPUs and Tesla versus Core i7\\n\\n■ 329\\n\\nCore i7 spends two-thirds of its time calculating transcendental functions, so\\nthe  GTX  280  is  5.', '6×  faster  on  the  GTX  because\\ncache blocking with the Core i7 caches prevents it from becoming memory\\nbandwidth bound, as it is on GPUs.', ' As mentioned above, GPUs offer gather-scatter\\naddressing  that  is  found  in  a  vector  architecture  but  omitted  from  SIMD\\nextensions.', ' This observation reinforces the importance of gather-scatter to\\nvector and GPU architectures that is missing from SIMD extensions.', ' As mentioned above, the atomic updates of\\nthe Fermi GTX 480 are 5 to 20× faster than those of the Tesla GTX 280, so\\nonce again it would be interesting to run Hist on the newer GPU.', '\\n\\n\\x0c330 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nIt is striking how often weaknesses in the Tesla GTX 280 that were uncov-\\nered by kernels selected by Intel researchers were already being addressed in the\\nsuccessor architecture to Tesla: Fermi has faster double-precision floating-point\\nperformance, atomic operations, and caches.', ' It will be interesting\\nto see if future generations of the multicore and GPU hardware, compilers, and\\nlibraries respond with features that improve performance on such kernels.', '\\n\\nWe  hope  that  there  will  be  more  such  multicore-GPU  comparisons.', ' \\n\\nFallacy GPUs suffer from being coprocessors.', '\\n\\nWhile  the  split  between  main  memory  and  GPU  memory  has  disadvantages,\\nthere are advantages to being at a distance from the CPU.', '\\n\\nFor example, PTX exists in part because of the I/O device nature of GPUs.', '\\nThis  level  of  indirection  between  the  compiler  and  the  hardware  gives  GPU\\narchitects much more flexibility than system processor architects.', '  PTX  allows  GPU  architects  to  try  innovations\\nspeculatively and drop them in subsequent generations if they disappoint or fade\\nin importance, which encourages experimentation.', '\\n\\n\\x0c332 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nperformance makes it slower than a fast scalar processor as measured by the har-\\nmonic mean.', '\\n\\nFallacy On GPUs, just add more threads if you don’t have enough memory performance.', '\\n\\nGPUs use many CUDA threads to hide the latency to main memory.', '  Indeed,  we  can\\nalready see this emphasis in products, as both GPUs and traditional processors\\nhave been increasing the number of SIMD lanes at least as fast as they have been\\nadding processors (see Figure 4.', ' \\n\\nHence, we are seeing system processors take on more of the characteristics of\\nGPUs,  and  vice  versa.', '  One  of  the  biggest  differences  in  performance  between\\nconventional processors and GPUs has been for gather-scatter addressing.', '4,  the  GPU  question  is  not  simply\\nwhich architecture is best, but, given the hardware investment to do graphics well,\\nhow can it be enhanced to support computation that is more general? Although\\nvector  architectures  have  many  advantages  on  paper,  it  remains  to  be  proven\\nwhether vector architectures can be as good a foundation for graphics as GPUs.', ' \\n\\nGPU  SIMD  processors  and  compilers  are  still  of  relatively  simple  design.', '\\nTechniques  that  are  more  aggressive  will  likely  be  introduced  over  time  to\\nincrease GPU utilization, especially since GPU computing applications are just\\nstarting to be developed.', ' By studying these new programs, GPU designers will\\nsurely  discover  and  implement  new  machine  optimizations.', '  One  question  is\\nwhether the scalar processor (or control processor), which serves to save hard-\\nware and energy in vector processors, will appear within GPUs.', '\\n\\nThe Fermi architecture has already included many features found in conven-\\ntional processors to make GPUs more mainstream, but there are still others neces-\\nsary to close the gap.', '\\n\\n■ Virtualizable  GPUs.', ' For GPUs to be included\\nin the cloud, they will need to be just as virtualizable as the processors and\\nmemory that they are attached to.', '\\n\\n■ Relatively small size of GPU memory.', '  This  GPU  inconsistency  between  speed  and  size  can  be\\naddressed  with  more  memory  capacity.', '\\n\\n■ Direct I/O to GPU memory.', '  Today’s  GPU  systems  must  transfer  between  I/O  devices  and\\nsystem  memory  and  then  between  system  memory  and  GPU  memory.', '  This\\nextra  hop  significantly  lowers  I/O  performance  in  some  programs,  making\\nGPUs less attractive.', ' We expect that future GPUs\\nwill make all I/O first-class citizens, just as it does for frame buffer I/O today.', ' An alternative solution to the prior two bullets is\\nto have a single physical memory for the system and GPU, just as some inex-\\npensive  GPUs  do  for  PMDs  and  laptops.', '  The  AMD  Fusion  architecture,\\nannounced just as this edition was being finished, is an initial merger between\\ntraditional  GPUs  and  traditional  CPUs.', '  NVIDIA  also  announced  Project\\nDenver, which combines an ARM scalar processor with NVIDIA GPUs in a\\nsingle address space.', '\\n\\n\\x0c334 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\n4.', ' We also look at multimedia SIMD extensions and the history of GPUs.', ' Bakos\\n\\nCase Study: Implementing a Vector Kernel on a Vector \\nProcessor and GPU\\n\\nConcepts illustrated by this case study\\n\\n■ Programming Vector Processors\\n\\n■ Programming GPUs\\n\\n■ Performance Estimation\\n\\nMrBayes is a popular and well-known computational biology application for inferring\\nthe evolutionary histories among a set of input species based on their multiply-aligned\\nDNA sequence data  of  length n.', '  How  does  this  affect  the  way  you  can  write  the\\nVMIPS code for this kernel? Assume that you can initialize vector registers with\\nintegers using the following technique which would, for example, initialize vec-\\ntor register V1 with values (0,0,2000,2000):\\n\\nLI R2,0\\n\\nSW R2,vec\\n\\nSW R2,vec+4\\n\\nLI R2,2000\\n\\nSW R2,vec+8\\n\\nSW R2,vec+12\\n\\nLV V1,vec\\n\\n\\x0c336 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nAssume the maximum vector length is 64.', '4> Now assume we want to implement the MrBayes kernel on a GPU\\nusing  a  single  thread  block.', '4> How  well  do you expect  this  code to perform  on a GPU? Explain your\\nanswer.', '4> In this problem, we will compare the performance of a vector proces-\\nsor with a hybrid system that contains a scalar processor and a GPU-based copro-\\ncessor.', ' In the hybrid system, the host processor has superior scalar performance\\nto the GPU, so in this case all scalar code is executed on the host processor while\\nall vector code is executed on the GPU.', ' The vector processor has a peak memory bandwidth of 30 GB/sec and\\nthe GPU has a peak memory bandwidth of 150 GB/sec.', ' The hybrid system has an\\nadditional overhead that requires all input vectors to be transferred between the\\nhost memory and GPU local memory before and after the kernel is invoked.', ' Assume that both the vector processor and GPU are\\n\\n\\x0c338 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nperformance bound by memory bandwidth.', '5> When performing a reduction on a GPU, one thread is associ-\\nated with each element in the input vector.', ' Bakos\\n\\n■ 339\\n\\nonly the first of these guidelines and also uses the modulo operator which is\\nvery expensive for GPUs:\\n\\nunsigned int tid = threadIdx.', '4>  Assume  a  GPU  architecture  that  contains  10  SIMD  processors.', '\\nEach SIMD instruction has a width of 32 and each SIMD processor contains 8\\nlanes  for  single-precision  arithmetic  and  load/store  instructions,  meaning  that\\n\\n\\x0c340 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\neach  non-diverged  SIMD  instruction  can  produce  32  results  every  4  cycles.', ' Assume that\\nthe GPU has a clock speed of 1.', '4>  Compute  the  throughput,  in  GFLOP/sec,  for  this  kernel  on  this\\nGPU.', '4> List and describe at least four factors that influence the performance\\nof GPU kernels.', '4> Assume a hypothetical GPU with the following characteristics:\\n\\n 4.', '5 GHz\\n\\npoint units\\n\\n■ Contains 16 SIMD processors, each containing 16 single-precision floating-\\n\\n■ Has 100 GB/sec off-chip memory bandwidth\\n\\nWithout  considering  memory  bandwidth,  what  is  the  peak  single-precision\\nfloating-point throughput for this GPU in GLFOP/sec, assuming that all mem-\\nory latencies can be hidden? Is this throughput sustainable given the memory\\nbandwidth limitation?\\n\\n[60]  <4.', ' Use the NVIDIA CUDA Tool-\\nkit  along  with  GPU-SIM  from  the  University  of  British  Columbia  (http://\\nwww.', ' \\n\\nThreads can also be used to exploit data-level parallelism, although the over-\\nhead is likely to be higher than would be seen with an SIMD processor or with a\\nGPU (see Chapter 4).', ' For example, although a vector proces-\\nsor or GPU may be able to efficiently parallelize operations on short vectors, the\\nresulting grain size when the parallelism is split among many threads may be so\\nsmall  that  the  overhead  makes  the  exploitation  of  the  parallelism  prohibitively\\nexpensive in an MIMD.', '  At  the  present,\\ndealing effectively with modest numbers of homogeneous core strains is beyond\\nexisting compiler capability for many applications, but multiprocessors that have\\nheterogeneous cores with clear differences in functional capability and obvious\\nmethods to decompose an application are becoming more commonplace, includ-\\ning special processing units such as GPUs and media processors.', '\\n\\nSuch data suggest why the Fermi GPU (Chapter 4) adds ECC to its memory\\nwhere its predecessors didn’t even have parity protection.', '1\\n\\nIntroduction\\n\\n \\n\\n \\n\\nChapter  4  introduces  vector  architectures  and  places  Multimedia  SIMD  exten-\\nsions and GPUs in proper context to vector architectures.', ' If the even wider SIMD units of GPUs become\\nwell  integrated  with  the  scalar  cores,  including  scatter-gather  support,  we  may\\nwell conclude that vector architectures have won the architecture wars!\\n\\nThis historical perspective adds some details and references that were left out of\\nthe version in Chapter 4.', ' “Believe It or Not!: Multi-core CPUs can\\nMatch GPU Performance for a FLOP-Intensive Application!” 19th International Con-\\nference on Parallel Architecture and Compilation Techniques (PACT 2010).', '  “Debunking  the  100X  GPU  vs.', '  CPU  myth:  An  evaluation  of\\nthroughput  computing  on  CPU  and  GPU,”  Proc.', '\\n\\nRajesh Bordawekar, Uday Bondhugula, Ravi Rao: Believe it or not!: mult-core CPUs can\\nmatch GPU performance for a FLOP-intensive application! 19th International Confer-\\nence  on  Parallel  Architecture  and  Compilation  Techniques  (PACT  2010),  Vienna,\\nAustria, September 11-15, 2010: 537-538.', ' deterministic routing, F-52 to \\nF-55, F-54\\n\\nnetwork fault tolerance, F-94\\nand overhead, F-93 to F-94\\n\\nAdders\\n\\ncarry-lookahead, J-37 to J-41\\nchip comparison, J-60\\nfull, J-2, J-3\\nhalf, J-2\\ninteger division speedup, J-54 to \\n\\nJ-58\\n\\ninteger multiplication speedup\\n\\neven/odd array, J-52\\nmany adders, J-50, J-50 to J-54\\nmultipass array multiplier, J-51\\nsigned-digit addition table, \\n\\nJ-54\\n\\nto J-49\\n\\nWallace tree, J-53\\nradix-2 division, J-55\\nradix-4 division, J-56\\nradix-4 SRT division, J-57\\nripple-carry, J-3, J-3\\n\\ncarry-lookahead, J-37 to J-41\\ncarry-lookahead circuit, J-38\\ncarry-lookahead tree, J-40\\ncarry-lookahead tree adder, \\n\\ncarry-select adder, J-43, J-43 to \\n\\nJ-41\\n\\nJ-44, J-44\\n\\ncarry-skip adder, J-41 to J43, \\n\\nJ-42\\n\\noverview, J-37\\n\\nripply-carry addition, J-3\\n\\nAddress aliasing prediction\\n\\ndefinition, 213\\nideal processor, 214\\nILP for realizable processors, 216\\n\\nAddress Coalescing Unit\\n\\nfunction, 310\\ngather-scatter, 329\\nGPUs, 300\\nMultithreaded SIMD Processor \\n\\nblock diagram, 294\\n\\nvector processor, 310\\nAddress fault, virtual memory \\ndefinition, B-42\\n\\ncomparison, A-11\\ncompiler writing-architecture \\nrelationship, A-30\\n\\ncontrol flow instructions, A-17 to \\n\\nA-18\\ndesktop architectures, K-5\\n\\nI-1\\n\\nsingle adder, J-47 to J-49, J-48 \\n\\nAddressing modes\\n\\n\\x0cI-2 ■\\n\\nIndex\\n\\nAddressing modes (continued )\\ndisplacement mode, A-10\\nembedded architectures, K-6\\ninstruction set encoding, A-21\\nIntel 80x86, K-47 to K-49, K-58 to \\n\\nK-59, K-59 to K-60\\n\\nIntel 80x86 operands, K-59\\nISA, 11–12, A-9 to A-10\\nMIPS data transfers, A-34\\nRISC architectures, K-5 to K-6\\nselection, A-9\\nVAX, K-66 to K-68, K-71\\nVAX instruction encoding, K-68 to \\n\\nK-69\\nAddress offset, virtual memory, \\nB-56\\n\\nAddress space\\n\\nFermi GPU architecture, 306–307\\nmemory hierarchy, B-48 to B-49, \\nB-57 to B-58\\nMultimedia SIMD vs.', ' GPUs, 312\\nSMP/DSM shared memory, 348\\nvirtual memory, B-40 to B-41\\n\\nAddress specifier\\n\\ninstruction set encoding, A-21\\nVAX instruction encoding, K-68 to \\n\\nK-69\\n\\nAddress stage, TI 320C55 DSP, E-7\\nAddress trace, cache performance, B-4\\nAddress translation\\n\\nAMD64 paged virtual memory, \\nB-55 to B-56\\n\\nduring indexing, B-36 to B-40\\nmemory hierarchy basics, 77–78\\nOpteron data TLB, B-47\\nvirtual memory, B-46\\nvirtual memory definition, B-42\\nvirtual memory protection, 106\\n\\nAdministrative costs, WSC vs.', ' success, A-45\\nGPU computing history, L-52\\npower consumption, F-85\\nrecent advances, L-33\\nRISC history, L-22\\nshared-memory multiprogramming \\nworkload, 378\\n\\nterminology, 313–315\\ntournament predictors, 164\\nVirtual Machines, 110\\nVMMs, 129\\n\\nstudy, D-64 to D-67\\n\\nAMPS, see Advanced mobile phone \\n\\nservice (AMPS)\\n\\nAndreessen, Marc, F-98\\nAndroid OS, 324\\nAnnulling delayed branch, \\n\\nAntialiasing, address translation, B-38\\nAntidependences\\n\\ncompiler history, L-30 to L-31\\ndefinition, 152\\nfinding, H-7 to H-8\\nloop-level parallelism calculations, \\n\\n320\\n\\nMIPS scoreboarding, C-72, C-79\\n\\nApogee Software, A-44\\nApollo DN 10000, L-30\\nApple iPad\\n\\nARM Cortex-A8, 114\\nmemory hierarchy basics, 78\\nApplication binary interface (ABI), \\n\\ncontrol flow \\ninstructions, A-20\\n\\nApplication layer, definition, F-82\\nApplied Minds, L-74\\nArbitration algorithm\\n\\ncollision detection, F-23\\ncommercial interconnection \\n\\nnetworks, F-56\\n\\nexamples, F-49\\nIntel SCCC, F-70\\ninterconnection networks, F-21 to \\nF-22, F-27, F-49 to F-50\\n\\nnetwork impact, F-52 to F-55\\nSAN characteristics, F-76\\nswitched-media networks, F-24\\nswitch microarchitecture, F-57 to \\n\\nF-58\\nswitch microarchitecture \\n\\npipelining, F-60\\n\\nsystem area network history, F-100\\nArchitect-compiler writer relationship, \\n\\nA-29 to A-30\\n\\nArchitecturally visible registers, \\n\\nregister renaming vs.', ' \\nROB, 208–209\\n\\nArchitectural Support for Compilers \\nand Operating Systems \\n(ASPLOS), L-11\\n\\narchitecture; CUDA \\n(Compute Unified \\nDevice Architecture); \\nInstruction set \\narchitecture (ISA); \\nVector architectures\\n\\nAmortization of overhead, sorting case \\n\\nArchitecture, see also Computer \\n\\nIndex\\n\\n■\\n\\nI-3\\n\\ndefinition, 15\\nheterogeneous, 262\\nmicroarchitecture, 15–16, 247–254\\nstack, A-3, A-27, A-44 to A-45\\n\\nAreal density, disk storage, D-2\\nArgument pointer, VAX, K-71\\nArithmetic intensity\\n\\nas FP operation, 286, 286–288\\nRoofline model, 326, 326–327\\n\\nArithmetic/logical instructions\\ndesktop RISCs, K-11, K-22\\nembedded RISCs, K-15, K-24\\nIntel 80x86, K-49, K-53\\nSPARC, K-31\\nVAX, B-73\\n\\nArithmetic-logical units (ALUs)\\nARM Cortex-A8, 234, 236\\nbasic MIPS pipeline, C-36\\nbranch condition evaluation, A-19\\ndata forwarding, C-40 to C-41\\ndata hazards requiring stalls, C-19 \\n\\ndata hazard stall minimization, \\n\\nto C-20\\n\\nC-17 to C-19\\n\\nDSP media extensions, E-10\\neffective address cycle, C-6\\nhardware-based execution, 185\\nhardware-based speculation, \\n\\n200–201, 201\\nIA-64 instructions, H-35\\nimmediate operands, A-12\\ninteger division, J-54\\ninteger multiplication, J-48\\ninteger shifting over zeros, J-45 to \\n\\nJ-46\\n\\nIntel Core i7, 238\\nISA operands, A-4 to A-5\\nISA performance and efficiency \\n\\nprediction, 241\\n\\nload interlocks, C-39\\nmicroarchitectural techniques case \\n\\nstudy, 253\\nMIPS operations, A-35, A-37\\nMIPS pipeline control, C-38 to C-39\\nMIPS pipeline FP operations, C-52 \\n\\nto C-53\\nMIPS R4000, C-65\\noperand forwarding, C-19\\noperands per instruction example, \\n\\ninstructions, K-25\\n\\ncompiler writer-architect \\n\\nAntenna, radio receiver, E-23\\n\\nrelationship, A-29 to A-30\\n\\nA-6\\nparallelism, 45\\n\\n\\x0cpipeline execution rate, C-10 to \\n\\nK-24\\n\\nI-4 ■\\n\\nIndex\\n\\nArithmetic-logical units (continued )\\npipeline branch issues, C-39 to \\n\\nC-41\\n\\nC-11\\n\\npower/DLP issues, 322\\nRISC architectures, K-5\\nRISC classic pipeline, C-7\\nRISC instruction set, C-4\\nsimple MIPS implementation, \\n\\nC-31 to C-33\\n\\nTX-2, L-49\\n\\nARM (Advanced RISC Machine)\\naddressing modes, K-5, K-6\\narithmetic/logical instructions, \\nK-15, K-24\\n\\ncharacteristics, K-4\\ncondition codes, K-12 to K-13\\nconstant extension, K-9\\ncontrol flow instructions, 14\\ndata transfer instructions, K-23\\nembedded instruction format, K-8\\nGPU computing history, L-52\\nISA class, 11\\nmemory addressing, 11\\nmultiply-accumulate, K-20\\noperands, 12\\nRISC instruction set lineage, K-43\\nunique instructions, K-36 to K-37\\n\\nARM AMBA, OCNs, F-3\\nARM Cortex-A8\\n\\ndynamic scheduling, 170\\nILP concepts, 148\\ninstruction decode, 234\\nISA performance and efficiency \\n\\nprediction, 241–243\\n\\nmemory access penalty, 117\\nmemory hierarchy design, 78, \\n\\n114–117, 115\\n\\nmemory performance, 115–117\\nmultibanked caches, 86\\noverview, 233\\npipeline performance, 233–236, \\n\\n235\\n\\npipeline structure, 232\\nprocessor comparison, 242\\nway prediction, 81\\n\\nARM Cortex-A9\\n\\nvs.', ' server GPUs, \\n323–324, 324\\n\\nARM Thumb\\n\\nASPLOS, see Architectural Support \\n\\naddressing modes, K-6\\narithmetic/logical instructions, \\n\\ncharacteristics, K-4\\ncondition codes, K-14\\nconstant extension, K-9\\ndata transfer instructions, K-23\\nembedded instruction format, K-8\\nISAs, 14\\nmultiply-accumulate, K-20\\nRISC code size, A-23\\nunique instructions, K-37 to K-38\\nARPA (Advanced Research Project \\nAgency)\\nLAN history, F-99 to F-100\\nWAN history, F-97\\n\\nARPANET, WAN history, F-97 to \\n\\nF-98\\n\\nArray multiplier\\n\\nexample, J-50\\nintegers, J-50\\nmultipass system, J-51\\n\\nArrays\\n\\naccess age, 91\\nblocking, 89–90\\nbubble sort procedure, K-76\\ncluster server outage/anomaly \\n\\nstatistics, 435\\n\\nexamples, 90\\nFFT kernel, I-7\\nGoogle WSC servers, 469\\nLayer 3 network linkage, 445\\nloop interchange, 88–89\\nloop-level parallelism \\n\\nocean application, I-9 to I-10\\nrecurrences, H-12\\nWSC memory hierarchy, 445\\nWSCs, 443\\n\\nArray switch, WSCs, 443–444\\nASC, see Advanced Simulation and \\n\\nComputing (ASC) \\nprogram\\n\\nASCI, see Accelerated Strategic \\n\\nComputing Initiative \\n(ASCI)\\nASCII character format, 12, A-14\\nASC Purple, F-67, F-100\\nASI, see Advanced Switching \\n\\nInterconnect (ASI)\\n\\nfor Compilers and \\nOperating Systems \\n(ASPLOS)\\n\\nAssembly language, 2\\nAssociation of Computing Machinery \\n(ACM), L-3\\nAssociativity, see also Set \\nassociativity\\ncache block, B-9 to B-10, B-10\\ncache optimization, B-22 to B-24, \\n\\nB-26, B-28 to B-30\\n\\ncloud computing, 460–461\\nloop-level parallelism, 322\\nmultilevel inclusion, 398\\nOpteron data cache, B-14\\nshared-memory multiprocessors, \\n\\nAstronautics ZS-1, L-29\\nAsynchronous events, exception \\n\\nrequirements, C-44 to \\nC-45\\n\\nAsynchronous I/O, storage systems, \\n\\n368\\n\\nD-35\\n\\nAsynchronous Transfer Mode (ATM)\\ninterconnection networks, F-89\\nLAN history, F-99\\npacket format, F-75\\ntotal time statistics, F-90\\nVOQs, F-60\\nas WAN, F-79\\nWAN history, F-98\\nWANs, F-4\\n\\nATA (Advanced Technology \\n\\nAttachment) disks\\n\\nD-12\\n\\ndisk storage, D-4\\nhistorical background, L-81\\npower, D-5\\nRAID 6, D-9\\nserver energy savings, 25\\n\\nAtanasoff, John, L-5\\nAtanasoff Berry Computer (ABC), L-5\\nATI Radeon 9700, L-51\\nAtlas computer, L-9\\nATM, see Asynchronous Transfer \\n\\nMode (ATM)\\n\\nATM systems\\n\\nserver benchmarks, 41\\nTP benchmarks, D-18\\n\\ndependences, 318–319\\n\\nBerkeley’s Tertiary Disk project, \\n\\n\\x0cperformance, 229\\n\\nAverage reception factor\\n\\ntwo-device networks, F-12 to \\n\\nAtomicity-consistency-isolation-durab\\n\\ncentralized switched networks, \\n\\nAtomic exchange\\n\\nlock implementation, 389–390\\nsynchronization, 387–388\\n\\nAtomic instructions\\n\\nbarrier synchronization, I-14\\nCore i7, 329\\nFermi GPU, 308\\nT1 multithreading unicore \\n\\ncentralized shared-memory \\n\\narchitectures, 351–352\\n\\ndefinition, B-30 to B-31\\nmemory hierarchy basics, 75–76\\nmiss penalty reduction, B-32\\nvia miss rates, B-29, B-29 to B-30\\nas processor performance \\n\\npredictor, B-17 to B-20\\n\\ncommercial interconnection \\n\\nBackside bus, centralized \\n\\nility (ACID), vs.', ' WSC \\nstorage, 439\\n\\nAtomic operations\\n\\ncache coherence, 360–361\\nsnooping cache coherence \\n\\nimplementation, 365\\n\\n“Atomic swap,” definition, K-20\\nAttributes field, IA-32 descriptor \\n\\nAutoincrement deferred addressing, \\n\\ntable, B-52\\n\\nVAX, K-67\\n\\nAutonet, F-48\\nAvailability\\n\\nnetworks, F-66\\n\\ncomputer architecture, 11, 15\\ncomputer systems, D-43 to D-44, \\n\\ndata on Internet, 344\\nfault detection, 57–58\\nI/O system design/evaluation, \\n\\nD-44\\n\\nD-36\\n\\nloop-level parallelism, 217–218\\nmainstream computing classes, 5\\nmodules, 34\\nopen-source software, 457\\nRAID systems, 60\\nas server characteristic, 7\\nservers, 16\\nsource operands, C-74\\nWSCs, 8, 433–435, 438–439\\nAverage instruction execution time, \\n\\nL-6\\nAverage Memory Access Time \\n\\n(AMAT)\\nblock size calculations, B-26 to \\n\\nB-28\\n\\ncache optimizations, B-22, B-26 to \\n\\nB-32, B-36\\ncache performance, B-16 to B-21\\ncalculation, B-16 to B-17\\n\\nF-32\\n\\nmulti-device interconnection \\n\\nnetworks, F-26\\nAVX, see Advanced Vector \\n\\nExtensions (AVX)\\n\\nAWS, see Amazon Web Services \\n(AWS)\\n\\nB\\nBack-off time, shared-media \\nnetworks, F-23\\n\\nBackpressure, congestion \\n\\nmanagement, F-65\\n\\nshared-memory \\nmultiprocessors, 351\\n\\nBalanced systems, sorting case study, \\n\\nD-64 to D-67\\n\\nBalanced tree, MINs with nonblicking, \\n\\nF-34\\nBandwidth, see also Throughput\\n\\narbitration, F-49\\nand cache miss, B-2 to B-3\\ncentralized shared-memory \\nmultiprocessors, \\n351–352\\ncommunication mechanism, I-3\\ncongestion management, F-64 to \\n\\nF-65\\nCray Research T3D, F-87\\nDDR DRAMS and DIMMS, 101\\ndefinition, F-13\\nDSM architecture, 379\\nEthernet and bridges, F-78\\nFP arithmetic, J-62\\nGDRAM, 322–323\\nGPU computation, 327–328\\nGPU Memory, 327\\nILP instruction fetch\\n\\nbasic considerations, 202–203\\nbranch-target buffers, 203–206\\n\\nIndex\\n\\n■\\n\\nI-5\\n\\nintegrated units, 207–208\\nreturn address predictors, \\n\\n206–207\\ninterconnection networks, F-28\\n\\nmulti-device networks, F-25 to \\n\\nperformance considerations, \\n\\nF-29\\n\\nF-89\\n\\nF-20\\n\\nvs.', ' TCP/IP reliance, F-95\\nand topology, F-39\\nvector load/store units, 276–277\\nWSC memory hierarchy, 443–444, \\n\\n444\\nBandwidth gap, disk storage, D-3\\nBanerjee, Uptal, L-30 to L-31\\nBank busy time, vector memory \\n\\nsystems, G-9\\n\\nBanked memory, see also Memory \\nbanks\\n\\nand graphics memory, 322–323\\nvector architectures, G-10\\n\\nBanks, Fermi GPUs, 297\\nBarcelona Supercomputer Center, \\nF-76\\n\\nBarnes\\n\\ncharacteristics, I-8 to I-9\\ndistributed-memory \\n\\nmultiprocessor, I-32\\n\\nsymmetric shared-memory \\n\\nmultiprocessors, I-22, \\nI-23, I-25\\n\\n\\x0cI-6 ■\\n\\nIndex\\n\\ndefinition, C-45\\n\\nBlock addressing\\n\\nBarnes-Hut n-body algorithm, basic \\n\\nexample, F-33\\n\\nconcept, I-8 to I-9\\n\\nBarriers\\n\\ncommercial workloads, 370\\nCray X1, G-23\\nfetch-and-increment, I-20 to I-21\\nhardware primitives, 387\\nlarge-scale multiprocessor \\n\\nsynchronization, I-13 to \\nI-16, I-14, I-16, I-19, \\nI-20\\n\\nBER, see Bit error rate (BER)\\nBerkeley’s Tertiary Disk project\\n\\nfailure statistics, D-13\\noverview, D-12\\nsystem log, D-43\\nBerners-Lee, Tim, F-98\\nBertram, Jack, L-28\\nBest-case lower bounds, multi-device \\n\\ninterconnection \\nnetworks, F-25\\n\\nsynchronization, 298, 313, 329\\nBARRNet, see Bay Area Research \\nNetwork (BARRNet)\\n\\nBased indexed addressing mode, Intel \\n\\n80x86, K-49, K-58\\n\\nBest-case upper bounds\\n\\nmulti-device interconnection \\n\\nnetworks, F-26\\nnetwork performance and \\ntopology, F-41\\n\\nBase field, IA-32 descriptor table, \\n\\nBetween instruction exceptions, \\n\\nB-52 to B-53\\n\\nBase station\\n\\ncell phones, E-23\\nwireless networks, E-22\\n\\nBasic block, ILP, 149\\nBatch processing workloads\\n\\nWSC goals/requirements, 433\\nWSC MapReduce and Hadoop, \\n437–438\\n\\nBay Area Research Network \\n\\nBBN Butterfly, L-60\\nBBN Monarch, L-60\\nBefore rounding rule, J-36\\nBenchmarking, see also specific \\n\\nbenchmark suites\\n\\ndesktop, 38–40\\nEEMBC, E-12\\nembedded applications\\n\\nbasic considerations, E-12\\npower consumption and \\n\\nefficiency, E-13\\n\\nfallacies, 56\\ninstruction set operations, A-15\\nas performance measurement, \\n\\nreal-world server considerations, \\n\\n37–41\\n\\n52–55\\n\\nresponse time restrictions, D-18\\nserver performance, 40–41\\nsorting case study, D-64 to D-67\\n\\nBenesˆ topology\\n\\ncentralized switched networks, \\n\\nF-33\\n\\nBiased exponent, J-15\\nBidirectional multistage \\n\\ninterconnection \\nnetworks\\n\\n Benesˆ topology, F-33\\ncharacteristics, F-33 to F-34\\nSAN characteristics, F-76\\n\\nBidirectional rings, topology, F-35 to \\n\\nF-36\\n\\nA-7\\n\\nMIPS core extensions, K-20 to \\n\\nK-21\\nMIPS data transfers, A-34\\n\\nBigtable (Google), 438, 441\\nBINAC, L-5\\nBinary code compatibility\\n\\nembedded systems, E-15\\nVLIW processors, 196\\n\\nBinary-coded decimal, definition, A-14\\nBinary-to-decimal conversion, FP \\n\\nprecisions, J-34\\n\\nBing search\\n\\ndelays and user behavior, 451\\nlatency effects, 450–452\\nWSC processor cost-performance, \\n\\n473\\n\\nBisection bandwidth\\n\\nas network cost constraint, F-89\\nnetwork performance and \\ntopology, F-41\\n\\nNEWS communication, F-42\\ntopology, F-39\\n\\nBisection bandwidth, WSC array \\n\\nswitch, 443\\n\\nBisection traffic fraction, network \\n\\nperformance and \\ntopology, F-41\\nBit error rate (BER), wireless \\nnetworks, E-21\\n\\nBit rot, case study, D-61 to D-64\\nBit selection, block placement, B-7\\nBlack box network\\n\\nbasic concept, F-5 to F-6\\neffective bandwidth, F-17\\nperformance, F-12\\nswitched-media networks, F-24\\nswitched network topologies, F-40\\n\\nblock identification, B-7 to B-8\\ninterleaved cache banks, 86\\nmemory hierarchy basics, 74\\nBlocked floating point arithmetic, \\n\\nDSP, E-6\\n\\nBlock identification\\n\\nmemory hierarchy considerations, \\n\\nB-7 to B-9\\nvirtual memory, B-44 to B-45\\n\\nF-32\\n\\ndirect networks, F-38\\nHOL, see Head-of-line (HOL) \\nblocking\\n\\nnetwork performance and \\ntopology, F-41\\n\\nBlocking calls, shared-memory \\n\\nmultiprocessor \\nworkload, 369\\n\\nBlocking factor, definition, 90\\nBlock multithreading, definition, \\nL-34\\n\\nBlock offset\\n\\nblock identification, B-7 to B-8\\ncache optimization, B-38\\ndefinition, B-7 to B-8\\ndirect-mapped cache, B-9\\nexample, B-9\\nmain memory, B-44\\nOpteron data cache, B-13, B-13 to \\n\\nB-14\\n\\n(BARRNet), F-80\\n\\nBig Endian\\n\\nBlocking\\n\\ninterconnection networks, F-12\\nmemory address interpretation, \\n\\nbenchmark fallacies, 56\\ncentralized switched networks, \\n\\n\\x0cBranches\\n\\nBranch-prediction buffers, basic \\n\\nBlock placement\\n\\nmemory hierarchy considerations, \\n\\nB-7\\n\\nvirtual memory, B-44\\n\\nBlock replacement\\n\\nmemory hierarchy considerations, \\nB-9 to B-10\\nvirtual memory, B-45\\n\\nBlocks, see also Cache block; Thread \\n\\nBlock\\n\\nARM Cortex-A8, 115\\nvs.', ' miss rate, B-27\\n\\nBlock transfer engine (BLT)\\nCray Research T3D, F-87\\ninterconnection network \\n\\nprotection, F-87\\n\\nBLT, see Block transfer engine (BLT)\\nBody of Vectorized Loop\\ndefinition, 292, 313\\nGPU hardware, 295–296, 311\\nGPU Memory structure, 304\\nNVIDIA GPU, 296\\nSIMD Lane Registers, 314\\nThread Block Scheduler, 314\\n\\nBoggs, David, F-99\\nBOMB, L-4\\nBooth recoding, J-8 to J-9, J-9, J-10 to \\n\\nJ-11\\n\\nchip comparison, J-60 to J-61\\ninteger multiplication, J-49\\n\\nBose-Einstein formula, definition, 30\\nBounds checking, segmented virtual \\n\\nmemory, B-52\\n\\nBranch byte, VAX, K-71\\nBranch delay slot\\n\\ncharacteristics, C-23 to C-25\\ncontrol hazards, C-41\\nMIPS R4000, C-64\\nscheduling, C-24\\n\\ncanceling, C-24 to C-25\\nconditional branches, 300–303, \\nA-17, A-19 to A-20, \\nA-21\\n\\ncontrol flow instructions, A-16, \\n\\nA-18\\n\\ndelayed, C-23\\ndelay slot, C-65\\nIBM 360, K-86 to K-87\\ninstructions, K-25\\nMIPS control flow instructions, \\n\\nA-38\\n\\nMIPS operations, A-35\\nnullifying, C-24 to C-25\\nRISC instruction set, C-5\\nVAX, K-71 to K-72\\nWCET, E-4\\n\\nBranch folding, definition, 206\\nBranch hazards\\n\\nbasic considerations, C-21\\npenalty reduction, C-22 to C-25\\npipeline issues, C-39 to C-42\\nscheme performance, C-25 to C-26\\nstall reduction, C-42\\n\\nC-27 to C-30\\n\\nBranch offsets, control flow \\n\\ninstructions, A-18\\n\\nBranch penalty\\n\\nexamples, 205\\ninstruction fetch bandwidth, \\n203–206\\n\\nreduction, C-22 to C-25\\nsimple scheme examples, C-25\\n\\nBranch prediction\\naccuracy, C-30\\nbranch cost reduction, 162–167\\ncorrelation, 162–164\\ncost reduction, C-26\\ndynamic, C-27 to C-30\\n\\nIndex\\n\\n■\\n\\nI-7\\n\\nearly schemes, L-27 to L-28\\nideal processor, 214\\nILP exploitation, 201\\ninstruction fetch bandwidth, 205\\nintegrated instruction fetch units, \\n\\n207\\n\\nIntel Core i7, 166–167, 239–241\\nmisprediction rates on SPEC89, 166\\nstatic, C-26 to C-27\\ntrace scheduling, H-19\\ntwo-bit predictor comparison, 165\\n\\nconsiderations, C-27 to \\nC-30, C-29\\n\\nBranch registers\\nIA-64, H-34\\nPowerPC instructions, K-32 to K-33\\nBranch stalls, MIPS R4000 pipeline, \\n\\nC-67\\n\\nBranch-target address\\n\\nbranch hazards, C-42\\nMIPS control flow instructions, \\n\\nA-38\\nMIPS pipeline, C-36, C-37\\nMIPS R4000, C-25\\npipeline branches, C-39\\nRISC instruction set, C-5\\n\\nBranch-target buffers\\n\\nARM Cortex-A8, 233\\nbranch hazard stalls, C-42\\nexample, 203\\ninstruction fetch bandwidth, \\n203–206\\n\\ninstruction handling, 204\\nMIPS control flow instructions, \\n\\nA-38\\n\\nbuffers\\n\\nBrewer, Eric, L-73\\nBridges\\n\\nand bandwidth, F-78\\ndefinition, F-78\\n\\nBubbles\\n\\nand deadlock, F-47\\nrouting comparison, F-54\\nstall as, C-13\\n\\nBubble sort, code example, K-76\\nBuckets, D-26\\nBuffered crossbar switch, switch \\n\\nmicroarchitecture, F-62\\n\\nBuffered wormhole switching, \\nF-51\\n\\nBranch history table, basic scheme, \\n\\nBranch-target cache, see Branch-target \\n\\n\\x0cBuffers\\n\\nTomasulo’s algorithm, 180, 182\\n\\nbranch-prediction, C-27 to C-30, \\n\\nBypassing, see also Forwarding\\n\\nC-29\\n\\ndata hazards requiring stalls, C-19 \\n\\nI-8 ■\\n\\nIndex\\n\\nbranch-target, 203–206, 204, 233, \\nA-38, C-42\\n\\nDSM multiprocessor cache \\n\\ncoherence, I-38 to I-40\\n\\nIntel SCCC, F-70\\ninterconnection networks, F-10 to \\n\\nF-11\\n\\nmemory, 208\\nMIPS scoreboarding, C-74\\nnetwork interface functions, F-7\\nROB, 184–192, 188–189, 199, \\n\\n208–210, 238\\nswitch microarchitecture, F-58 to \\n\\nF-60\\n\\nTLB, see Translation lookaside \\nbuffer (TLB)\\n\\ntranslation buffer, B-45 to B-46\\nwrite buffer, B-11, B-14, B-32, \\n\\nB-35 to B-36\\n\\nBundles\\n\\nIA-64, H-34 to H-35, H-37\\nItanium 2, H-41\\nBurks, Arthur, L-3\\nBurroughs B5000, L-16\\nBus-based coherent multiprocessors, \\n\\nL-59 to L-60\\n\\nBuses\\n\\nbarrier synchronization, I-16\\ncache coherence, 391\\ncentralized shared-memory \\n\\nmultiprocessors, 351\\n\\ndefinition, 351\\ndynamic scheduling with \\n\\nTomasulo’s algorithm, \\n172, 175\\n\\nGoogle WSC servers, 469\\nI/O bus replacements, D-34, D-34\\nlarge-scale multiprocessor \\n\\nsynchronization, I-12 to \\nI-13\\n\\nNEWS communication, F-42\\nscientific workloads on symmetric \\nshared-memory \\nmultiprocessors, I-25\\n\\nSony PlayStation 2 Emotion \\n\\nEngine, E-18\\n\\nvs.', ' block size, 378\\n\\nByte/word/long displacement \\n\\ndeferred addressing, \\nVAX, K-67\\n\\nC\\nCAC, see Computer aided design \\n\\n(CAD) tools\\n\\nCache bandwidth\\ncaches, 78\\nmultibanked caches, 85–86\\nnonblocking caches, 83–85\\npipelined cache access, 82\\n\\nCache block\\n\\nAMD Opteron data cache, B-13, \\nB-13 to B-14\\ncache coherence protocol, 357–358\\ncompiler optimizations, 89–90\\ncritical word first, 86–87\\ndefinition, B-2\\ndirectory-based cache coherence \\nprotocol, 382–386, 383\\n\\nfalse sharing, 366\\nGPU comparisons, 329\\ninclusion, 397–398\\nmemory block, B-61\\nmiss categories, B-26\\nmiss rate reduction, B-26 to B-28\\nscientific workloads on symmetric \\nshared-memory \\n\\nmultiprocessors, I-22, \\nI-25, I-25\\n\\nshared-memory multiprogramming \\nworkload, 375–377, 376\\n\\nway prediction, 81\\nwrite invalidate protocol \\n\\nimplementation, \\n356–357\\nwrite strategy, B-10\\n\\nadvanced directory protocol case \\n\\nstudy, 420–426\\n\\nbasic considerations, 112–113\\nCray X1, G-22\\ndirectory-based, see \\n\\nDirectory-based cache \\ncoherence\\n\\nenforcement, 354–355\\nextensions, 362–363\\nhardware primitives, 388\\nIntel SCCC, F-70\\nlarge-scale multiprocessor history, \\n\\nL-61\\nlarge-scale multiprocessors\\n\\ndeadlock and buffering, I-38 to \\n\\nI-40\\n\\nI-41\\n\\nI-37\\n\\n396\\n\\ndirectory controller, I-40 to \\n\\nDSM implementation, I-36 to \\n\\noverview, I-34 to I-36\\n\\nlatency hiding with speculation, \\n\\nlock implementation, 389–391\\nmechanism, 358\\nmemory hierarchy basics, 75\\nmultiprocessor-optimized \\n\\nsoftware, 409\\n\\nmultiprocessors, 352–353\\nprotocol definitions, 354–355\\nsingle-chip multicore processor \\ncase study, 412–418\\n\\nsingle memory location example, \\n\\n352\\n\\nsnooping, see Snooping cache \\ncoherence\\n\\nstate diagram, 361\\nsteps and bus traffic examples, 391\\nwrite-back cache, 360\\n\\nCache definition, B-2\\nCache hit\\n\\nAMD Opteron example, B-14\\n\\n\\x0caverage memory access time, B-16 \\n\\nsymmetric shared-memory \\n\\nCache prefetch, cache optimization, 92\\nCaches, see also Memory hierarchy\\naccess time vs.', ' writes, B-35 to \\n\\nB-35\\n\\nB-36\\n\\nmiss rate reduction\\n\\nvia associativity, B-28 to B-30\\nvia block size, B-26 to B-28\\nvia cache size, B-28\\n\\nmultibanked caches, 85–86, 86\\nnonblocking caches, 83–85, 84\\noverview, 78–79\\npipelined cache access, 82\\nsimple first-level caches, 79–80\\ntechniques overview, 96\\nway prediction, 81–82\\nwrite buffer merging, 87, 88\\n\\nCache organization\\nblocks, B-7, B-8\\nOpteron data cache, B-12 to B-13, \\n\\nB-13\\n\\noptimization, B-19\\nperformance impact, B-19\\n\\nCache performance\\n\\nto B-20\\n\\nB-16\\n\\nbasic considerations, B-3 to B-6, \\n\\nbasic equations, B-22\\nbasic optimizations, B-40\\ncache optimization, 96\\ncase study, 131–133\\nexample calculation, B-16 to B-17\\nout-of-order processors, B-20 to \\n\\nB-22\\n\\nprediction, 125–126\\n\\nB-15, B-13, B-15\\n\\nbasic considerations, B-48 to B-49\\ncoining of term, L-11\\ndefinition, B-2\\nearly work, L-10\\nembedded systems, E-4 to E-5\\nFermi GPU architecture, 306\\nideal processor, 214\\nILP for realizable processors, \\n216–218\\n\\nItanium 2, H-42\\nmultichip multicore \\n\\nmultiprocessor, 419\\n\\nparameter ranges, B-42\\nSony PlayStation 2 Emotion \\n\\nEngine, E-18\\nvector processors, G-25\\nvs.', ' miss rate, B-27\\n\\nCache size\\n\\nIndex\\n\\n■\\n\\nI-9\\n\\nmiss rate reduction, B-28\\nmultilevel caches, B-33\\nand relative execution time, B-34\\nscientific workloads\\n\\ndistributed-memory \\n\\nmultiprocessors, I-29 to \\nI-31\\n\\nmultiprocessors, I-22 to \\nI-23, I-24\\n\\nshared-memory multiprogramming \\nworkload, 376\\nvirtually addressed, B-37\\n\\nCACTI\\n\\ncache optimization, 79–80, 81\\nmemory access times, 77\\n\\nCaller saving, control flow \\n\\ninstructions, A-19 to \\nA-20\\n\\nIA-32 segment descriptors, B-53\\nsegmented virtual memory, B-54\\n\\ncompiler structure, A-25 to A-26\\ncontrol flow instructions, A-17, \\nA-19 to A-21\\n\\nCUDA Thread, 297\\ndependence analysis, 321\\nhigh-level instruction set, A-42 to \\n\\nIntel 80x86 integer operations, \\n\\nK-51\\ninvocation options, A-19\\nISAs, 14\\nMIPS control flow instructions, \\n\\nMIPS registers, 12\\nmultiprogrammed workload, \\n\\nA-43\\n\\nA-38\\n\\n378\\n\\nNVIDIA GPU Memory structures, \\n\\n304–305\\nreturn address predictors, 206\\nshared-memory multiprocessor \\n\\nworkload, 369\\n\\nuser-to-OS gates, B-54\\nVAX, K-71 to K-72\\n\\nCanceling branch, branch delay slots, \\n\\nC-24 to C-25\\nCanonical form, AMD64 paged virtual \\n\\nmemory, B-55\\n\\nCapabilities, protection schemes, L-9 \\nto L-10\\n\\n\\x0cI-10 ■\\n\\nIndex\\n\\nCapacity misses\\n\\nblocking, 89–90\\nand cache size, B-24\\ndefinition, B-23\\nmemory hierarchy basics, 75\\nscientific workloads on symmetric \\nshared-memory \\nmultiprocessors, I-22, \\nI-23, I-24\\nshared-memory workload, 373\\nCAPEX, see Capital expenditures \\n\\n(CAPEX)\\n\\nCapital expenditures (CAPEX)\\nWSC costs, 452–455, 453\\nWSC Flash memory, 475\\nWSC TCO case study, 476–478\\n\\nCarrier sensing, shared-media \\n\\nnetworks, F-23\\n\\nCarrier signal, wireless networks, \\nE-21\\n\\nCarry condition code, MIPS core, K-9 \\n\\nto K-16\\n\\nCarry-in, carry-skip adder, J-42\\nCarry-lookahead adder (CLA)\\nchip comparison, J-60\\nearly computer arithmetic, J-63\\nexample, J-38\\ninteger addition speedup, J-37 to \\n\\nwith ripple-carry adder, J-42\\ntree, J-40 to J-41\\n\\nCarry-out\\n\\ncarry-lookahead circuit, J-38\\nfloating-point addition speedup, \\n\\nJ-41\\n\\nJ-25\\n\\nCarry-propagate adder (CPA)\\n\\ninteger multiplication, J-48, J-51\\nmultipass array multiplier, J-51\\n\\nCarry-save adder (CSA)\\n\\ninteger division, J-54 to J-55\\ninteger multiplication, J-47 to J-48, \\n\\nJ-48\\nCarry-select adder\\n\\ncharacteristics, J-43 to J-44\\nchip comparison, J-60\\nexample, J-43\\n\\nCarry-skip adder (CSA)\\n\\ncharacteristics, J-41 to J43\\nexample, J-42, J-44\\n\\ncontrol flow instruction addressing \\nmodes, A-18\\n\\nCCD, see Charge-coupled device \\n(CCD)\\n\\nreturn address predictors, 206\\n\\nC/C++ language\\n\\nCase studies\\n\\nadvanced directory protocol, \\n420–426\\ncache optimization, 131–133\\ncell phones\\n\\nblock diagram, E-23\\nNokia circuit board, E-24\\noverview, E-20\\nradio receiver, E-23\\nstandards and evolution, E-25\\nwireless communication \\nchallenges, E-21\\n\\nwireless networks, E-21 to \\n\\nE-22\\nchip fabrication cost, 61–62\\ncomputer system power \\n\\nconsumption, 63–64\\n\\ndirectory-based coherence, \\n418–420\\n\\ndirty bits, D-61 to D-64\\ndisk array deconstruction, D-51 to \\n\\nD-55, D-52 to D-55\\n\\ndisk deconstruction, D-48 to D-51, \\n\\nhighly parallel memory systems, \\n\\ninstruction set principles, A-47 to \\n\\nD-50\\n\\n133–136\\n\\nA-54\\n\\nI/O subsystem design, D-59 to D-61\\nmemory hierarchy, B-60 to B-67\\nmicroarchitectural techniques, \\n247–254\\n\\npipelining example, C-82 to C-88\\nRAID performance prediction, \\n\\nD-57 to D-59\\n\\nRAID reconstruction, D-55 to \\n\\nSanyo VPC-SX500 digital camera, \\n\\nD-57\\n\\nE-19\\n\\nsingle-chip multicore processor, \\n\\n412–418\\nSony PlayStation 2 Emotion \\n\\nEngine, E-15 to E-18\\n\\nsorting, D-64 to D-67\\nvector kernel on vector processor \\n\\nand GPU, 334–336\\n\\ndependence analysis, H-6\\nGPU computing history, L-52\\nhardware impact on software \\n\\ndevelopment, 4\\n\\ninteger division/remainder, J-12\\nloop-level parallelism \\n\\ndependences, 318, \\n320–321\\n\\nNVIDIA GPU programming, 289\\nreturn address predictors, 206\\nCDB, see Common data bus (CDB)\\nCDC, see Control Data Corporation \\n(CDC)\\nCDF, datacenter, 487\\nCDMA, see Code division multiple \\n\\naccess (CDMA)\\n\\nCedar project, L-60\\nCell, Barnes-Hut n-body algorithm, \\n\\nI-9\\n\\nCell phones\\n\\nblock diagram, E-23\\nembedded system case study\\n\\ncharacteristics, E-22 to E-24\\noverview, E-20\\nradio receiver, E-23\\nstandards and evolution, E-25\\nwireless network overview, \\nE-21 to E-22\\n\\nFlash memory, D-3\\nGPU features, 324\\nNokia circuit board, E-24\\nwireless communication \\n\\nchallenges, E-21\\n\\nwireless networks, E-22\\nCentralized shared-memory \\n\\nmultiprocessors\\n\\nbasic considerations, 351–352\\nbasic structure, 346–347, 347\\ncache coherence, 352–353\\ncache coherence enforcement, \\n354–355\\n\\ncache coherence example, \\n\\n357–362\\ncache coherence extensions, \\n362–363\\ninvalidate protocol \\n\\nimplementation, \\n356–357\\n\\nCAS, see Column access strobe (CAS)\\nCase statements\\n\\nWSC resource allocation, 478–479\\nWSC TCO, 476–478\\n\\n\\x0cSMP and snooping limitations, \\n363–364\\nsnooping coherence \\n\\nimplementation, \\n365–366\\nsnooping coherence protocols, \\n355–356\\n\\nCentralized switched networks\\n\\nexample, F-31\\nrouting algorithms, F-48\\ntopology, F-30 to F-34, F-31\\n\\nCentrally buffered switch, \\n\\nmicroarchitecture, F-57\\n\\nCentral processing unit (CPU)\\n\\nAmdahl’s law, 48\\naverage memory access time, B-17\\ncache performance, B-4\\ncoarse-grained multithreading, 224\\nearly pipelined versions, L-26 to \\n\\nL-27\\n\\nexception stopping/restarting, C-47\\nextensive pipelining, C-81\\nGoogle server usage, 440\\nGPU computing history, L-52\\nvs.', ' GPUs, 288\\ninstruction set complications, C-50\\nMIPS implementation, C-33 to \\n\\nMIPS precise exceptions, C-59 to \\n\\nC-60\\nMIPS scoreboarding, C-77\\nperformance measurement history, \\n\\npipeline branch issues, C-41\\npipelining exceptions, C-43 to \\n\\nL-6\\n\\nC-46\\n\\npipelining performance, C-10\\nSony PlayStation 2 Emotion \\n\\nEngine, E-17\\n\\nSPEC server benchmarks, 40\\nTI TMS320C55 DSP, E-8\\nvector memory systems, G-10\\nCentral processing unit (CPU) time\\n\\nexecution time, 36\\nmodeling, B-18\\nprocessor performance \\n\\ncalculations, B-19 to \\nB-21\\n\\nCERN, see European Center for \\nParticle Research \\n(CERN)\\n\\nCFM, see Current frame pointer \\n(CFM)\\n\\nChaining\\n\\nconvoys, DAXPY code, G-16\\nvector processor performance, \\nG-11 to G-12, G-12\\n\\nVMIPS, 268–269\\n\\nChannel adapter, see Network \\n\\ninterface\\n\\nChannels, cell phones, E-24\\nCharacter\\n\\nfloating-point performance, A-2\\nas operand type, A-13 to A-14\\noperand types/sizes, 12\\n\\nCharge-coupled device (CCD), Sanyo \\n\\nVPC-SX500 digital \\ncamera, E-19\\n\\nChecksum\\n\\ndirty bits, D-61 to D-64\\npacket format, F-7\\n\\nChillers\\n\\nGoogle WSC, 466, 468\\nWSC containers, 464\\nWSC cooling systems, 448–449\\n\\ndefinition, 309\\nGPUs vs.', ' vector architectures, 308\\nmultiple lanes, 272\\nNVIDIA GPU computational \\n\\nstructures, 296\\n\\nvector chaining, G-12\\nvector execution time, 269, G-4\\nvector performance, G-2\\nvector sequence calculations, 270\\n\\nChip-crossing wire delay, F-70\\n\\nOCN history, F-103\\n\\nChipkill\\n\\nmemory dependability, 104–105\\nWSCs, 473\\n\\nChoke packets, congestion \\n\\nChunk\\n\\ndisk array deconstruction, D-51\\nShear algorithm, D-53\\n\\nCIFS, see Common Internet File \\n\\nC-34\\n\\nChime\\n\\nprocessor performance equation, \\n\\nSystem (CIFS)\\n\\n49–51\\n\\nCircuit switching\\n\\nprocessor performance time, 49\\n\\ncongestion management, F-64 to \\n\\nCerf, Vint, F-97\\n\\nF-65\\n\\nIndex\\n\\n■\\n\\nI-11\\n\\ninterconnected networks, F-50\\n\\nCirculating water system (CWS)\\ncooling system design, 448\\nWSCs, 448\\n\\nCISC, see Complex Instruction Set \\n\\nComputer (CISC)\\n\\nCLA, see Carry-lookahead adder \\n(CLA)\\n\\nClean block, definition, B-11\\nClimate Savers Computing Initiative, \\n\\npower supply \\nefficiencies, 462\\n\\nClock cycles\\n\\nbasic MIPS pipeline, C-34 to C-35\\nand branch penalties, 205\\ncache performance, B-4\\nFP pipeline, C-66\\nand full associativity, B-23\\nGPU conditional branching, 303\\nILP exploitation, 197, 200\\nILP exposure, 157\\ninstruction fetch bandwidth, \\n202–203\\n\\ninstruction steps, 173–175\\nIntel Core i7 branch predictor, 166\\nMIPS exceptions, C-48\\nMIPS pipeline, C-52\\nMIPS pipeline FP operations, C-52 \\n\\nto C-53\\nMIPS scoreboarding, C-77\\nmiss rate calculations, B-31 to B-32\\nmultithreading approaches, \\n225–226\\npipelining performance, C-10\\nprocessor performance equation, 49\\nRISC classic pipeline, C-7\\nSun T1 multithreading, 226–227\\nswitch microarchitecture \\n\\npipelining, F-61\\n\\nvector architectures, G-4\\nvector execution time, 269\\nvector multiple lanes, 271–273\\nVLIW processors, 195\\n\\naddressing modes, A-10\\nARM Cortex-A8, 235\\nbranch schemes, C-25 to C-26, \\n\\nC-26\\n\\ncache behavior impact, B-18 to \\n\\nB-19\\ncache hit calculation, B-5\\ndata hazards requiring stalls, C-20\\n\\nmanagement, F-65\\n\\nClock cycles per instruction (CPI)\\n\\n\\x0ccalculations, 218–219\\n\\nCloud computing\\n\\nI-12 ■\\n\\nIndex\\n\\nClock cycles per instruction (continued)\\n\\nextensive pipelining, C-81\\nfloating-point calculations, 50–52\\nILP concepts, 148–149, 149\\nILP exploitation, 192\\nIntel Core i7, 124, 240, 240–241\\nmicroprocessor advances, L-33\\nMIPS R4000 performance, C-69\\nmiss penalty reduction, B-32\\nmultiprocessing/\\n\\nmultithreading-based \\nperformance, 398–400\\n\\nmultiprocessor communication \\n\\ncalculations, 350\\npipeline branch issues, C-41\\npipeline with stalls, C-12 to C-13\\npipeline structural hazards, C-15 to \\n\\nC-16\\n\\npipelining concept, C-3\\nprocessor performance \\n\\nprocessor performance time, 49–51\\nand processor speed, 244\\nRISC history, L-21\\nshared-memory workloads, \\n369–370\\nsimple MIPS implementation, \\n\\nC-33 to C-34\\nstructural hazards, C-13\\nSun T1 multithreading unicore \\n\\nperformance, 229\\n\\nSun T1 processor, 399\\nTomasulo’s algorithm, 181\\nVAX 8700 vs.', ' price, 32–33\\ncost trends, 27–28, 32\\nEthernet rack switch, 442\\nHPC hardware, 436\\nshared-memory multiprocessor, \\n\\n441\\n\\nI-45\\n\\nWSCs, 441\\n\\nCommodity cluster, characteristics, \\n\\nCommon data bus (CDB)\\n\\ndynamic scheduling with \\n\\nTomasulo’s algorithm, \\n172, 175\\n\\nadaptive routing, F-93 to F-94\\ninternetworking, F-81 to F-82\\nlarge-scale multiprocessors\\nadvantages, I-4 to I-6\\nmetrics, I-3 to I-4\\n\\nmultiprocessor communication \\n\\ncalculations, 350\\n\\nnetwork interfaces, F-7 to F-8\\nNEWS communication, F-42 to \\n\\nF-43\\n\\nSMP limitations, 363\\n\\nCommunication protocol, definition, \\n\\nF-8\\nCommunication subnets, see \\nInterconnection \\nnetworks\\n\\nCommunication subsystems, see \\n\\nInterconnection \\nnetworks\\n\\nCompare instruction, VAX, K-71\\nCompares, MIPS core, K-9 to K-16\\nCompare-select-store unit (CSSU), TI \\n\\nTMS320C55 DSP, E-8\\n\\nCompiler-controlled prefetching, miss \\n\\npenalty/rate reduction, \\n92–95\\n\\nCompiler optimizations\\nblocking, 89–90\\ncache optimization, 131–133\\ncompiler assumptions, A-25 to \\n\\nA-26\\nand consistency model, 396\\nloop interchange, 88–89\\nmiss rate reduction, 87–90\\npasses, A-25\\nperformance impact, A-27\\n\\nIndex\\n\\n■\\n\\nI-13\\n\\ntypes and classes, A-28\\n\\nCompiler scheduling\\n\\ndata dependences, 151\\ndefinition, C-71\\nhardware support, L-30 to L-31\\nIBM 360 architecture, 171\\n\\nCompiler speculation, hardware support\\n\\nmemory references, H-32\\noverview, H-27\\npreserving exception behavior, \\n\\nILP exposure, 156–162\\nvectorization, G-14\\nvector sparse matrices, G-12\\n\\nCompiler technology\\n\\nand architecture decisions, A-27 to \\n\\nA-29\\n\\nCray X1, G-21 to G-22\\nISA and code size, A-43 to A-44\\nmultimedia instruction support, \\n\\nA-31 to A-32\\nregister allocation, A-26 to A-27\\nstructure, A-24 to A-26, A-25\\nCompiler writer-architect relationship, \\n\\nA-29 to A-30\\n\\nComplex Instruction Set Computer \\n(CISC)\\n\\nRISC history, L-22\\nVAX as, K-65\\nCompulsory misses\\n\\nand cache size, B-24\\ndefinition, B-23\\nmemory hierarchy basics, 75\\nshared-memory workload, 373\\nComputation-to-communication ratios\\nparallel programs, I-10 to I-12\\nscaling, I-11\\n\\nCompute-optimized processors, \\n\\ninterconnection \\nnetworks, F-88\\n\\nComputer aided design (CAD) tools, \\n\\ncache optimization, \\n79–80\\nComputer architecture, see also \\n\\nArchitecture\\n\\ncoining of term, K-83 to K-84\\ncomputer design innovations, 4\\ndefining, 11\\n\\n\\x0cComputer architecture (continued)\\n\\ncarry-skip adder, J-41 to J43, \\n\\nI-14 ■\\n\\nIndex\\n\\ndefinition, L-17 to L-18\\nexceptions, C-44\\nfactors in improvement, 2\\nflawless design, K-81\\nflaws and success, K-81\\nfloating-point addition, rules, J-24\\ngoals/functions requirements, 15, \\n\\n15–16, 16\\n\\nhigh-level language, L-18 to L-19\\ninstruction execution issues, K-81\\nISA, 11–15\\nmultiprocessor software \\n\\ndevelopment, 407–409\\n\\nparallel, 9–10\\nWSC basics, 432, 441–442\\n\\narray switch, 443\\nmemory hierarchy, 443–446\\nstorage, 442–443\\n\\nComputer arithmetic\\n\\nchip comparison, J-58, J-58 to \\n\\nJ-61, J-59 to J-60\\n\\nfloating point\\n\\nexceptions, J-34 to J-35\\nfused multiply-add, J-32 to J-33\\nIEEE 754, J-16\\niterative division, J-27 to J-31\\nand memory bandwidth, J-62\\noverview, J-13 to J-14\\nprecisions, J-33 to J-34\\nremainder, J-31 to J-32\\nspecial values, J-16\\nspecial values and denormals, \\n\\nunderflow, J-36 to J-37, J-62\\n\\nfloating-point addition\\n\\ndenormals, J-26 to J-27\\noverview, J-21 to J-25\\nspeedup, J-25 to J-26\\nfloating-point multiplication\\ndenormals, J-20 to J-21\\nexamples, J-19\\noverview, J-17 to J-20\\nrounding, J-18\\n\\ninteger addition speedup\\n\\ncarry-lookahead, J-37 to J-41\\ncarry-lookahead circuit, J-38\\ncarry-lookahead tree, J-40\\ncarry-lookahead tree adder, \\n\\ncarry-select adder, J-43, J-43 to \\n\\nJ-41\\n\\nJ-44, J-44\\n\\nrestoring/nonrestoring division, \\n\\nComputer room air-conditioning \\n\\nJ-42\\n\\noverview, J-37\\ninteger arithmetic\\n\\nlanguage comparison, J-12\\noverflow, J-11\\nRadix-2 multiplication/\\n\\ndivision, J-4, J-4 to \\nJ-7\\n\\nJ-6\\n\\nJ-3\\n\\nripply-carry addition, J-2 to J-3, \\n\\nsigned numbers, J-7 to J-10\\nsystems issues, J-10 to J-13\\n\\ninteger division\\n\\nradix-2 division, J-55\\nradix-4 division, J-56\\nradix-4 SRT division, J-57\\nwith single adder, J-54 to J-58\\nSRT division, J-45 to J-47, J-46\\n\\ninteger-FP conversions, J-62\\ninteger multiplication\\n\\narray multiplier, J-50\\nBooth recoding, J-49\\neven/odd array, J-52\\nwith many adders, J-50 to J-54\\nmultipass array multiplier, J-51\\nsigned-digit addition table, \\n\\nJ-54\\n\\nwith single adder, J-47 to J-49, \\n\\nJ-48\\nWallace tree, J-53\\n\\nshifting over zeros, J-45 \\nto J-47\\n\\noverview, J-2\\nrounding modes, J-20\\nComputer chip fabrication\\ncost case study, 61–62\\nCray X1E, G-24\\n\\nComputer classes\\ndesktops, 6\\nembedded computers, 8–9\\nexample, 5\\noverview, 5\\nparallelism and parallel \\n\\narchitectures, 9–10\\n\\nPMDs, 6\\nservers, 7\\nand system characteristics, E-4\\nwarehouse-scale computers, 8\\n\\nComputer design principles\\nAmdahl’s law, 46–48\\ncommon case, 45–46\\nparallelism, 44–45\\nprinciple of locality, 45\\nprocessor performance equation, \\n\\n48–52\\nComputer history, technology and \\n\\narchitecture, 2–5\\n\\n(CRAC), WSC \\ninfrastructure, 448–449\\n\\nCompute tiles, OCNs, F-3\\nCompute Unified Device Architecture, \\n\\nsee CUDA (Compute \\nUnified Device \\nArchitecture)\\n\\nConditional branches\\n\\nbranch folding, 206\\ncompare frequencies, A-20\\ncompiler performance, C-24 to \\n\\nC-25\\n\\ncontrol flow instructions, 14, A-16, \\n\\nA-17, A-19, A-21\\n\\ndesktop RISCs, K-17\\nembedded RISCs, K-17\\nevaluation, A-19\\nglobal code scheduling, H-16, H-16\\nGPUs, 300–303\\nideal processor, 214\\nISAs, A-46\\nMIPS control flow instructions, \\nA-38, A-40\\n\\nMIPS core, K-9 to K-16\\nPA-RISC instructions, K-34, K-34\\npredictor misprediction rates, 166\\nPTX instruction set, 298–299\\nstatic branch prediction, C-26\\ntypes, A-20\\nvector-GPU comparison, 311\\n\\nConditional instructions\\n\\nexposing parallelism, H-23 to H-27\\nlimitations, H-26 to H-27\\n\\nCondition codes\\n\\nbranch conditions, A-19\\ncontrol flow instructions, 14\\ndefinition, C-5\\nhigh-level instruction set, A-43\\ninstruction set complications, C-50\\nMIPS core, K-9 to K-16\\npipeline branch penalties, C-23\\nVAX, K-71\\n\\nJ-14 to J-15\\n\\ninteger multiplication/division, \\n\\n\\x0cIndex\\n\\n■\\n\\nI-15\\n\\nConflict misses\\n\\nand block size, B-28\\ncache coherence mechanism, 358\\nand cache size, B-24, B-26\\ndefinition, B-23\\nas kernel miss, 376\\nL3 caches, 371\\nmemory hierarchy basics, 75\\nOLTP workload, 370\\nPIDs, B-37\\nshared-memory workload, 373\\n\\nearly computer arithmetic, J-64\\nfirst dynamic scheduling, L-27\\nMIPS scoreboarding, C-75, C-77\\nmultiple-issue processor \\n\\ndevelopment, L-28\\nmultithreading history, L-34\\nRISC history, L-19\\n\\nControl Processor\\ndefinition, 309\\nGPUs, 333\\nSIMD, 10\\nThread Block Scheduler, 294\\nvector processor, 310, 310–311\\nvector unit structure, 273\\n\\nControl Data Corporation (CDC) \\n\\nConventional datacenters, vs.', ' start-up \\n\\nConvex Exemplar, L-61\\nConvex processors, vector processor \\n\\nCongestion control\\n\\noverhead, 331\\n\\nhistory, G-26\\n\\ncommercial interconnection \\n\\nControl Data Corporation (CDC) \\n\\nnetworks, F-64\\nsystem area network history, F-101\\nCongestion management, commercial \\n\\ninterconnection \\nnetworks, F-64 to F-66\\n\\nConnectedness\\n\\ndimension-order routing, F-47 to \\n\\nF-48\\n\\ninterconnection network topology, \\n\\nF-29\\nConnection delay, multi-device \\n\\ninterconnection \\nnetworks, F-25\\n\\nConnection Machine CM-5, F-91, \\nF-100\\nConnection Multiprocessor 2, L-44, \\n\\nL-57\\n\\nConsistency, see Memory consistency\\nConstant extension\\n\\ndesktop RISCs, K-9\\nembedded RISCs, K-9\\n\\nConstellation, characteristics, I-45\\nContainers\\n\\nairflow, 466\\ncluster history, L-74 to L-75\\nGoogle WSCs, 464–465, 465\\n\\nContext Switching\\n\\ndefinition, 106, B-49\\nFermi GPU, 307\\n\\nControl bits, messages, F-6\\nControl Data Corporation (CDC), first \\nvector computers, L-44 \\nto L-45\\n\\nControl Data Corporation (CDC) 6600\\ncomputer architecture definition, \\n\\nL-18\\n\\nSTAR processor, G-26\\n\\nControl dependences\\n\\nconditional instructions, H-24\\nas data dependence, 150\\nglobal code scheduling, H-16\\nhardware-based speculation, \\n\\n183\\nILP, 154–156\\nILP hardware model, 214\\nand Tomasulo’s algorithm, 170\\nvector mask registers, 275–276\\n\\naddressing modes, A-17 to A-18\\nbasic considerations, A-16 to \\nA-17, A-20 to A-21\\n\\nclasses, A-17\\nconditional branch options, A-19\\nconditional instructions, H-27\\nhardware vs.', ' start-up \\n\\noverhead, 331\\n\\npipeline depths, G-4\\nRISC history, L-19\\nvector performance, 332\\nvector performance measures, G-16\\nas VMIPS basis, 264, 270–271, \\n276–277\\n\\nCray-2\\n\\nDRAM, G-25\\nfirst vector computers, L-47\\ntailgating, G-20\\n\\nCray-3, G-27\\nCray-4, G-27\\nCray C90\\n\\nfirst vector computers, L-46, L-48\\nvector performance calculations, \\n\\nG-8\\n\\nCray J90, L-48\\nCray Research T3D, F-86 to F-87, \\nF-87\\n\\nCray supercomputers, early computer \\narithmetic, J-63 to J-64\\n\\nCray T3D, F-100, L-60\\nCray T3E, F-67, F-94, F-100, L-48, \\n\\nCray T90, memory bank calculations, \\n\\nL-60\\n\\n276\\n\\nCray X1\\n\\ncluster history, L-63\\nfirst vector computers, L-46, L-48\\nMSP module, G-22, G-23 to G-24\\noverview, G-21 to G-23\\npeak performance, 58\\n\\nCray X1E, F-86, F-91\\n\\ncharacteristics, G-24\\n\\nCray X-MP, L-45\\n\\nfirst vector computers, L-47\\n\\nCray XT3, L-58, L-63\\nCray XT3 SeaStar, F-63\\nCray Y-MP\\n\\nfirst vector computers, L-45 to \\n\\nL-47\\n\\nparallel processing debates, L-57\\nvector architecture programming, \\n281, 281–282\\n\\nCRC, see Cyclic redundancy check \\n(CRC)\\n\\nCreate vector index instruction (CVI), \\n\\nsparse matrices, G-13\\n\\nCredit-based control flow\\nInfiniBand, F-74\\ninterconnection networks, F-10, \\n\\nF-17\\n\\nCRISP, L-27\\nCritical path\\n\\nglobal code scheduling, H-16\\ntrace scheduling, H-19 to H-21, H-20\\nCritical word first, cache optimization, \\n\\n86–87\\n\\nCrossbars\\n\\ncentralized switched networks, \\nF-30, F-31\\ncharacteristics, F-73\\nConvex Exemplar, L-61\\nHOL blocking, F-59\\nOCN history, F-104\\nswitch microarchitecture, F-62\\nswitch microarchitecture \\n\\npipelining, F-60 to F-61, \\nF-61\\n\\nVMIPS, 265\\nCrossbar switch\\n\\ncentralized switched networks, F-30\\ninterconnecting node calculations, \\nF-31 to F-32\\n\\nCross-company interoperability, \\n\\ncommercial \\ninterconnection \\nnetworks, F-63 to F-64\\n\\nCrusoe, L-31\\nCryptanalysis, L-4\\nCSA, see Carry-save adder (CSA); \\n\\nCarry-skip adder (CSA)\\n\\nC# language, hardware impact on \\n\\nsoftware development, 4\\nCSSU, see Compare-select-store unit \\n(CSSU)\\n\\nCount register, PowerPC instructions, \\n\\nCray X2, L-46 to L-47\\n\\nK-32 to K-33\\n\\nfirst vector computers, L-48 to \\n\\nCP-67 program, L-10\\n\\nL-49\\n\\n\\x0cCUDA (Compute Unified Device \\n\\nArchitecture)\\n\\nGPU computing history, L-52\\nGPU conditional branching, 303\\nGPUs vs.', ' vector architectures, \\n\\nCyclic redundancy check (CRC)\\nIBM Blue Gene/L 3D torus \\n\\nnetwork, F-73\\n\\nnetwork interface, F-8\\nCydrome Cydra 6, L-30, L-32\\n\\n310\\n\\nNVIDIA GPU programming, \\n\\n289\\nPTX, 298, 300\\nsample program, 289–290\\nSIMD instructions, 297\\nterminology, 313–315\\n\\nCUDA Thread\\n\\nCUDA programming model, 300, \\n\\n315\\n\\ndefinition, 292, 313\\ndefinitions and terms, 314\\nGPU data addresses, 310\\nGPU Memory structures, 304\\nNVIDIA parallelism, 289–290\\nvs.', ' start-up \\n\\noverhead, 331\\nvector processor history, G-26 to \\n\\nG-27\\nCYBER 250, L-45\\nCycles, processor performance \\n\\nequation, 49\\n\\nCycle time, see also Clock cycle time\\n\\nCPI calculations, 350\\npipelining, C-81\\nscoreboarding, C-79\\nvector processors, 277\\n\\nD\\nDaCapo benchmarks\\n\\nISA, 242\\nSMT, 230–231, 231\\n\\nDAMQs, see Dynamically allocatable \\nmulti-queues (DAMQs)\\n\\nDASH multiprocessor, L-61\\nDatabase program speculation, via \\n\\nmultiple branches, 211\\n\\nData cache\\n\\nARM Cortex-A8, 236\\ncache optimization, B-33, B-38\\ncache performance, B-16\\nGPU Memory, 306\\nISA, 241\\nlocality principle, B-60\\nMIPS R4000 pipeline, C-62 to \\n\\nC-63\\nmultiprogramming, 374\\npage level write-through, B-56\\nRISC processor, C-7\\nstructural hazards, C-15\\nTLB, B-46\\nData cache miss\\n\\napplications vs.', ' WSCs, 436\\nData dependences\\n\\nconditional instructions, H-24\\ndata hazards, 167–168\\n\\nIndex\\n\\n■\\n\\nI-17\\n\\ndynamically scheduling with \\n\\nscoreboard, C-71\\n\\nexample calculations, H-3 to H-4\\nhazards, 153–154\\nILP, 150–152\\nILP hardware model, 214–215\\nILP limitation studies, 220\\nvector execution time, 269\\n\\nData fetching\\n\\nARM Cortex-A8, 234\\ndirectory-based cache coherence \\n\\nprotocol example, \\n382–383\\n\\ndynamically scheduled pipelines, \\nC-70 to C-71\\n\\nILP, instruction bandwidth\\n\\nbasic considerations, 202–203\\nbranch-target buffers, 203–206\\nreturn address predictors, \\n\\n206–207\\nMIPS R4000, C-63\\nsnooping coherence protocols, \\n355–356\\n\\nData flow\\n\\ncontrol dependence, 154–156\\ndynamic scheduling, 168\\nglobal code scheduling, H-17\\nILP limitation studies, 220\\nlimit, L-33\\n\\nData flow execution, hardware-based \\n\\nspeculation, 184\\n\\nDatagrams, see Packets\\nData hazards\\n\\nARM Cortex-A8, 235\\nbasic considerations, C-16\\ndefinition, C-11\\ndependences, 152–154\\ndynamic scheduling, 167–176\\nbasic concept, 168–170\\nexamples, 176–178\\nTomasulo’s algorithm, \\n\\n170–176, 178–179\\n\\nTomasulo’s algorithm \\n\\nloop-based example, \\n179–181\\n\\nILP limitation studies, 220\\ninstruction set complications, C-50 \\n\\nto C-51\\n\\nmicroarchitectural techniques case \\nstudy, 247–254\\n\\nMIPS pipeline, C-71\\nRAW, C-57 to C-58\\n\\n\\x0cI-18 ■\\n\\nIndex\\n\\nData hazards\\n\\nstall minimization by forwarding, \\n\\nC-16 to C-19, C-18\\n\\nstall requirements, C-19 to C-21\\nVMIPS, 264\\n\\nData-level parallelism (DLP)\\n\\ndefinition, 9\\nGPUs\\n\\nbasic considerations, 288\\nbasic PTX thread instructions, \\n\\n299\\n\\nconditional branching, 300–303\\ncoprocessor relationship, \\n\\n330–331\\nFermi GPU architecture \\n\\ninnovations, 305–308\\n\\nFermi GTX 480 floorplan, 295\\nmapping examples, 293\\nMultimedia SIMD comparison, \\n\\n312\\n\\nmultithreaded SIMD Processor \\nblock diagram, 294\\nNVIDIA computational \\n\\nstructures, 291–297\\nNVIDIA/CUDA and AMD \\n\\nterminology, 313–315\\nNVIDIA GPU ISA, 298–300\\nNVIDIA GPU Memory \\n\\nprogramming, 288–291\\nSIMD thread scheduling, 297\\nterminology, 292\\nvs.', ' GPU, 300\\n\\nData types\\n\\narchitect-compiler writer \\n\\nrelationship, A-30\\n\\ndependence analysis, H-10\\ndesktop computing, A-2\\nIntel 80x86, K-50\\nMIPS, A-34, A-36\\nMIPS64 architecture, A-34\\nmultimedia compiler support, A-31\\noperand types/sizes, A-14 to A-15\\nSIMD Multimedia Extensions, \\n282–283\\n\\nSPARC, K-31\\nVAX, K-66, K-70\\n\\nDauber, Phil, L-28\\nDAXPY loop\\n\\nchained convoys, G-16\\non enhanced VMIPS, G-19 to G-21\\nmemory bandwidth, 332\\nMIPS/VMIPS calculations, \\n267–268\\npeak performance vs.', ' Intel 80x86 operations, K-62, \\nK-63 to K-64\\n\\nDMA, see Direct memory access \\n(DMA)\\nDOR, see Dimension-order routing \\n(DOR)\\n\\nDouble data rate (DDR)\\n\\nARM Cortex-A8, 117\\nDRAM performance, 100\\nDRAMs and DIMMS, 101\\nGoogle WSC servers, 468–469\\nIBM Blue Gene/L, I-43\\nInfiniBand, F-77\\nIntel Core i7, 121\\nSDRAMs, 101\\n\\nDRAM internal organization, 98\\nGDRAM, 102\\nIntel Core i7, 118\\nSDRAM power consumption, 102, \\n\\n103\\n\\n99\\n\\nDouble data rate 4 (DDR4), DRAM, \\n\\nDouble data rate 5 (DDR5), GDRAM, \\n\\n102\\nDouble-extended floating-point \\n\\nDouble failures, RAID reconstruction, \\n\\nD-55 to D-57\\n\\nDouble-precision floating point\\n\\nadd-divide, C-68\\nAVX for x86, 284\\nchip comparison, J-58\\ndata access benchmarks, A-15\\nDSP media extensions, E-10 to \\n\\nE-11\\n\\nFermi GPU architecture, 306\\nfloating-point pipeline, C-65\\nGTX 280, 325, 328–330\\nIBM 360, 171\\nMIPS, 285, A-38 to A-39\\nMIPS data transfers, A-34\\nMIPS registers, 12, A-34\\nMultimedia SIMD vs.', ' GPUs, 312\\noperand sizes/types, 12\\nas operand type, A-13 to A-14\\noperand usage, 297\\npipeline timing, C-54\\n\\ntopology, F-34 to F-40\\n\\narithmetic, J-33 to J-34\\n\\n\\x0cI-22 ■\\n\\nIndex\\n\\nDouble-precision (continued )\\nRoofline model, 287, 326\\nSIMD Extensions, 283\\nVMIPS, 266, 266–267\\n\\nDouble rounding\\n\\nFP precisions, J-34\\nFP underflow, J-37\\n\\nDouble words\\n\\naligned/misaligned addresses, A-8\\ndata access benchmarks, A-15\\nIntel 80x86, K-50\\nmemory address interpretation, \\nA-7 to A-8\\n\\nMIPS data types, A-34\\noperand types/sizes, 12, A-14\\nstride, 278\\n\\nDPL, see Descriptor privilege level \\n(DPL)\\n\\nDRAM, see Dynamic random-access \\n\\nmemory (DRAM)\\n\\nDRDRAM, Sony PlayStation 2, E-16 \\nto E-17\\n\\nDriver domains, Xen VM, 111\\nDSM, see Distributed shared memory \\n\\n(DSM)\\nDSP, see Digital signal processor \\n(DSP)\\nDSS, see Decision support system \\n(DSS)\\n\\nDual inline memory modules (DIMMs)\\nclock rates, bandwidth, names, 101\\nDRAM basics, 99\\nGoogle WSC server, 467\\nGoogle WSC servers, 468–469\\ngraphics memory, 322–323\\nIntel Core i7, 118, 121\\nIntel SCCC, F-70\\nSDRAMs, 101\\nWSC memory, 473–474\\nDual SIMD Thread Scheduler, \\n\\nDVFS, see Dynamic \\n\\nexample, 305–306\\n\\nvoltage-frequency \\nscaling (DVFS)\\n\\nDynamically allocatable multi-queues \\n\\n(DAMQs), switch \\nmicroarchitecture, F-56 \\nto F-57\\nDynamically scheduled pipelines\\n\\nbasic considerations, C-70 to C-71\\nwith scoreboard, C-71 to C-80\\n\\nDynamically shared libraries, control \\n\\nflow instruction \\naddressing modes, A-18\\n\\nDynamic energy, definition, 23\\nDynamic network reconfiguration, \\n\\nfault tolerance, F-67 to \\nF-68\\n\\nDynamic power\\n\\nenergy efficiency, 211\\nmicroprocessors, 23\\nvs.', ' access time, D-3\\ncost trends, 27\\nCray X1, G-22\\nCUDA, 290\\ndependability, 104\\ndisk storage, D-3 to D-4\\nembedded benchmarks, E-13\\nerrors and faults, D-11\\nfirst vector computers, L-45, L-47\\nFlash memory, 103–104\\nGoogle WSC servers, 468–469\\nGPU SIMD instructions, 296\\nIBM Blue Gene/L, I-43 to I-44\\nimprovement over time, 17\\nintegrated circuit costs, 28\\nIntel Core i7, 121\\ninternal organization, 98\\nmagnetic storage history, L-78\\nmemory hierarchy design, 73, 73\\nmemory performance, 100–102\\nmultibanked caches, 86\\nNVIDIA GPU Memory structures, \\n\\nperformance milestones, 20\\npower consumption, 63\\nreal-world server considerations, \\n\\n305\\n\\n52–55\\n\\nRoofline model, 286\\nserver energy savings, 25\\nSony PlayStation 2, E-16, E-17\\nspeed trends, 99\\ntechnology trends, 17\\nvector memory systems, G-9\\nvector processor, G-25\\nWSC efficiency measurement, 450\\n\\nWSC memory costs, 473–474\\nWSC memory hierarchy, 444–445\\nWSC power modes, 472\\nyield, 32\\n\\nDynamic scheduling\\nfirst use, L-27\\nILP\\n\\nbasic concept, 168–169\\ndefinition, 168\\nexample and algorithms, \\n176–178\\nwith multiple issue and \\n\\nspeculation, 197–202\\n\\novercoming data hazards, \\n\\n167–176\\n\\nTomasulo’s algorithm, 170–176, \\n178–179, 181–183\\n\\nMIPS scoreboarding, C-79\\nSMT on superscalar processors, 230\\nand unoptimized code, C-81\\nDynamic voltage-frequency scaling \\n(DVFS)\\n\\nenergy efficiency, 25\\nGoogle WSC, 467\\nprocessor performance equation, \\n\\n52\\nDynamo (Amazon), 438, 452\\n\\nE\\nEarly restart, miss penalty reduction, \\n\\n86\\n\\nEarth Simulator, L-46, L-48, L-63\\nEBS, see Elastic Block Storage (EBS)\\nEC2, see Amazon Elastic Computer \\n\\nCloud (EC2)\\n\\nECC, see Error-Correcting Code \\n(ECC)\\n\\nEckert, J.', ' network-only features, F-94 to \\n\\nF-95\\nEnergy efficiency, see also Power \\n\\nembedded benchmarks, E-13\\nhardward fallacies, 56\\nILP exploitation, 201\\nIntel Core i7, 401–405\\nISA, 241–243\\nmicroprocessor, 23–26\\nPMDs, 6\\nprocessor performance equation, 52\\nservers, 25\\nand speculation, 211–212\\nsystem trends, 21–23\\nWSC, measurement, 450–452\\nWSC goals/requirements, 433\\nWSC infrastructure, 447–449\\nWSC servers, 462–464\\n\\nEnergy proportionality, WSC servers, \\n\\n462\\nEngineering Research Associates \\n\\n(ERA), L-4 to L-5\\n\\nENIAC (Electronic Numerical \\n\\nIntegrator and \\nCalculator), L-2 to L-3, \\nL-5 to L-6, L-77\\n\\nEnigma coding machine, L-4\\nEntry time, transactions, D-16, D-17\\nEnvironmental faults, storage systems, \\n\\nD-11\\n\\nEPIC approach\\n\\nhistorical background, L-32\\nIA-64, H-33\\nVLIW processors, 194, 196\\n\\nEqual condition code, PowerPC, K-10 \\n\\nto K-11\\n\\nERA, see Engineering Research \\nAssociates (ERA)\\n\\nErasure encoding, WSCs, 439\\nError-Correcting Code (ECC)\\n\\ndisk storage, D-11\\nfault detection pitfalls, 58\\nFermi GPU architecture, 307\\nhardware dependability, D-15\\nmemory dependability, 104\\nRAID 2, D-6\\nand WSCs, 473–474\\n\\nError handling, interconnection \\n\\nnetworks, F-12\\n\\nErrors, definition, D-10 to D-11\\nEscape resource set, F-47\\nETA processor, vector processor \\n\\nhistory, G-26 to G-27\\n\\nand bandwidth, F-78\\ncommercial interconnection \\n\\nnetworks, F-63\\ncross-company interoperability, F-64\\ninterconnection networks, F-89\\nas LAN, F-77 to F-79\\nLAN history, F-99\\nLANs, F-4\\npacket format, F-75\\nshared-media networks, F-23\\nshared- vs.', ' no-write allocate, B-12\\nWSC memory latency, 445\\nWSC running service availability, \\n\\n434–435\\nWSC server data transfer, 446\\n\\nALU instructions, C-4\\narchitecture-specific examples, \\n\\nC-44\\ncategories, C-46\\ncontrol dependence, 154–155\\nfloating-point arithmetic, J-34 to \\n\\nJ-35\\n\\nhardware-based speculation, 190\\nimprecise, 169–170, 188\\nlong latency pipelines, C-55\\nMIPS, C-48, C-48 to C-49\\nout-of-order completion, 169–170\\nprecise, C-47, C-58 to C-60\\npreservation via hardward support, \\nH-28 to H-32\\n\\nIndex\\n\\n■\\n\\nI-25\\n\\nreturn address buffer, 207\\nROB instructions, 190\\nspeculative execution, 222\\nstopping/restarting, C-46 to C-47\\ntypes and requirements, C-43 to \\n\\nC-46\\n\\nExecute step\\n\\ninstruction steps, 174\\nItanium 2, H-42\\nROB instruction, 186\\nTI 320C55 DSP, E-7\\nExecution address cycle (EX)\\nbasic MIPS pipeline, C-36\\ndata hazards requiring stalls, C-21\\ndata hazard stall minimization, \\n\\nexception stopping/restarting, C-46 \\n\\nC-17\\n\\nto C-47\\n\\nhazards and forwarding, C-56 to \\n\\nC-57\\nMIPS FP operations, basic \\n\\nconsiderations, C-51 to \\nC-53\\n\\nMIPS pipeline, C-52\\nMIPS pipeline control, C-36 to \\n\\nMIPS R4000, C-63 to C-64, C-64\\nMIPS scoreboarding, C-72, C-74, \\n\\nC-39\\n\\nC-77\\n\\nout-of-order execution, C-71\\npipeline branch issues, C-40, C-42\\nRISC classic pipeline, C-10\\nsimple MIPS implementation, \\n\\nC-31 to C-32\\nsimple RISC implementation, C-6\\n\\nAmdahl’s law, 46–47, 406\\napplication/OS misses, B-59\\ncache performance, B-3 to B-4, \\n\\nB-16\\ncalculation, 36\\ncommercial workloads, 369–370, \\n\\n370\\n\\nenergy efficiency, 211\\nintegrated circuits, 22\\nloop unrolling, 160\\nmultilevel caches, B-32 to B-34\\nmultiprocessor performance, \\n405–406\\n\\nmultiprogrammed parallel “make” \\nworkload, 375\\n\\nmultithreading, 232\\n\\npipeline execution rate, C-10 to \\n\\nExecution time\\n\\nExceptions\\n\\n\\x0cI-26 ■\\n\\nIndex\\n\\nExecution time (continued )\\n\\nperformance equations, B-22\\npipelining performance, C-3, C-10 \\n\\nto C-11\\n\\nPMDs, 6\\nprinciple of locality, 45\\nprocessor comparisons, 243\\nprocessor performance equation, \\n\\n49, 51\\nreduction, B-19\\nsecond-level cache size, B-34\\nSPEC benchmarks, 42–44, 43, 56\\nand stall time, B-21\\nvector length, G-7\\nvector mask registers, 276\\nvector operations, 268–271\\n\\nExpand-down field, B-53\\nExplicit operands, ISA classifications, \\nA-3 to A-4\\n\\nExplicit parallelism, IA-64, H-34 to \\n\\nH-35\\n\\nExplicit unit stride, GPUs vs.', ' vector \\n\\narchitectures, 310\\n\\nExponential back-off\\n\\nlarge-scale multiprocessor \\n\\nsynchronization, I-17\\n\\nspin lock, I-17\\n\\nExponential distribution, definition, \\n\\nD-27\\n\\nExtended accumulator\\n\\nflawed architectures, A-44\\nISA classification, A-3\\n\\nF\\nFacebook, 460\\nFailures, see also Mean time between \\nfailures (MTBF); Mean \\ntime to failure (MTTF)\\n\\nAmdahl’s law, 56\\nBerkeley’s Tertiary Disk project, \\n\\nD-12\\n\\ncloud computing, 455\\ndefinition, D-10\\ndependability, 33–35\\ndirty bits, D-61 to D-64\\nDRAM, 473\\nexample calculation, 48\\nGoogle WSC networking, 469–470\\npower failure, C-43 to C-44, C-46\\npower utilities, 435\\nRAID reconstruction, D-55 to \\n\\nD-57\\n\\nRAID row-diagonal parity, D-9\\nrate calculations, 48\\nservers, 7, 434\\nSLA states, 34\\nstorage system components, D-43\\nstorage systems, D-6 to D-10\\nTDP, 22\\nTertiary Disk, D-13\\nWSC running service, 434–435\\nWSCs, 8, 438–439\\nWSC storage, 442–443\\n\\nFalse sharing\\n\\ndefinition, 366–367\\nshared-memory workload, 373\\n\\nFarmVille, 460\\nFast Fourier transformation (FFT)\\n\\ncharacteristics, I-7\\ndistributed-memory \\n\\nmultiprocessor, I-32\\n\\nexample calculations, I-27 to I-29\\nsymmetric shared-memory \\n\\nmultiprocessors, I-22, \\nI-23, I-25\\nFast traps, SPARC instructions, K-30\\nFat trees\\n\\ndefinition, F-34\\nNEWS communication, F-43\\nrouting algorithms, F-48\\nSAN characteristics, F-76\\ntopology, F-38 to F-39\\ntorus topology interconnections, \\nF-36 to F-38\\n\\nFault detection, pitfalls, 57–58\\nFault-induced deadlock, routing, F-44\\nFaulting prefetches, cache \\n\\noptimization, 92\\n\\nFaults, see also Exceptions; Page \\nfaults\\n\\naddress fault, B-42\\ndefinition, D-10\\nand dependability, 33\\ndependability benchmarks, D-21\\nprogramming mistakes, D-11\\nstorage systems, D-6 to D-10\\nTandem Computers, D-12 to D-13\\nVAX systems, C-44\\n\\nFault tolerance\\n\\nand adaptive routing, F-94\\ncommercial interconnection \\n\\nnetworks, F-66 to F-69\\n\\nDECstation 5000 reboots, F-69\\ndependability benchmarks, D-21\\n\\nRAID, D-7\\nSAN example, F-74\\nWSC memory, 473–474\\nWSC network, 461\\n\\nFault-tolerant routing, commercial \\n\\ninterconnection \\nnetworks, F-66 to F-67\\n\\nFC, see Fibre Channel (FC)\\nFC-AL, see Fibre Channel Arbitrated \\n\\nLoop (FC-AL)\\n\\nFC-SW, see Fibre Channel Switched \\n(FC-SW)\\n\\nFeature size\\n\\ndependability, 33\\nintegrated circuits, 19–21\\nFEC, see Forward error correction \\n(FEC)\\n\\nFederal Communications Commission \\n\\n(FCC), telephone \\ncompany outages, D-15\\n\\nFermi GPU\\n\\narchitectural innovations, 305–308\\nfuture features, 333\\nGrid mapping, 293\\nmultithreaded SIMD Processor, \\n\\n307\\n\\nNVIDIA, 291, 305\\nSIMD, 296–297\\nSIMD Thread Scheduler, 306\\nFermi Tesla, GPU computing history, \\n\\nL-52\\n\\nFermi Tesla GTX 280\\n\\nGPU comparison, 324–325, 325\\nmemory bandwidth, 328\\nraw/relative GPU performance, \\n\\n328\\n\\nsynchronization, 329\\nweaknesses, 330\\nFermi Tesla GTX 480\\nfloorplan, 295\\nGPU comparisons, 323–330, 325\\n\\nFetch-and-increment\\n\\nlarge-scale multiprocessor \\n\\nsynchronization, I-20 to \\nI-21\\n\\nsense-reversing barrier, I-21\\nsynchronization, 388\\nFetching, see Data fetching\\nFetch stage, TI 320C55 DSP, E-7\\nFFT, see Fast Fourier transformation \\n\\n(FFT)\\n\\nFibre Channel (FC), F-64, F-67, F-102\\n\\n\\x0c(FPGAs), WSC array \\nswitch, 443\\n\\nFlash memory\\n\\nfile system benchmarking, D-20\\nNetApp FAS6000 filer, D-42\\nFibre Channel Arbitrated Loop \\n\\n(FC-AL), F-102\\n\\nblock servers vs.', ' window size, 217\\n\\n\\x0cI-28 ■\\n\\nIndex\\n\\nFloating-point operations (continued)\\npipeline hazards and forwarding, \\nC-55 to C-57\\npipeline structural hazards, C-16\\nprecisions, J-33 to J-34\\nremainder, J-31 to J-32\\nROB commit, 187\\nSMT, 398–400\\nSPARC, K-31\\nSPEC benchmarks, 39\\nspecial values, J-14 to J-15\\nstalls from RAW hazards, C-55\\nstatic branch prediction, C-26 to \\n\\nC-27\\nTomasulo’s algorithm, 185\\nunderflow, J-36 to J-37, J-62\\nVAX, B-73\\nvector chaining, G-11\\nvector sequence chimes, 270\\nVLIW processors, 195\\nVMIPS, 264\\n\\nFloating-point registers (FPRs)\\n\\nIA-64, H-34\\nIBM Blue Gene/L, I-42\\nMIPS data transfers, A-34\\nMIPS operations, A-36\\nMIPS64 architecture, A-34\\nwrite-back, C-56\\n\\ncalculation, 47–48\\nCPI calculations, 50–51\\n\\nFloating Point Systems AP-120B, \\nL-28\\nFloppy disks, L-78\\nFlow-balanced state, D-23\\nFlow control\\n\\nand arbitration, F-21\\ncongestion management, F-65\\ndirect networks, F-38 to F-39\\nformat, F-58\\ninterconnection networks, F-10 to \\n\\nsystem area network history, F-100 \\n\\nF-11\\n\\nto F-101\\n\\ndependence analysis, H-6\\ninteger division/remainder, J-12\\nloop-level parallelism \\n\\ndependences, 320–321\\n\\nMIPS scoreboarding, C-77\\nperformance measurement history, \\n\\nL-6\\n\\nreturn address predictors, 206\\nForward error correction (FEC), DSP, \\nE-5 to E-7\\n\\nFull adders, J-2, J-3\\nFully associative cache\\n\\nblock placement, B-7\\nconflict misses, B-23\\ndirect-mapped cache, B-9\\nmemory hierarchy basics, 74\\n\\nFully connected topology\\n\\ndistributed switched networks, \\n\\nF-34\\n\\nNEWS communication, F-43\\n\\nForwarding, see also Bypassing\\n\\nFunctional hazards\\n\\nALUs, C-40 to C-41\\ndata hazard stall minimization, \\n\\nC-16 to C-19, C-18\\n\\ndynamically scheduled pipelines, \\nC-70 to C-71\\n\\nload instruction, C-20\\nlonger latency pipelines, C-54 to \\n\\nC-58\\noperand, C-19\\nForwarding table\\n\\nrouting implementation, F-57\\nswitch microarchitecture \\n\\npipelining, F-60\\n\\nForward path, cell phones, E-24\\nFourier-Motzkin algorithm, L-31\\nFourier transform, DSP, E-5\\nFour-way conflict misses, definition, \\n\\nFP, see Floating-point (FP) operations\\nFPGAs, see Field-programmable gate \\n\\narrays (FPGAs)\\n\\nFPRs, see Floating-point registers \\n(FPRs)\\n\\nFPSQR, see Floating-point square root \\n(FPSQR)\\n\\nFrame pointer, VAX, K-71\\nFreeze, branch penalty reduction, \\nC-22\\n\\nFrequency modulation (FM), wireless \\n\\nneworks, E-21\\n\\nFront-end stage, Itanium 2, H-42\\nFU, see Functional unit (FU)\\nFujitsu Primergy BX3000 blade \\n\\nARM Cortex-A8, 233\\nmicroarchitectural techniques case \\nstudy, 247–254\\n\\nFunctional unit (FU)\\n\\nFP operations, C-66\\ninstruction execution example, \\n\\nC-80\\n\\nIntel Core i7, 237\\nItanium 2, H-41 to H-43\\nlatencies, C-53\\nMIPS pipeline, C-52\\nMIPS scoreboarding, C-75 to C-80\\nOCNs, F-3\\nvector add instruction, 272, \\n272–273\\n\\nVMIPS, 264\\n\\nFunction calls\\n\\nGPU programming, 289\\nNVIDIA GPU Memory structures, \\n\\n304–305\\nPTX assembler, 301\\n\\nFunction pointers, control flow \\n\\ninstruction addressing \\nmodes, A-18\\n\\nFused multiply-add, floating point, \\n\\nJ-32 to J-33\\n\\nFuture file, precise exceptions, C-59\\n\\nG\\nGateways, Ethernet, F-79\\nGather-Scatter\\n\\ndefinition, 309\\nGPU comparisons, 329\\nmultimedia instruction compiler \\nsupport, A-31\\n\\nsparse matrices, G-13 to G-14\\nvector architectures, 279–280\\nGCD, see Greatest common divisor \\n\\n(GCD) test\\nGDDR, see Graphics double data rate \\n(GDDR)\\n\\nFluent, F-76, F-77\\nFlush, branch penalty reduction, C-22\\nFM, see Frequency modulation (FM)\\nForm factor, interconnection \\n\\nserver, F-85\\nFujitsu VP100, L-45, L-47\\nFujitsu VP200, L-45, L-47\\nFull access\\n\\nnetworks, F-9 to F-12\\n\\ndimension-order routing, F-47 to \\n\\nFORTRAN\\n\\ncompiler types and classes, A-28\\ncompiler vectorization, G-14, G-15\\n\\nF-48\\n\\nF-29\\n\\ninterconnection network topology, \\n\\nFloating-point square root (FPSQR)\\n\\nB-23\\n\\n\\x0cGDRAM, see Graphics dynamic \\n\\nGlobal optimizations\\n\\nrandom-access memory \\n(GDRAM)\\n\\ncompilers, A-26, A-29\\noptimization types, A-28\\n\\ncomputing history, L-52\\ndefinition, 9\\nDLP\\n\\nGE 645, L-9\\nGeneral-Purpose Computing on GPUs \\n(GPGPU), L-51 to L-52\\nGeneral-purpose electronic computers, \\n\\nhistorical background, \\nL-2 to L-4\\n\\nGeneral-purpose registers (GPRs)\\nadvantages/disadvantages, A-6\\nIA-64, H-38\\nIntel 80x86, K-48\\nISA classification, A-3 to A-5\\nMIPS data transfers, A-34\\nMIPS operations, A-36\\nMIPS64, A-34\\nVMIPS, 265\\n\\nGENI, see Global Environment for \\nNetwork Innovation \\n(GENI)\\n\\nGeometric means, example \\n\\ncalculations, 43–44\\n\\nGFS, see Google File System (GFS)\\nGibson mix, L-6\\nGiga Thread Engine, definition, 292, \\n\\n314\\nGlobal address space, segmented \\n\\nvirtual memory, B-52\\n\\nGlobal code scheduling\\nexample, H-16\\nparallelism, H-15 to H-23\\nsuperblock scheduling, H-21 to \\nH-23, H-22\\ntrace scheduling, H-19 to H-21, \\n\\nH-20\\nGlobal common subexpression \\n\\nelimination, compiler \\nstructure, A-26\\n\\nGlobal data area, and compiler \\ntechnology, A-27\\n\\nGlobal Environment for Network \\nInnovation (GENI), \\nF-98\\nGlobal load/store, definition, 309\\nGlobal Memory\\n\\ndefinition, 292, 314\\nGPU programming, 290\\nlocks via coherence, 390\\n\\nGlobal miss rate\\n\\ndefinition, B-31\\nmultilevel caches, B-33\\n\\nGlobal Positioning System, CDMA, E-25\\nGlobal predictors\\n\\nIntel Core i7, 166\\ntournament predictors, 164–166\\n\\nGlobal scheduling, ILP, VLIW \\n\\nprocessor, 194\\n\\nGlobal system for mobile \\n\\ncommunication (GSM), \\ncell phones, E-25\\n\\nGoldschmidt’s division algorithm, \\n\\nJ-29, J-61\\n\\nGoldstine, Herman, L-2 to L-3\\nGoogle\\n\\nBigtable, 438, 441\\ncloud computing, 455\\ncluster history, L-62\\ncontainers, L-74\\nMapReduce, 437, 458–459, 459\\nserver CPUs, 440\\nserver power-performance \\n\\nbenchmarks, 439–441\\n\\nWSCs, 432, 449\\n\\ncontainers, 464–465, 465\\ncooling and power, 465–468\\nmonitoring and repairing, \\n\\n469–470\\n\\nPUE, 468\\nservers, 467, 468–469\\n\\nGoogle App Engine, L-74\\nGoogle Clusters\\n\\nmemory dependability, 104\\npower consumption, F-85\\n\\nGoogle File System (GFS)\\n\\nMapReduce, 438\\nWSC storage, 442–443\\n\\nGoogle Goggles\\nPMDs, 6\\nuser experience, 4\\n\\nGoogle search\\n\\nshared-memory workloads, 369\\nworkload demands, 439\\n\\nGordon Bell Prize, L-57\\nGPGPU (General-Purpose Computing \\non GPUs), L-51 to L-52\\nGPRs, see General-purpose registers \\n(GPRs)\\n\\nGPU (Graphics Processing Unit)\\n\\nbanked and graphics memory, \\n322–323\\n\\nIndex\\n\\n■\\n\\nI-29\\n\\nbasic considerations, 288\\nbasic PTX thread instructions, \\n\\n299\\n\\nconditional branching, 300–303\\ncoprocessor relationship, \\n\\n330–331\\ndefinitions, 309\\nFermi GPU architecture \\n\\ninnovations, 305–308\\n\\nFermi GTX 480 floorplan, 295\\nGPUs vs.', ' vector architectures, \\n\\n308–312, 310\\n\\nmapping examples, 293\\nMultimedia SIMD comparison, \\n\\n312\\n\\nmultithreaded SIMD Processor \\nblock diagram, 294\\n\\nNVIDIA computational \\n\\nstructures, 291–297\\nNVIDIA/CUDA and AMD \\n\\nterminology, 313–315\\nNVIDIA GPU ISA, 298–300\\nNVIDIA GPU Memory \\n\\nstructures, 304, 304–305\\n\\nprogramming, 288–291\\nSIMD thread scheduling, 297\\nterminology, 292\\n\\nfine-grained multithreading, 224\\nfuture features, 332\\ngather/scatter operations, 280\\nhistorical background, L-50\\nloop-level parallelism, 150\\nvs.', ' vector processor operation, \\n\\n276\\n\\n\\x0cI-30 ■\\n\\nIndex\\n\\nGPU Memory\\ncaches, 306\\nCUDA program, 289\\ndefinition, 292, 309, 314\\nfuture architectures, 333\\nGPU programming, 288\\nNVIDIA, 304, 304–305\\nsplitting from main memory, 330\\n\\nGradual underflow, J-15, J-36\\nGrain size\\n\\nMIMD, 10\\nTLP, 346\\n\\nGrant phase, arbitration, F-49\\nGraph coloring, register allocation, \\n\\nA-26 to A-27\\n\\nGraphics double data rate (GDDR)\\n\\ncharacteristics, 102\\nFermi GTX 480 GPU, 295, 324\\nGraphics dynamic random-access \\nmemory (GDRAM)\\n\\nbandwidth issues, 322–323\\ncharacteristics, 102\\n\\nGraphics-intensive benchmarks, \\n\\nSIMD Processors, 295\\nThread Blocks, 295\\n\\nGrid computing, L-73 to L-74\\nGrid topology\\n\\ncharacteristics, F-36\\ndirect networks, F-37\\n\\nGSDRAM, see Graphics synchronous \\ndynamic random-access \\nmemory (GSDRAM)\\nGSM, see Global system for mobile \\ncommunication (GSM)\\n\\nGuest definition, 108\\nGuest domains, Xen VM, 111\\n\\nH\\nHadoop, WSC batch processing, 437\\nHalf adders, J-2\\nHalf words\\n\\naligned/misaligned addresses, A-8\\nmemory address interpretation, \\nA-7 to A-8\\n\\nMIPS data types, A-34\\noperand sizes/types, 12\\nas operand type, A-13 to A-14\\n\\ndesktop performance, 38\\n\\nGraphics pipelines, historical \\n\\nHandshaking, interconnection \\n\\nbackground, L-51\\n\\nGraphics Processing Unit, see GPU \\n(Graphics Processing \\nUnit)\\nGraphics synchronous dynamic \\n\\nnetworks, F-10\\n\\nHard drive, power consumption, 63\\nHard real-time systems, definition, E-3 \\n\\nto E-4\\n\\nHardware\\n\\nGraphics Synthesizer, Sony \\n\\nrandom-access memory \\n(GSDRAM), \\ncharacteristics, 102\\n\\nPlayStation 2, E-16, \\nE-16 to E-17\\n\\nGreater than condition code, \\n\\nPowerPC, K-10 to K-11\\nGreatest common divisor (GCD) test, \\n\\nloop-level parallelism \\ndependences, 319, H-7\\n\\nGrid\\n\\narithmetic intensity, 286\\nCUDA parallelism, 290\\ndefinition, 292, 309, 313\\nand GPU, 291\\nGPU Memory structures, 304\\nGPU terms, 308\\nmapping example, 293\\nNVIDIA GPU computational \\n\\nstructures, 291\\n\\nas architecture component, 15\\ncache optimization, 96\\ncompiler scheduling support, L-30 \\n\\nto L-31\\ncompiler speculation support\\nmemory references, H-32\\noverview, H-27\\npreserving exception behavior, \\n\\nH-28 to H-32\\n\\ndescription notation, K-25\\nenergy/performance fallacies, 56\\nfor exposing parallelism, H-23 to \\n\\nH-27\\n\\nILP approaches, 148, 214–215\\ninterconnection networks, F-9\\npipeline hazard detection, C-38\\nVirtual Machines protection, 108\\nWSC cost-performance, 474\\nWSC running service, 434–435\\n\\nHardware-based speculation\\nbasic algorithm, 191\\n\\ndata flow execution, 184\\nFP unit using Tomasulo’s \\nalgorithm, 185\\n\\nILP\\n\\ndata flow execution, 184\\nwith dynamic scheduling and \\nmultiple issue, 197–202\\n\\nFP unit using Tomasulo’s \\nalgorithm, 185\\n\\nkey ideas, 183–184\\nmultiple-issue processors, 198\\nreorder buffer, 184–192\\nvs.', ' software speculation, \\n221–222\\nkey ideas, 183–184\\n\\nHardware faults, storage systems, \\nD-11\\n\\nHardware prefetching\\n\\ncache optimization, 131–133\\nmiss penalty/rate reduction, 91–92\\nNVIDIA GPU Memory structures, \\n\\n305\\n\\nSPEC benchmarks, 92\\n\\nHardware primitivies\\n\\nbasic types, 387–389\\nlarge-scale multiprocessor \\n\\nsynchronization, I-18 to \\nI-21\\n\\nsynchronization mechanisms, \\n387–389\\n\\nHarvard architecture, L-4\\nHazards, see also Data hazards\\n\\nbranch hazards, C-21 to C-26, \\nC-39 to C-42, C-42\\n\\ncontrol hazards, 235, C-11\\ndetection, hardware, C-38\\ndynamically scheduled pipelines, \\nC-70 to C-71\\n\\nexecution sequences, C-80\\nfunctional hazards, 233, 247–254\\ninstruction set complications, C-50\\nlonger latency pipelines, C-54 to \\n\\nC-58\\n\\nstructural hazards, 268–269, C-11, \\n\\nC-13 to C-16, C-71, \\nC-78 to C-79\\n\\nHCAs, see Host channel adapters \\n(HCAs)\\n\\nHeader\\n\\nmessages, F-6\\npacket format, F-7\\n\\n\\x0ccharacteristics, F-20\\n\\nhistory, G-28\\n\\ninterconnection network topology, \\n\\nHP-Compaq servers\\n\\nswitch microarchitecture \\n\\nHigh-level language computer \\n\\nHops\\n\\npipelining, F-60\\n\\nTCP/IP, F-84\\n\\narchitecture (HLLCA), \\nL-18 to L-19\\n\\nHead-of-line (HOL) blocking\\n\\nHigh-level optimizations, compilers, \\n\\ncongestion management, F-64\\nswitch microarchitecture, F-58 to \\nF-59, F-59, F-60, F-62\\n\\nsystem area network history, F-101\\nvirtual channels and throughput, \\n\\nF-93\\n\\nHeap, and compiler technology, A-27 \\nto A-28\\nHEP processor, L-34\\nHeterogeneous architecture, \\ndefinition, 262\\n\\nHewlett-Packard AlphaServer, \\n\\nHewlett-Packard PA-RISC\\naddressing modes, K-5\\narithmetic/logical instructions, \\n\\ncharacteristics, K-4\\nconditional branches, K-12, K-17, \\n\\nF-100\\n\\nK-11\\n\\nK-34\\n\\nconstant extension, K-9\\nconventions, K-13\\ndata transfer instructions, K-10\\nEPIC, L-32\\nfeatures, K-44\\nfloating-point precisions, J-33\\nFP instructions, K-23\\nMIPS core extensions, K-23\\nmultimedia support, K-18, K-18, \\n\\nK-19\\n\\nunique instructions, K-33 to K-36\\n\\nHewlett-Packard PA-RISC MAX2, \\nmultimedia support, \\nE-11\\n\\nHewlett-Packard Precision \\n\\nArchitecture, integer \\narithmetic, J-12\\n\\nHewlett-Packard ProLiant BL10e G2 \\n\\nBlade server, F-85\\n\\nA-26\\n\\nHighly parallel memory systems, case \\n\\nstudies, 133–136\\n\\nHigh-order functions, control flow \\n\\ninstruction addressing \\nmodes, A-18\\n\\nHigh-performance computing (HPC)\\n\\nInfiniBand, F-74\\ninterconnection network \\n\\nF-44\\n\\nstorage area network history, F-102\\nswitch microarchitecture, F-56\\nvector processor history, G-27\\nwrite strategy, B-10\\nvs.', '3 (Ethernet), F-77 \\n\\nto F-79\\nLAN history, F-99\\n\\nIF, see Instruction fetch (IF) cycle\\nIF statement handling\\n\\ncontrol dependences, 154\\nGPU conditional branching, 300, \\n\\n302–303\\n\\nmemory consistency, 392\\nvectorization in code, 271\\nvector-mask registers, 267, 275–276\\n\\nIlliac IV, F-100, L-43, L-55\\nILP, see Instruction-level parallelism \\n\\n(ILP)\\n\\nImmediate addressing mode\\nALU operations, A-12\\nbasic considerations, A-10 to A-11\\nMIPS, 12\\nMIPS instruction format, A-35\\nMIPS operations, A-37\\nvalue distribution, A-13\\n\\nIMPACT, L-31\\nImplicit operands, ISA classifications, \\n\\nIndirect addressing, VAX, K-67\\nIndirect networks, definition, F-31\\nInexact exception\\n\\nfloating-point arithmetic, J-35\\nfloating-point underflow, J-36\\nInfiniBand, F-64, F-67, F-74 to F-77\\n\\ncluster history, L-63\\npacket format, F-75\\nstorage area network history, \\n\\nF-102\\n\\nswitch vs.', ' NIC, F-86\\nsystem area network history, F-101\\n\\nInfinite population model, queuing \\n\\nIn flight instructions, ILP hardware \\n\\nInformation tables, examples, \\n\\nInfrastructure costs\\n\\nWSC, 446–450, 452–455, 453\\nWSC efficiency, 450–452\\n\\nInitiation interval, MIPS pipeline FP \\n\\nmodel, D-30\\n\\nmodel, 214\\n\\n176–177\\n\\nA-3\\n\\nInitiation rate\\n\\nImplicit unit stride, GPUs vs.', ' vector \\n\\nfloating-point pipeline, C-65 to \\n\\narchitectures, 310\\n\\nImprecise exceptions\\n\\ndata hazards, 169–170\\nfloating-point, 188\\n\\nIMT-2000, see International Mobile \\n\\nC-66\\nmemory banks, 276–277\\nvector execution time, 269\\n\\nInktomi, L-62, L-73\\nIn-order commit\\n\\nTelephony 2000 \\n(IMT-2000)\\n\\nInactive power modes, WSCs, 472\\nInclusion\\n\\ncache hierarchy, 397–398\\nimplementation, 397–398\\ninvalidate protocols, 357\\nmemory hierarchy history, L-11\\n\\nIndexed addressing\\n\\nIntel 80x86, K-49, K-58\\nVAX, K-67\\n\\nIndexes\\n\\naddress translation during, B-36 to \\n\\nAMD Opteron data cache, B-13 to \\n\\nB-40\\n\\nB-14\\n\\nARM Cortex-A8, 115\\nrecurrences, H-12\\nsize equations, B-22\\n\\nhardware-based speculation, \\n188–189\\n\\nspeculation concept origins, L-29\\n\\nIn-order execution\\n\\naverage memory access time, B-17 \\n\\nto B-18\\n\\ncache behavior calculations, B-18\\ncache miss, B-2 to B-3\\ndynamic scheduling, 168–169\\nIBM Power processors, 247\\nILP exploitation, 193–194\\nmultiple-issue processors, 194\\nsuperscalar processors, 193\\nIn-order floating-point pipeline, \\n\\ndynamic scheduling, \\n169\\n\\nIn-order issue\\n\\nARM Cortex-A8, 233\\ndynamic scheduling, 168–170, \\n\\nIndex\\n\\n■\\n\\nI-33\\n\\nInput buffered switch\\n\\nHOL blocking, F-59, F-60\\nmicroarchitecture, F-57, F-57\\npipelined version, F-61\\nInput-output buffered switch, \\n\\nmicroarchitecture, F-57\\n\\nInstruction cache\\n\\nAMD Opteron example, B-15\\nantialiasing, B-38\\napplication/OS misses, B-59\\nbranch prediction, C-28\\ncommercial workload, 373\\nGPU Memory, 306\\ninstruction fetch, 202–203, 237\\nISA, 241\\nMIPS R4000 pipeline, C-63\\nmiss rates, 161\\nmultiprogramming workload, \\n374–375\\n\\nprefetch, 236\\nRISCs, A-23\\nTI TMS320C55 DSP, E-8\\n\\nhardware-based speculation, \\n\\n184–185, 187–188, 188, \\n190\\n\\ninstruction set complications, C-49\\nIntel Core i7, 237\\nspeculation support, 208–209\\n\\nInstruction count (IC)\\n\\naddressing modes, A-10\\ncache performance, B-4, B-16\\ncompiler optimization, A-29, A-29 \\n\\nto A-30\\n\\nprocessor performance time, 49–51\\nRISC history, L-22\\nInstruction decode (ID)\\n\\nbasic MIPS pipeline, C-36\\nbranch hazards, C-21\\ndata hazards, 169\\nhazards and forwarding, C-55 to \\n\\nMIPS pipeline, C-71\\nMIPS pipeline control, C-36 to \\n\\nC-57\\n\\nC-39\\n\\nMIPS pipeline FP operations, C-53\\nMIPS scoreboarding, C-72 to C-74\\nout-of-order execution, 170\\npipeline branch issues, C-39 to \\nC-41, C-42\\nRISC classic pipeline, C-7 to C-8, \\n\\nIndex field, block identification, B-8\\nIndex vector, gather/scatter operations, \\n279–280\\n\\nC-71\\n\\nISA, 241\\n\\nIn-order scalar processors, VMIPS, 267\\n\\nC-10\\n\\noperations, C-52 to C-53\\n\\nInstruction commit\\n\\n\\x0cI-34 ■\\n\\nIndex\\n\\nInstruction decode (continued )\\n\\nsimple MIPS implementation, C-31\\nsimple RISC implementation, C-5 \\n\\nInstruction delivery stage, Itanium 2, \\n\\nto C-6\\n\\nH-42\\n\\nInstruction fetch (IF) cycle\\n\\nbasic MIPS pipeline, C-35 to C-36\\nbranch hazards, C-21\\nbranch-prediction buffers, C-28\\nexception stopping/restarting, C-46 \\n\\nto C-47\\n\\nMIPS exceptions, C-48\\nMIPS R4000, C-63\\npipeline branch issues, C-42\\nRISC classic pipeline, C-7, C-10\\nsimple MIPS implementation, \\n\\nC-31\\n\\nsimple RISC implementation, C-5\\n\\nInstruction fetch units\\n\\nintegrated, 207–208\\nIntel Core i7, 237\\n\\nInstruction formats\\n\\nARM-unique, K-36 to K-37\\nhigh-level language computer \\n\\narchitecture, L-18\\n\\nIA-64 ISA, H-34 to H-35, H-38, \\n\\nH-39\\n\\nIBM 360, K-85 to K-88\\nIntel 80x86, K-49, K-52, K-56 to \\n\\nK-57\\nM32R-unique, K-39 to K-40\\nMIPS16-unique, K-40 to K-42\\nPA-RISC unique, K-33 to K-36\\nPowerPC-unique, K-32 to K-33\\nRISCs, K-43\\n\\nAlpha-unique, K-27 to K-29\\narithmetic/logical, K-11, K-15\\nbranches, K-25\\ncontrol instructions, K-12, \\n\\nK-16\\n\\ndata transfers, K-10, K-14, \\n\\nK-21\\ndesktop/server, K-7\\ndesktop/server systems, K-7\\nembedded DSP extensions, \\n\\nembedded systems, K-8\\nFP instructions, K-13\\nhardware description notation, \\n\\nK-19\\n\\nK-25\\n\\nMIPS core, K-6 to K-9\\nMIPS core extensions, K-19 to \\n\\nMIPS unaligned word reads, \\n\\nK-24\\n\\nK-26\\n\\nmultimedia extensions, K-16 to \\n\\nK-19\\noverview, K-5 to K-6\\nSPARC-unique, K-29 to K-32\\nSuperH-unique, K-38 to K-39\\nThumb-unique, K-37 to K-38\\nInstruction groups, IA-64, H-34\\nInstruction issue\\n\\ndefinition, C-36\\nDLP, 322\\ndynamic scheduling, 168–169, \\n\\nC-71 to C-72\\n\\nILP, 197, 216–217\\ninstruction-level parallelism, 2\\nIntel Core i7, 238\\nItanium 2, H-41 to H-43\\nMIPS pipeline, C-52\\nmultiple issue processor, 198\\nmultithreading, 223, 226\\nparallelism measurement, 215\\nprecise exceptions, C-58, C-60\\nprocessor comparison, 323\\nROB, 186\\nspeculation support, 208, 210\\nTomasulo’s scheme, 175, 182\\nInstruction-level parallelism (ILP)\\nARM Cortex-A8, 233–236, \\n235–236\\n\\nbasic concepts/challenges, \\n\\n148–149, 149\\n\\n“big and dumb” processors, 245\\nbranch-prediction buffers, C-29, \\nC-29 to C-30\\n\\ncompiler scheduling, L-31\\ncompiler techniques for exposure, \\n\\n156–162\\ncontrol dependence, 154–156\\ndata dependences, 150–152\\ndata flow limit, L-33\\ndefinition, 9, 149–150\\ndynamic scheduling\\n\\nbasic concept, 168–169\\ndefinition, 168\\nexample and algorithms, \\n176–178\\n\\novercoming data hazards, \\n\\n167–176\\nTomasulo’s algorithm, \\n\\n170–176, 178–179, \\n181–183\\n\\nearly studies, L-32 to L-33\\nexploitation methods, H-22 to \\n\\nH-23\\nexploitation statically, H-2\\nexposing with hardware support, \\n\\nH-23\\nGPU programming, 289\\nhardware-based speculation, \\n183–192\\n\\nhardware vs.', ' success, A-44 to A-45\\nGPR advantages/disadvantages, \\n\\nA-6\\n\\nhigh-level considerations, A-39, \\nA-41 to A-43\\n\\nhigh-level language computer \\narchitecture, L-18 to \\nL-19\\n\\nIA-64\\n\\ninstruction formats, H-39\\ninstructions, H-35 to H-37\\ninstruction set basics, H-38\\noverview, H-32 to H-33\\npredication and speculation, \\nH-38 to H-40\\nIBM 360, K-85 to K-88\\nimmediate addressing mode, A-10 \\n\\nliteral addressing mode, A-10 to \\n\\nto A-11\\n\\nA-11\\n\\nIndex\\n\\n■\\n\\nI-35\\n\\nregisters, A-34\\nusage, A-39\\nMIPS64, 14, A-40\\nmultimedia instruction compiler \\nsupport, A-31 to A-32\\n\\nNVIDIA GPU, 298–300\\noperand locations, A-4\\noperands per ALU instruction, A-6\\noperand type and size, A-13 to \\n\\nA-14\\noperations, A-14 to A-16\\noperator categories, A-15\\noverview, K-2\\nperformance and efficiency \\n\\nprediction, 241–243\\n\\nand protection, 112\\nRISC code size, A-23 to A-24\\nRISC history, L-19 to L-22, L-21\\nstack architectures, L-16 to L-17\\ntop 80x86 instructions, A-16\\n“typical” program fallacy, A-43\\nVirtual Machines protection, \\n107–108\\n\\nVirtual Machines support, \\n\\n109–110\\n\\nVMIPS, 264–265\\nVMM implementation, 128–129\\n\\nInstructions per clock (IPC)\\nARM Cortex-A8, 236\\nflawless architecture design, A-45\\nILP for realizable processors, \\n216–218\\n\\nMIPS scoreboarding, C-72\\nmultiprocessing/\\n\\nmultithreading-based \\nperformance, 398–400\\n\\nmemory addressing, A-11 to A-13\\nmemory address interpretation, \\nA-7 to A-8\\n\\nprocessor performance time, 49\\nSun T1 multithreading unicore \\n\\nperformance, 229\\n\\ncontrol flow instructions, A-37 \\n\\naddition speedup\\n\\naddressing modes for data \\ntransfer, A-34\\nbasic considerations, A-32 to \\n\\nA-33\\n\\nto A-38\\n\\ndata types, A-34\\ndynamic instruction mix, A-41 \\n\\nto A-42, A-42\\nFP operations, A-38 to A-39\\ninstruction format, A-35\\nMIPS operations, A-35 to A-37\\n\\nSun T1 processor, 399\\n\\nInstruction status\\n\\ndynamic scheduling, 177\\nMIPS scoreboarding, C-75\\n\\nInteger arithmetic\\n\\ncarry-lookahead, J-37 to J-41\\ncarry-lookahead circuit, J-38\\ncarry-lookahead tree, J-40\\ncarry-lookahead tree adder, \\n\\ncarry-select adder, J-43, J-43 to \\n\\nJ-41\\n\\nJ-44, J-44\\n\\ncompiler register allocation, A-26 \\n\\nMIPS\\n\\n\\x0cI-36 ■\\n\\nIndex\\n\\nInteger arithmetic (continued )\\n\\ncarry-skip adder, J-41 to J43, \\n\\nJ-42\\n\\noverview, J-37\\n\\ndivision\\n\\nradix-2 division, J-55\\nradix-4 division, J-56\\nradix-4 SRT division, J-57\\nwith single adder, J-54 to J-58\\n\\nFP conversions, J-62\\nlanguage comparison, J-12\\nmultiplication\\n\\narray multiplier, J-50\\nBooth recoding, J-49\\neven/odd array, J-52\\nwith many adders, J-50 to J-54\\nmultipass array multiplier, J-51\\nsigned-digit addition table, \\n\\nJ-54\\n\\nwith single adder, J-47 to J-49, \\n\\nJ-48\\nWallace tree, J-53\\n\\nmultiplication/division, shifting \\n\\nover zeros, J-45 to J-47\\n\\noverflow, J-11\\nRadix-2 multiplication/division, \\nJ-4, J-4 to J-7\\n\\nrestoring/nonrestoring division, \\n\\nripply-carry addition, J-2 to J-3, \\n\\nJ-6\\n\\nJ-3\\n\\nsigned numbers, J-7 to J-10\\nSRT division, J-45 to J-47, J-46\\nsystems issues, J-10 to J-13\\n\\nInteger operand\\n\\nflawed architecture, A-44\\nGCD, 319\\ngraph coloring, A-27\\ninstruction set encoding, A-23\\nMIPS data types, A-34\\nas operand type, 12, A-13 to A-14\\n\\nInteger operations\\n\\naddressing modes, A-11\\nALUs, A-12, C-54\\nARM Cortex-A8, 116, 232, 235, \\n\\n236\\nbenchmarks, 167, C-69\\nbranches, A-18 to A-20, A-20\\ncache misses, 83–84\\ndata access distribution, A-15\\ndata dependences, 151\\ndependences, 322\\n\\ndesktop benchmarks, 38–39\\ndisplacement values, A-12\\nexceptions, C-43, C-45\\nhardware ILP model, 215\\nhardware vs.', ' Alpha processors, 368\\narchitecture, 15\\nbasic function, 236–238\\n“big and dumb” processors, 245\\nbranch predictor, 166–167\\nclock rate, 244\\ndynamic scheduling, 170\\nGPU comparisons, 324–330, 325\\nhardware prefetching, 91\\nISA performance and efficiency \\n\\nprediction, 241–243\\n\\nL2/L3 miss rates, 125\\nmemory hierarchy basics, 78, \\n\\n117–124, 119\\n\\nmemory hierarchy design, 73\\nmemory performance, 122–124\\nMESIF protocol, 362\\nmicroprocessor die example, 29\\nmiss rate benchmarks, 123\\nmultibanked caches, 86\\nmultithreading, 225\\nnonblocking cache, 83\\nperformance, 239, 239–241, 240\\nperformance/energy efficiency, \\n401–405\\n\\npipelined cache access, 82\\npipeline structure, 237\\nprocessor comparison, 242\\nraw/relative GPU performance, 328\\nRoofline model, 286–288, 287\\n\\n\\x0cIntel 8087, floating point remainder, \\n\\nIntel MMX, multimedia instruction \\n\\nmeasurements, K-62 to \\nK-64\\n\\nto B-54\\n\\nIntel IA-64 architecture\\n\\nIntel Core i7 (continued )\\n\\nsingle-threaded benchmarks, 243\\nSMP limitations, 363\\nSMT, 230–231\\nsnooping cache coherence \\n\\nimplementation, 365\\nthree-level cache hierarchy, 118\\nTLB structure, 118\\nwrite invalid protocol, 356\\n\\nIntel 80x86 processors\\n\\naddress encoding, K-58\\naddressing modes, K-58\\naddress space, B-58\\narchitecture flaws and success, K-81\\narchitecture flaws vs.', ' vector architectures, 282\\n\\nIntel Teraflops processors, OCNs, F-3\\nIntel Thunder Tiger 4 QsNetII, F-63, \\n\\nF-76\\n\\nIntel VT-x, 129\\nIntel x86\\n\\nAmazon Web Services, 456\\nAVX instructions, 284\\nclock rates, 244\\ncomputer architecture, 15\\nconditional instructions, H-27\\nGPUs as coprocessors, 330–331\\nIntel Core i7, 237–238\\nMultimedia SIMD Extensions, \\n282–283\\n\\nNVIDIA GPU ISA, 298\\nparallelism, 262–263\\nperformance and energy \\nefficiency, 241\\n\\nvs.', ' GPUs, 312\\nmultiprocessor cost effectiveness, \\n\\n407\\nperformance, D-15 to D-16\\nSANs, F-3 to F-4\\nshared-media networks, F-23\\nswitched networks, F-2\\nswitch vs.', ' NIC, F-86\\nVirtual Machines impact, 110–111\\nwrite strategy, B-11\\nXen VM, 111\\n\\nInternal Mask Registers, definition, \\n\\napproach, H-10\\n\\nI/O interfaces\\n\\n309\\n\\nInterprocessor communication, \\n\\nlarge-scale \\nmultiprocessors, I-3 to \\nI-6\\n\\ndisk storage, D-4\\nstorage area network history, \\n\\nF-102\\n\\nI/O latency, shared-memory \\n\\nInternational Computer Architecture \\n\\nSymposium (ISCA), \\nL-11 to L-12\\n\\nInternational Mobile Telephony 2000 \\n(IMT-2000), cell phone \\nstandards, E-25\\n\\nInternet\\n\\nAmazon Web Services, 457\\narray switch, 443\\ncloud computing, 455–456, 461\\ndata-intensive applications, 344\\ndependability, 33\\nGoogle WSC, 464\\nLayer 3 network linkage, 445\\nNetflix traffic, 460\\nSaaS, 4\\nWSC efficiency, 452\\nWSC memory hierarchy, 445\\nWSCs, 432–433, 435, 437, 439, \\n446, 453–455\\n\\ncontainer history, L-74 to L-75\\noverview, D-37\\nperformance, dependability, cost, \\nD-38 to D-40\\n\\nTB-80 cluster MTTF, D-40 to \\n\\nD-41\\nTB-80 VME rack, D-38\\n\\nInternet Protocol (IP)\\n\\ninternetworking, F-83\\nstorage area network history, \\n\\nF-102\\n\\nWAN history, F-98\\n\\nInternet Protocol (IP) cores, OCNs, F-3\\nInternet Protocol (IP) routers, VOQs, \\n\\nF-60\\n\\nInternetworking\\n\\ndirectory-based cache coherence \\n\\nI/O network, commercial \\n\\nInterrupt, see Exceptions\\nInvalidate protocol\\n\\nprotocol example, \\n382–383\\nexample, 359, 360\\nimplementation, 356–357\\nsnooping coherence, 355, 355–356\\n\\nInvalid exception, floating-point \\n\\narithmetic, J-35\\n\\nInverted page table, virtual memory \\n\\nblock identification, \\nB-44 to B-45\\n\\nI/O bandwidth, definition, D-15\\nI/O benchmarks, response time \\nrestrictions, D-18\\n\\nI/O bound workload, Virtual Machines \\n\\nI/O bus\\n\\nhistorical background, L-80 to L-81\\ninterconnection networks, F-88\\npoint-to-point replacement, D-34\\nSony PlayStation 2 Emotion \\n\\nEngine case study, E-15\\n\\nI/O cache coherency, basic \\n\\nconsiderations, 113\\n\\nI/O devices\\n\\naddress translation, B-38\\naverage memory access time, B-17\\ncache coherence enforcement, 354\\ncentralized shared-memory \\n\\nmultiprocessors, 351\\n\\nfuture GPU features, 332\\nhistorical background, L-80 to \\n\\nworkloads, 368–369, \\n371\\n\\ninterconnection network \\nconnectivity, F-63\\n\\nIOP, see I/O processor (IOP)\\nI/O processor (IOP)\\n\\nfirst dynamic scheduling, L-27\\nSony PlayStation 2 Emotion \\n\\nEngine case study, E-15\\nI/O registers, write buffer merging, 87\\nI/O subsystems\\n\\ndesign, D-59 to D-61\\ninterconnection network speed, \\n\\nF-88\\n\\nvs.', ', L-77\\nJump prediction\\n\\nhardware model, 214\\nideal processor, 214\\n\\nJumps\\n\\ncontrol flow instructions, 14, A-16, \\n\\nA-17, A-21\\n\\nGPU conditional branching, \\n301–302\\nMIPS control flow instructions, \\nA-37 to A-38\\nMIPS operations, A-35\\nreturn address predictors, 206\\nRISC instruction set, C-5\\nVAX, K-71 to K-72\\nJust-in-time (JIT), L-17\\nJVM, see Java Virtual Machine (JVM)\\n\\nK\\nKahle, Brewster, L-74\\nKahn, Robert, F-97\\nk-ary n-cubes, definition, F-38\\nKendall Square Research KSR-1, L-61\\nKernels\\n\\narithmetic intensity, 286, 286–287, \\n\\nbenchmarks, 56\\nbytes per reference, vs.', ' block size, \\n\\n327\\n\\n378\\n\\ncaches, 329\\ncommercial workload, 369–370\\ncompilers, A-24\\ncompute bandwidth, 328\\nvia computing, 327\\nEEMBC benchmarks, 38, E-12\\nFFT, I-7\\nFORTRAN, compiler \\n\\nvectorization, G-15\\n\\nFP benchmarks, C-29\\nLivermore Fortran kernels, 331\\nLU, I-8\\nmultimedia instructions, A-31\\nmultiprocessor architecture, 408\\nmultiprogramming workload, \\n\\n375–378, 377\\nperformance benchmarks, 37, 331\\nprimitives, A-30\\nprotecting processes, B-50\\nsegmented virtual memory, B-51\\nSIMD exploitation, 330\\nvector, on vector processor and \\n\\nGPU, 334–336\\n\\nvirtual memory protection, 106\\nWSCs, 438\\n\\nL\\nL1 caches, see also First-level caches\\n\\naddress translation, B-46\\nAlpha 21164 hierarchy, 368\\n\\nARM Cortex-A8, 116, 116, 235\\nARM Cortex-A8 vs.', ' A9, 236\\nARM Cortex-A8 example, 117\\ncache optimization, B-31 to B-33\\ncase study examples, B-60, B-63 to \\n\\nB-64\\n\\ndirectory-based coherence, 418\\nFermi GPU, 306\\nhardware prefetching, 91\\nhit time/power reduction, 79–80\\ninclusion, 397–398, B-34 to B-35\\nIntel Core i7, 118–119, 121–122, \\n123, 124, 124, 239, 241\\ninvalidate protocol, 355, 356–357\\nmemory consistency, 392\\nmemory hierarchy, B-39\\nmiss rates, 376–377\\nmultiprocessor cache coherence, \\n\\n352\\n\\nmultiprogramming workload, 374\\nnonblocking cache, 85\\nNVIDIA GPU Memory, 304\\nOpteron memory, B-57\\nprocessor comparison, 242\\nspeculative execution, 223\\nT1 multithreading unicore \\n\\nperformance, 228\\n\\nvirtual memory, B-48 to B-49\\nL2 caches, see also Second-level \\ncaches\\n\\nARM Cortex-A8, 114, 115–116, \\n\\n235–236\\nARM Cortex-A8 example, 117\\ncache optimization, B-31 to B-33, \\n\\nB-34\\n\\ncase study example, B-63 to B-64\\ncoherency, 352\\ncommercial workloads, 373\\ndirectory-based coherence, 379, \\n\\n418–420, 422, 424\\n\\nfault detection, 58\\nFermi GPU, 296, 306, 308\\nhardware prefetching, 91\\nIBM Blue Gene/L, I-42\\ninclusion, 397–398, B-35\\nIntel Core i7, 118, 120–122, 124, \\n\\n124–125, 239, 241\\n\\ninvalidation protocol, 355, 356–357\\nand ISA, 241\\nmemory consistency, 392\\nmemory hierarchy, B-39, B-48, \\n\\nB-57\\n\\n\\x0cL2 caches (continued )\\n\\nmultithreading, 225, 228\\nnonblocking cache, 85\\nNVIDIA GPU Memory, 304\\nprocessor comparison, 242\\nsnooping coherence, 359–361\\nspeculation, 223\\n\\nL3 caches, see also Third-level caches\\n\\nAlpha 21164 hierarchy, 368\\ncoherence, 352\\ncommercial workloads, 370, 371, \\n\\n374\\n\\ndirectory-based coherence, 379, 384\\nIBM Blue Gene/L, I-42\\nIBM Power processors, 247\\ninclusion, 398\\nIntel Core i7, 118, 121, 124, \\n\\n124–125, 239, 241, \\n403–404\\n\\ninvalidation protocol, 355, \\n\\n356–357, 360\\n\\nmemory access cycle shift, 372\\nmiss rates, 373\\nmulticore processors, 400–401\\nmultithreading, 225\\nnonblocking cache, 83\\nperformance/price/power \\n\\nconsiderations, 52\\n\\nsnooping coherence, 359, 361, 363\\n\\nLabVIEW, embedded benchmarks, \\n\\nE-13\\n\\nLampson, Butler, F-99\\nLanes\\n\\nGPUs vs.', ' adaptive routing, \\nF-52 to F-55\\n\\ndirectory coherence, 425\\ndistributed-memory \\n\\nmultiprocessors, I-30, \\nI-32\\n\\ndynamically scheduled pipelines, \\nC-70 to C-71\\n\\nFlash memory, D-3\\nFP operations, 157\\nFP pipeline, C-66\\nfunctional units, C-53\\nGPU SIMD instructions, 296\\nGPUs vs.', ' multiple thread \\n\\nexecutions, 228\\n\\nmultimedia instruction compiler \\nsupport, A-31\\nNVIDIA GPU Memory structures, \\n\\nOCNs vs.', ' datacenters, 456\\n\\n\\x0cLayer 3 network, array and Internet \\n\\nVMIPS performance, G-17 to \\n\\nLayer 3 network, WSC memory \\n\\nLinux operating systems\\n\\nG-19\\n\\nI-42 ■\\n\\nIndex\\n\\nlinkage, 445\\n\\nhierarchy, 445\\n\\nLCA, see Least common ancestor \\n(LCA)\\nLCD, see Liquid crystal display \\n(LCD)\\n\\nLearning curve, cost trends, 27\\nLeast common ancestor (LCA), \\n\\nrouting algorithms, F-48\\n\\nLeast recently used (LRU)\\n\\nAMD Opteron data cache, B-12, \\n\\nLISP\\n\\nblock replacement, B-9\\nmemory hierarchy history, L-11\\nvirtual memory block replacement, \\n\\nLisp\\n\\nB-14\\n\\nB-45\\n\\n460\\n\\nLess than condition code, PowerPC, \\n\\nLiteral addressing mode, basic \\n\\nK-10 to K-11\\n\\nLevel 3, as Content Delivery Network, \\n\\nconsiderations, A-10 to \\nA-11\\n\\nAmazon Web Services, 456–457\\narchitecture costs, 2\\nprotection and ISA, 112\\nRAID benchmarks, D-22, D-22 to \\n\\nD-23\\n\\nWSC services, 441\\n\\nLiquid crystal display (LCD), Sanyo \\n\\nVPC-SX500 digital \\ncamera, E-19\\n\\nRISC history, L-20\\nSPARC instructions, K-30\\n\\nILP, 215\\nas MapReduce inspiration, 437\\n\\nLittle Endian\\n\\nIntel 80x86, K-49\\ninterconnection networks, F-12\\nmemory address interpretation, \\n\\nA-7\\n\\nMIPS core extensions, K-20 to K-21\\nMIPS data transfers, A-34\\n\\nLivelock, network routing, F-44\\nLiveness, control dependence, 156\\nLivermore Fortran kernels, \\n\\nperformance, 331, L-6\\nLMD, see Load memory data (LMD)\\nLoad instructions\\n\\ncontrol dependences, 155\\ndata hazards requiring stalls, C-20\\ndynamic scheduling, 177\\nILP, 199, 201\\nloop-level parallelism, 318\\nmemory port conflict, C-14\\npipelined cache access, 82\\nRISC instruction set, C-4 to C-5\\nTomasulo’s algorithm, 182\\nVLIW sample code, 252\\n\\nLoad interlocks\\n\\ndefinition, C-37 to C-39\\ndetection logic, C-39\\n\\nLoad linked\\n\\nsynchronization, 388–389\\nLoad locked, synchronization, \\n\\n388–389\\nLoad memory data (LMD), simple \\n\\nMIPS implementation, \\nC-32 to C-33\\n\\nLoad stalls, MIPS R4000 pipeline, \\nC-67\\n\\nLoad-store instruction set architecture\\n\\nbasic concept, C-4 to C-5\\nIBM 360, K-87\\nIntel Core i7, 124\\nIntel 80x86 operations, K-62\\nas ISA, 11\\nISA classification, A-5\\nMIPS nonaligned data transfers, \\nK-24, K-26\\nMIPS operations, A-35 to A-36, \\n\\nA-36\\nPowerPC, K-33\\nRISC history, L-19\\nsimple MIPS implementation, C-32\\nVMIPS, 265\\nLoad/store unit\\n\\nFermi GPU, 305\\nILP hardware model, 215\\nmultiple lanes, 273\\nTomasulo’s algorithm, 171–173, \\n\\n182, 197\\n\\noperations, A-37\\n\\nLocal address space, segmented \\n\\nvirtual memory, B-52\\n\\nLocal area networks (LANs)\\n\\ncharacteristics, F-4\\ncross-company interoperability, F-64\\neffective bandwidth, F-18\\nEthernet as, F-77 to F-79\\nfault tolerance calculations, F-68\\nhistorical overview, F-99 to F-100\\nInfiniBand, F-74\\ninterconnection network domain \\n\\nrelationship, F-4\\n\\nlatency and effective bandwidth, \\nF-26 to F-28\\n\\noffload engines, F-8\\npacket latency, F-13, F-14 to F-16\\nrouters/gateways, F-79\\nshared-media networks, F-23\\nstorage area network history, \\n\\nF-102 to F-103\\n\\nLimit field, IA-32 descriptor table, \\nB-52\\nLine, memory hierarchy basics, 74\\nLinear speedup\\n\\ncost effectiveness, 407\\nIBM eServer p5 multiprocessor, \\n\\nmulticore processors, 400, 402\\nperformance, 405–406\\n\\nLine locking, embedded systems, E-4 \\n\\nto E-5\\n\\nLink injection bandwidth\\ncalculation, F-17\\ninterconnection networks, F-89\\n\\nLink pipelining, definition, F-16\\nLink reception bandwidth, calculation, \\n\\nF-17\\n\\nLink register\\n\\nMIPS control flow instructions, \\nA-37 to A-38\\n\\nPowerPC instructions, K-32 to \\n\\nprocedure invocation options, \\n\\nK-33\\n\\nA-19\\n\\nsynchronization, 389\\n\\nLinpack benchmark\\n\\ncluster history, L-63\\nparallel processing debates, L-58\\nvector processor example, \\n\\n408\\n\\nLittle’s law\\n\\ndefinition, D-24 to D-25\\nserver utilization calculation, D-29\\n\\nvector units, 265, 276–277\\n\\nLoad upper immediate (LUI), MIPS \\n\\n267–268\\n\\nlocks via coherence, 391\\n\\n\\x0cswitches, F-29\\nTCP/IP reliance, F-95\\ntime of flight, F-13\\ntopology, F-30\\n\\nLocality, see Principle of locality\\nLocal Memory\\n\\ncentralized shared-memory \\narchitectures, 351\\n\\ndefinition, 292, 314\\ndistributed shared-memory, 379\\nFermi GPU, 306\\nGrid mapping, 293\\nmultiprocessor architecture, 348\\nNVIDIA GPU Memory structures, \\n304, 304–305\\n\\nSIMD, 315\\nsymmetric shared-memory \\nmultiprocessors, \\n363–364\\n\\nLocal miss rate, definition, B-31\\nLocal node, directory-based cache \\ncoherence protocol \\nbasics, 382\\nLocal optimizations, compilers, A-26\\nLocal predictors, tournament \\n\\npredictors, 164–166\\n\\nLocal scheduling, ILP, VLIW \\n\\nprocessor, 194–195\\n\\nLocks\\n\\nvia coherence, 389–391\\nhardware primitives, 387\\nlarge-scale multiprocessor \\n\\nsynchronization, I-18 to \\nI-21\\nmultiprocessor software \\n\\ndevelopment, 409\\n\\nLock-up free cache, 83\\nLogical units, D-34\\n\\nstorage systems, D-34 to D-35\\n\\nLogical volumes, D-34\\nLong displacement addressing, VAX, \\n\\nK-67\\n\\nLong-haul networks, see Wide area \\n\\nnetworks (WANs)\\n\\nCUDA, 290\\ndefinition, 315–316\\ndependence distance, H-6\\ndependent computation \\n\\nelimination, 321\\n\\nexample calculations, H-4 to H-5\\nGCD, 319\\nloop-level parallelism, H-3\\nas recurrence, 318\\nrecurrence form, H-5\\nVMIPS, 268\\n\\nLoop exit predictor, Intel Core i7, 166\\nLoop interchange, compiler \\n\\noptimizations, 88–89\\n\\nLoop-level parallelism\\ndefinition, 149–150\\ndetection and enhancement\\n\\nbasic approach, 315–318\\ndependence analysis, H-6 to \\n\\nH-10\\n\\ndependence computation \\n\\nelimination, 321–322\\n\\ndependences, locating, \\n318–321\\ndependent computation \\n\\nelimination, H-10 to \\nH-12\\noverview, H-2 to H-6\\n\\nhistory, L-30 to L-31\\nILP in perfect processor, 215\\nILP for realizable processors, \\n217–218\\nLoop stream detection, Intel Core i7 \\n\\nmicro-op buffer, 238\\n\\nLoop unrolling\\n\\nbasic considerations, 161–162\\nILP exposure, 157–161\\nILP limitation studies, 220\\nrecurrences, H-12\\nsoftware pipelining, H-12 to H-15, \\nH-13, H-15\\n\\nTomasulo’s algorithm, 179, \\n181–183\\n\\nVLIW processors, 195\\n\\nLong Instruction Word (LIW)\\n\\nLossless networks\\n\\nEPIC, L-32\\nmultiple-issue processors, L-28, \\n\\nL-30\\n\\nLong integer\\n\\noperand sizes/types, 12\\nSPEC benchmarks, A-14\\n\\nLoop-carried dependences\\n\\ndefinition, F-11 to F-12\\nswitch buffer organizations, F-59\\nLossy networks, definition, F-11 to \\n\\nF-12\\n\\nLRU, see Least recently used (LRU)\\nLucas\\n\\ncompiler optimizations, A-29\\n\\nIndex\\n\\n■\\n\\nI-43\\n\\ndata cache misses, B-10\\n\\nLUI, see Load upper immediate (LUI)\\nLU kernel\\n\\ncharacteristics, I-8\\ndistributed-memory \\n\\nmultiprocessor, I-32\\n\\nsymmetric shared-memory \\n\\nmultiprocessors, I-22, \\nI-23, I-25\\n\\nM\\nMAC, see Multiply-accumulate \\n\\n(MAC)\\n\\nMachine language programmer, L-17 \\nto L-18\\nMachine memory, Virtual Machines, \\n\\n110\\nMacro-op fusion, Intel Core i7, \\n\\n237–238\\n\\nMagnetic storage\\n\\naccess time, D-3\\ncost vs.', ' access time, D-3\\nhistorical background, L-77 to \\n\\nL-79\\nMail servers, benchmarking, D-20\\nMain Memory\\n\\naddressing modes, A-10\\naddress translation, B-46\\narithmetic intensity example, 286, \\n\\n286–288\\n\\nblock placement, B-44\\ncache function, B-2\\ncache optimization, B-30, B-36\\ncoherence protocol, 362\\ndefinition, 292, 309\\nDRAM, 17\\ngather-scatter, 329\\nGPU vs.', ' MIMD, 327\\nGPUs and coprocessors, 330\\nGPU threads, 332\\nILP considerations, 245\\ninterlane wiring, 273\\nlinear speedups, 407\\nmemory hierarchy basics, 76\\nmemory hierarchy design, 72\\nmemory mapping, B-42\\nMIPS operations, A-36\\nMultimedia SIMD vs.', ' GPUs, 312\\nmultiprocessor cache coherence, \\n\\n352\\n\\npaging vs.', ' operation cost, 33\\n\\nMapReduce\\n\\ncloud computing, 455\\ncost calculations, 458–460, 459\\nGoogle usage, 437\\nreductions, 321\\nWSC batch processing, 437–438\\nWSC cost-performance, 474\\n\\nMark-I, L-3 to L-4, L-6\\nMark-II, L-4\\nMark-III, L-4\\nMark-IV, L-4\\nMask Registers\\n\\nbasic operation, 275–276\\ndefinition, 309\\nMultimedia SIMD, 283\\nNVIDIA GPU computational \\n\\nstructures, 291\\n\\nvector compilers, 303\\nvector vs.', ' GPU, 311\\nVMIPS, 267\\n\\nMasPar, L-44\\nMassively parallel processors (MPPs)\\n\\ncharacteristics, I-45\\ncluster history, L-62, L-72 to \\n\\nsystem area network history, F-100 \\n\\nL-73\\n\\nto F-101\\n\\nMatrix300 kernel\\ndefinition, 56\\nprediction buffer, C-29\\n\\nMatrix multiplication\\nbenchmarks, 56\\nLU kernel, I-8\\nmultidimensional arrays in vector \\n\\nvs.', ' GPU, 311\\nVLRs, 274–275\\n\\nM-bus, see Memory bus (M-bus)\\nMcCreight, Ed, F-99\\nMCF\\n\\ncompiler optimizations, A-29\\ndata cache misses, B-10\\nIntel Core i7, 240–241\\nMCP operating system, L-16\\nMean time between failures (MTBF)\\n\\nfallacies, 56–57\\nRAID, L-79\\nSLA states, 34\\n\\nMean time to failure (MTTF)\\ncomputer system power \\n\\nconsumption case study, \\n63–64\\n\\ndependability benchmarks, D-21\\ndisk arrays, D-6\\nexample calculations, 34–35\\nI/O subsystem design, D-59 to \\n\\nD-61\\n\\nRAID reconstruction, D-55 to \\n\\nD-57\\nSLA states, 34\\nTB-80 cluster, D-40 to D-41\\nWSCs vs.', ' PMD, 72\\nsystem call virtualization/\\n\\nparavirtualization \\nperformance, 141\\n\\nvirtual machine monitor, 108–109\\nVirtual Machines ISA support, \\n109–110\\nVirtual Machines protection, \\n107–108\\nVirtual Machines and virtual \\nmemory and I/O, \\n110–111\\nvirtual memory protection, \\n105–107\\nVMM on nonvirtualizable ISA, \\n128–129\\n\\nXen VM example, 111\\n\\nMemory Interface Unit\\n\\nNVIDIA GPU ISA, 300\\nvector processor example, 310\\n\\nMemoryless, definition, D-28\\nMemory mapping\\n\\nmemory hierarchy, B-48 to B-49\\nsegmented virtual memory, B-52\\nTLBs, 323\\nvirtual memory definition, B-42\\n\\nMemory-memory instruction set \\n\\narchitecture, ISA \\nclassification, A-3, A-5\\n\\nMemory protection\\n\\ncontrol dependence, 155\\nPentium vs.', ' x86, 298\\nswitches, see Switch \\n\\nmicroarchitecture\\n\\nperformance equations, B-22\\n\\nMesh interface unit (MIU), Intel \\n\\ntechniques case study, 247–254\\n\\nMemory system\\n\\nSCCC, F-70\\n\\nMicrobenchmarks\\n\\ncache optimization, B-36\\ncoherency, 352–353\\ncommercial workloads, 367, \\n369–371\\n\\ncomputer architecture, 15\\nC program evaluation, 134–135\\ndependability enhancement, \\n104–105\\n\\ndistributed shared-memory, 379, 418\\ngather-scatter, 280\\nGDRAMs, 323\\nGPUs, 332\\nILP, 245\\n\\nhardware vs.', ' software \\n\\nspeculation, 221–222\\n\\nspeculative execution, 222–223\\n\\nIntel Core i7, 237, 242\\nlatency, B-21\\nMIPS, C-33\\nmultiprocessor architecture, 347\\nmultiprocessor cache coherence, \\n\\n352\\n\\nmultiprogramming workload, \\n377–378\\n\\npage size changes, B-58\\nprice/performance/power \\n\\nRISC, C-7\\nRoofline model, 286\\nshared-memory multiprocessors, \\n\\n363\\nSMT, 399–400\\nstride handling, 279\\nT1 multithreading unicore \\n\\nperformance, 227\\n\\nvector architectures, G-9 to G-11\\nvector chaining, G-11\\nvector processors, 271, 277\\nvirtual, B-43, B-46\\n\\nMesh network\\n\\ndisk array deconstruction, D-51 to \\n\\ncharacteristics, F-73\\ndeadlock, F-47\\ndimension-order routing, F-47 to \\n\\nD-55\\n\\ndisk deconstruction, D-48 to D-51\\nMicrofusion, Intel Core i7 micro-op \\n\\nF-48\\n\\nOCN history, F-104\\nrouting example, F-46\\n\\nMesh topology\\n\\ncharacteristics, F-36\\ndirect networks, F-37\\nNEWS communication, F-42 to \\n\\nF-43\\nMESI, see Modified-Exclusive-\\n\\nShared-Invalid (MESI) \\nprotocol\\nMessage ID, packet header, F-8, F-16\\nMessage-passing communication\\n\\nhistorical background, L-60 to \\n\\nL-61\\n\\nlarge-scale multiprocessors, I-5 to \\n\\nI-6\\nMessage Passing Interface (MPI)\\n\\nfunction, F-8\\nInfiniBand, F-77\\nlack in shared-memory \\n\\nadaptive routing, F-93 to F-94\\ncoherence maintenance, 381\\nInfiniBand, F-76\\ninterconnection networks, F-6 to \\n\\nF-9\\nzero-copy protocols, F-91\\n\\nMFLOPS, see Millions of \\n\\nfloating-point \\noperations per second \\n(MFLOPS)\\n\\nMicroarchitecture\\n\\nas architecture component, 15–16\\n\\nbuffer, 238\\n\\nMicroinstructions\\n\\ncomplications, C-50 to C-51\\nx86, 298\\n\\nMicro-ops\\n\\nIntel Core i7, 237, 238–240, 239\\nprocessor clock rates, 244\\n\\nMicroprocessor overview\\nclock rate trends, 24\\ncost trends, 27–28\\ndesktop computers, 6\\nembedded computers, 8–9\\nenergy and power, 23–26\\ninside disks, D-4\\nintegrated circuit improvements, 2\\nand Moore’s law, 3–4\\nperformance trends, 19–20, 20\\npower and energy system trends, \\n\\n21–23\\n\\nrecent advances, L-33 to L-34\\ntechnology trends, 18\\n\\nPipeline Stages, see \\nMIPS (Microprocessor \\nwithout Interlocked \\nPipeline Stages)\\n\\nMicrosoft\\n\\ncloud computing, 455\\ncontainers, L-74\\nIntel support, 245\\nWSCs, 464–465\\n\\nMicrosoft Azure, 456, L-74\\nMicrosoft DirectX, L-51 to L-52\\nMicrosoft Windows\\nbenchmarks, 38\\n\\nconsiderations, 53\\n\\nMessages\\n\\nmultiprocessors, I-5\\n\\nMicroprocessor without Interlocked \\n\\n\\x0cmultithreading, 223\\nRAID benchmarks, D-22, D-22 to \\n\\nD-23\\n\\ntime/volume/commoditization \\nimpact, 28\\nWSC workloads, 441\\n\\nMicrosoft Windows 2008 Server\\n\\nreal-world considerations, 52–55\\nSPECpower benchmark, 463\\n\\nMicrosoft XBox, L-51\\nMigration, cache coherent \\n\\nmultiprocessors, 354\\n\\nMillions of floating-point operations \\nper second (MFLOPS)\\nearly performance measures, L-7\\nparallel processing debates, L-57 \\n\\nto L-58\\nSIMD computer history, L-55\\nSIMD supercomputer \\n\\ndevelopment, L-43\\nvector performance measures, \\n\\nG-15 to G-16\\n\\nMIMD (Multiple Instruction Streams, \\nMultiple Data Streams)\\n\\nand Amdahl’s law, 406–407\\ndefinition, 10\\nearly computers, L-56\\nfirst vector computers, L-46, L-48\\nGPU programming, 289\\nGPUs vs.', ' GPU, \\n\\n324–330\\nmultiprocessor architecture, \\n346–348\\nspeedup via parallelism, 263\\nTLP, basic considerations, 344–345\\n\\nMinicomputers, replacement by \\n\\nmicroprocessors, 3–4\\n\\nMinniespec benchmarks\\n\\nARM Cortex-A8, 116, 235\\nARM Cortex-A8 memory, \\n\\n115–116\\nMINs, see Multistage interconnection \\n\\nnetworks (MINs)\\nMIPS (Microprocessor without \\n\\nInterlocked Pipeline \\nStages)\\n\\naddressing modes, 11–12\\nbasic pipeline, C-34 to C-36\\nbranch predictor correlation, 163\\ncache performance, B-6\\nconditional branches, K-11\\n\\nconditional instructions, H-27\\ncontrol flow instructions, 14\\ndata dependences, 151\\ndata hazards, 169\\ndynamic scheduling with \\n\\nTomasulo’s algorithm, \\n171, 173\\nearly pipelined CPUs, L-26\\nembedded systems, E-15\\nencoding, 14\\nexceptions, C-48, C-48 to C-49\\nexception stopping/restarting, C-46 \\n\\nto C-47\\n\\nfeatures, K-44\\nFP pipeline performance, C-60 to \\nC-61, C-62\\n\\nFP unit with Tomasulo’s \\nalgorithm, 173\\n\\nhazard checks, C-71\\nILP, 149\\nILP exposure, 157–158\\nILP hardware model, 215\\ninstruction execution issues, K-81\\ninstruction formats, core \\n\\ninstructions, K-6\\n\\ninstruction set complications, C-49 \\n\\nto C-51\\n\\nISA class, 11\\nISA example\\n\\naddressing modes for data \\ntransfer, A-34\\n\\narithmetic/logical instructions, \\n\\nbasic considerations, A-32 to \\n\\nA-37\\n\\nA-33\\n\\nto A-38, A-38\\n\\ndata types, A-34\\ndynamic instruction mix, A-41, \\nA-41 to A-42, A-42\\n\\nFP operations, A-38 to A-39\\ninstruction format, A-35\\nload-store instructions, A-36\\nMIPS operations, A-35 to A-37\\nregisters, A-34\\nusage, A-39\\n\\nLivermore Fortran kernel \\n\\nperformance, 331\\n\\nmemory addressing, 11\\nmulticycle operations\\n\\nbasic considerations, C-51 to \\n\\nC-54\\n\\nIndex\\n\\n■\\n\\nI-47\\n\\nhazards and forwarding, C-54 \\n\\nto C-58\\n\\nprecise exceptions, C-58 to \\n\\nC-60\\nmultimedia support, K-19\\nmultiple-issue processor history, \\n\\noperands, 12\\nperformance measurement history, \\n\\nL-6 to L-7\\npipeline branch issues, C-39 to \\n\\nL-29\\n\\nC-42\\n\\npipeline control, C-36 to C-39\\npipe stage, C-37\\nprocessor performance \\n\\ncalculations, 218–219\\n\\nregisters and usage conventions, 12\\nRISC code size, A-23\\nRISC history, L-19\\nRISC instruction set lineage, K-43\\nas RISC systems, K-4\\nscoreboard components, C-76\\nscoreboarding, C-72\\nscoreboarding steps, C-73, C-73 to \\n\\nsimple implementation, C-31 to \\nC-34, C-34\\n\\nSony PlayStation 2 Emotion \\n\\nEngine, E-17\\nunaligned word read instructions, \\n\\nunpipelined functional units, C-52\\nvs.', ' virtual addressed cache size, \\n\\nB-37\\n\\nMIPS R4000\\n\\ncache optimization, 79, B-35 to \\n\\n370–373\\n\\n\\x0cMoore’s law\\n\\nhistorical background, L-64 to \\n\\nMIT Raw, characteristics, F-73\\nMitsubishi M32R\\n\\naddressing modes, K-6\\narithmetic/logical instructions, \\n\\nK-24\\n\\ncharacteristics, K-4\\ncondition codes, K-14\\nconstant extension, K-9\\ndata transfer instructions, K-23\\nembedded instruction format, K-8\\nmultiply-accumulate, K-20\\nunique instructions, K-39 to K-40\\nMIU, see Mesh interface unit (MIU)\\nMixed cache\\n\\nAMD Opteron example, B-15\\ncommercial workload, 373\\n\\nMixer, radio receiver, E-23\\nMiya, Eugene, L-65\\nM/M/1 model\\n\\nexample, D-32, D-32 to D-33\\noverview, D-30\\nRAID performance prediction, D-57\\nsample calculations, D-33\\nM/M/2 model, RAID performance \\n\\nprediction, D-57\\n\\nMMX, see Multimedia Extensions \\n(MMX)\\n\\nMobile clients\\n\\ndata usage, 3\\nGPU features, 324\\nvs.', ' server GPUs, 323–330\\n\\n(MESI) protocol, \\ncharacteristics, 362\\n\\nModified-Owned-Exclusive-Shared-In\\nvalid (MOESI) protocol, \\ncharacteristics, 362\\n\\ncoherence protocol, 362\\ndirectory-based cache coherence \\n\\nprotocol basics, 380\\n\\nlarge-scale multiprocessor cache \\n\\ncoherence, I-35\\n\\nsnooping coherence protocol, \\n358–359\\nModula-3, integer division/remainder, \\n\\nJ-12\\nModule availability, definition, 34\\nModule reliability, definition, 34\\nMOESI, see Modified-Owned-\\n\\nDRAM, 100\\nflawed architectures, A-45\\ninterconnection networks, F-70\\nand microprocessor dominance, \\n\\npoint-to-point links and switches, \\n\\n3–4\\n\\nD-34\\n\\nRISC, A-3\\nRISC history, L-22\\nsoftware importance, 55\\nswitch size, F-29\\ntechnology trends, 17\\n\\nMortar shot graphs, multiprocessor \\n\\nMotion JPEG encoder, Sanyo \\n\\nperformance \\nmeasurement, 405–406\\n\\nVPC-SX500 digital \\ncamera, E-19\\n\\nMotorola 68000\\n\\ncharacteristics, K-42\\nmemory protection, L-10\\nMotorola 68882, floating-point \\n\\nprecisions, J-33\\n\\nMove address, VAX, K-70\\nMPEG\\n\\nMultimedia SIMD Extensions \\n\\nhistory, L-49\\n\\nmultimedia support, K-17\\nSanyo VPC-SX500 digital camera, \\n\\nE-19\\n\\nEngine, E-17\\n\\nMPI, see Message Passing Interface \\n(MPI)\\n\\nMPPs, see Massively parallel \\n\\n(MSP)\\n\\nMTBF, see Mean time between \\n\\nfailures (MTBF)\\n\\nMTDL, see Mean time until data loss \\n(MTDL)\\nMTTF, see Mean time to failure (MTTF)\\nMTTR, see Mean time to repair (MTTR)\\nMultibanked caches\\n\\ncache optimization, 85–86\\nexample, 86\\n\\nMultichip modules, OCNs, F-3\\nMulticomputers\\n\\nModified-Exclusive-Shared-Invalid \\n\\nSony PlayStation 2 Emotion \\n\\nExclusive-Shared-Invali\\nd (MOESI) protocol\\n\\ncluster history, L-63\\ndefinition, 345, L-59\\n\\nIndex\\n\\n■\\n\\nI-49\\n\\nL-65\\n\\nMulticore processors\\n\\narchitecture goals/requirements, 15\\ncache coherence, 361–362\\ncentralized shared-memory \\n\\nmultiprocessor \\nstructure, 347\\n\\nCray X1E, G-24\\ndirectory-based cache coherence, \\n\\n380\\n\\n419\\n\\ndirectory-based coherence, 381, \\n\\nDSM architecture, 348, 379\\nmultichip\\n\\ncache and memory states, 419\\nwith DSM, 419\\nmultiprocessors, 345\\nOCN history, F-104\\nperformance, 400–401, 401\\nperformance gains, 398–400\\nperformance milestones, 20\\nsingle-chip case study, 412–418\\nand SMT, 404–405\\nsnooping cache coherence \\n\\nimplementation, 365\\n\\nSPEC benchmarks, 402\\nuniform memory access, 364\\nwrite invalidate protocol \\n\\nimplementation, \\n356–357\\n\\nMultics protection software, L-9\\nMulticycle operations, MIPS pipeline\\nbasic considerations, C-51 to C-54\\nhazards and forwarding, C-54 to \\n\\nC-58\\n\\nMultidimensional arrays\\ndependences, 318\\nin vector architectures, 278–279\\n\\nMultiflow processor, L-30, L-32\\nMultigrid methods, Ocean application, \\nI-9 to I-10\\n\\nMultilevel caches\\n\\ncache optimizations, B-22\\ncentralized shared-memory \\narchitectures, 351\\nmemory hierarchy basics, 76\\nmemory hierarchy history, L-11\\nmiss penalty reduction, B-30 to \\n\\nB-35\\n\\nmiss rate vs.', ' GPU, 312\\nperformance equations, B-22\\npurpose, 397\\nwrite process, B-11\\n\\nMultilevel exclusion, definition, B-35\\nMultilevel inclusion\\n\\ndefinition, 397, B-34\\nimplementation, 397\\nmemory hierarchy history, L-11\\n\\nMultimedia applications\\n\\ndesktop processor support, \\nE-11\\n\\nGPUs, 288\\nISA support, A-46\\nMIPS FP operations, A-39\\nvector architectures, 267\\nMultimedia Extensions (MMX)\\ncompiler support, A-31\\ndesktop RISCs, K-18\\ndesktop/server RISCs, K-16 to \\n\\nK-19\\nSIMD history, 262, L-50\\nvs.', ' GPUs, 312\\nhistorical background, L-49 to \\n\\nDSP, E-5\\nembedded RISCs, K-20\\nTI TMS320C55 DSP, E-8\\n\\nMultiply operations\\n\\nchip comparison, J-61\\nfloating point\\n\\nmodel, 285–288, 287\\n\\ninteger arithmetic\\n\\nL-50\\nMIMD, vs.', ' GPU, 324–330\\nparallelism classes, 10\\nprogramming, 285\\nRoofline visual performance \\n\\n256-bit-wide operations, 282\\nvs.', ' direct network costs, F-92\\nexample, F-31\\nself-routing, F-48\\nsystem area network history, F-100 \\n\\nto F-101\\ntopology, F-30 to F-31, F-38 to \\n\\nF-39\\n\\nMultistage switch fabrics, topology, \\n\\nF-30\\nMulti-Streaming Processor (MSP)\\nCray X1, G-21 to G-23, G-22, \\n\\nG-23 to G-24\\n\\nCray X1E, G-24\\nfirst vector computers, L-46\\nMultithreaded SIMD Processor\\n\\nblock diagram, 294\\ndefinition, 292, 309, 313–314\\nFermi GPU architectural \\n\\ninnovations, 305–308\\nFermi GPU block diagram, 307\\nFermi GTX 480 GPU floorplan, \\n295, 295–296\\n\\nGPU programming, 289–290\\nGPUs vs.', ' vector architectures, 310, \\n\\n310–311\\nGrid mapping, 293\\nNVIDIA GPU computational \\n\\nstructures, 291\\nNVIDIA GPU Memory structures, \\n304, 304–305\\n\\nRoofline model, 326\\n\\nMultithreaded vector processor\\n\\ndefinition, 292\\nFermi GPU comparison, 305\\n\\nMultithreading\\n\\ncoarse-grained, 224–226\\ndefinition and types, 223–225\\nfine-grained, 224–226\\nGPU programming, 289\\nhistorical background, L-34 to \\n\\nL-35\\n\\nILP, 223–232\\nmemory hierarchy basics, 75–76\\nparallel benchmarks, 231, 231–232\\nfor performance gains, 398–400\\nSMT, see Simultaneous \\n\\nmultithreading (SMT)\\n\\nSun T1 effectiveness, 226–229\\n\\nMVAPICH, F-77\\nMVL, see Maximum vector length \\n(MVL)\\nMXP processor, components, E-14\\nMyrinet SAN, F-67\\n\\ncharacteristics, F-76\\ncluster history, L-62 to L-63, L-73\\nrouting algorithms, F-48\\nswitch vs.', ' NIC, F-86\\n\\nNetwork technology, see also \\nInterconnection \\nnetworks\\n\\nGoogle WSC, 469\\nperformance trends, 19–20\\npersonal computers, F-2\\ntrends, 18\\nWSC bottleneck, 461\\nWSC goals/requirements, 433\\nNetwork of Workstations, L-62, L-73\\nNEWS communication, see \\n\\nNewton’s iteration, J-27 to J-30\\nNFS, see Network File System (NFS)\\nNIC, see Network interface card (NIC)\\nNicely, Thomas, J-64\\nNMOS, DRAM, 99\\nNoC, see Network on chip (NoC)\\nNodes\\n\\ncoherence maintenance, 381\\ncommunication bandwidth, I-3\\ndirect network topology, F-37\\ndirectory-based cache coherence, \\n\\n380\\n\\ndistributed switched networks, \\n\\nF-34 to F-36\\n\\nIBM Blue Gene/L, I-42 to I-44\\nIBM Blue Gene/L 3D torus \\n\\nvector processor, 310, 310–311, \\n\\nG-25\\n\\nNorth-East-West-South \\n\\ncommunication, \\nnetwork topology \\ncalculations, F-41 to \\nF-43\\n\\nNorth-last routing, F-48\\nNot a Number (NaN), J-14, J-16, J-21, \\n\\nJ-34\\nNotifications, interconnection \\n\\nnetworks, F-10\\n\\nNOW project, L-73\\nNo-write allocate\\n\\ndefinition, B-11\\nexample calculation, B-12\\n\\nnetwork, F-73\\nnetwork topology performance and \\n\\nNSFNET, F-98\\nNTSC/PAL encoder, Sanyo \\n\\ncosts, F-40\\n\\nin parallel, 336\\npoints-to analysis, H-9\\n\\nNokia cell phone, circuit board, E-24\\nNonaligned data transfers, MIPS64, \\n\\nK-24 to K-26\\n\\nNonatomic operations\\n\\ncache coherence, 361\\ndirectory protocol, 386\\nNonbinding prefetch, cache \\n\\noptimization, 93\\n\\nNonblocking caches\\n\\ncache optimization, 83–85, \\n131–133\\n\\neffectiveness, 84\\nILP speculative execution, \\n\\n222–223\\n\\nIntel Core i7, 118\\nmemory hierarchy history, L-11\\nNonblocking crossbar, centralized \\n\\nswitched networks, F-32 \\nto F-33\\n\\nNonfaulting prefetches, cache \\noptimization, 92\\nNonrestoring division, J-5, J-6\\nNonuniform memory access \\n\\n(NUMA)\\n\\nDSM as, 348\\nlarge-scale multiprocessor history, \\n\\nL-61\\n\\nsnooping limitations, 363–364\\n\\nNon-unit strides\\n\\nVPC-SX500 digital \\ncamera, E-19\\n\\nNullification, PA-RISC instructions, \\n\\nNullifying branch, branch delay slots, \\n\\nK-33 to K-34\\n\\nC-24 to C-25\\n\\nNUMA, see Nonuniform memory \\n\\naccess (NUMA)\\n\\nNVIDIA GeForce, L-51\\nNVIDIA systems\\n\\nfine-grained multithreading, 224\\nGPU comparisons, 323–330, \\n\\n325\\n\\nGPU computational structures, \\n291–297\\nGPU computing history, L-52\\nGPU ISA, 298–300\\nGPU Memory structures, 304, \\n304–305\\n\\nGPU programming, 289\\ngraphics pipeline history, L-51\\nscalable GPUs, L-51\\nterminology, 313–315\\n\\nN-way set associative\\n\\nblock placement, B-7\\nconflict misses, B-23\\nmemory hierarchy basics, 74\\nTLBs, B-49\\n\\nNYU Ultracomputer, L-60\\n\\nO\\nObserved performance, fallacies, 57\\nOccupancy, communication \\nbandwidth, I-3\\n\\nNorth-East-West-South \\ncommunication\\n\\nmultidimensional arrays in vector \\narchitectures, 278–279\\n\\n\\x0cOcean application\\n\\ncharacteristics, I-9 to I-10\\ndistributed-memory \\n\\nmultiprocessor, I-32\\n\\noptimization, 79\\nSRAM, 98–99\\n\\nOn-chip memory, embedded systems, \\nE-4 to E-5\\n\\ndistributed-memory \\n\\nOn-chip networks (OCNs)\\n\\nmultiprocessors, I-30\\n\\nexample calculations, I-11 to I-12\\nmiss rates, I-28\\nsymmetric shared-memory \\n\\nbasic considerations, F-3\\ncommercial implementations, F-73\\ncommercial interconnection \\n\\nnetworks, F-63\\n\\nOperands\\n\\nmultiprocessors, I-23\\n\\ncross-company interoperability, \\n\\nOCNs, see On-chip networks (OCNs)\\nOffline reconstruction, RAID, D-55\\nOffload engines\\n\\nnetwork interfaces, F-8\\nTCP/IP reliance, F-95\\n\\nOffset\\n\\nF-64\\n\\nDOR, F-46\\neffective bandwidth, F-18, \\nF-28\\n\\nexample system, F-70 to F-72\\nhistorical overview, F-103 to \\n\\ninterconnection network domain \\n\\nrelationship, F-4\\n\\ninterconnection network speed, \\n\\nto C-53\\n\\nF-104\\n\\nF-88\\n\\naddressing modes, 12\\nAMD64 paged virtual memory, \\n\\nB-55\\n\\nblock identification, B-7 to B-8\\ncache optimization, B-38\\ncall gates, B-54\\ncontrol flow instructions, A-18\\ndirectory-based cache coherence \\n\\nprotocols, 381–382\\n\\nexample, B-9\\ngather-scatter, 280\\nIA-32 segment, B-53\\ninstruction decode, C-5 to C-6\\nmain memory, B-44\\nmemory mapping, B-52\\nMIPS, C-32\\nMIPS control flow instructions, \\nA-37 to A-38\\n\\nmisaligned addresses, A-8\\nOpteron data cache, B-13 to B-14\\npipelining, C-42\\nPTX instructions, 300\\nRISC, C-4 to C-6\\nRISC instruction set, C-4\\nTLB, B-46\\nTomasulo’s approach, 176\\nvirtual memory, B-43 to B-44, \\nB-49, B-55 to B-56\\nOLTP, see On-Line Transaction \\nProcessing (OLTP)\\n\\nOmega\\n\\nexample, F-31\\npacket blocking, F-32\\ntopology, F-30\\n\\nOMNETPP, Intel Core i7, 240–241\\nOn-chip cache\\n\\nlatency and effective bandwidth, \\nF-26 to F-28\\nlatency vs.', ' nodes, F-27\\nlink bandwidth, F-89\\npacket latency, F-13, F-14 to F-16\\nswitch microarchitecture, F-57\\ntime of flight, F-13\\ntopology, F-30\\nwormhole switching, F-51\\n\\nOne’s complement, J-7\\nOne-way conflict misses, definition, \\n\\nB-23\\nOnline reconstruction, RAID, D-55\\nOn-Line Transaction Processing \\n\\n(OLTP)\\n\\ncommercial workload, 369, 371\\nserver benchmarks, 41\\nshared-memory workloads, \\n\\n368–370, 373–374\\n\\nstorage system benchmarks, D-18\\n\\nOpenCL\\n\\nGPU programming, 289\\nGPU terminology, 292, 313–315\\nNVIDIA terminology, 291\\nprocessor comparisons, 323\\n\\nOpenGL, L-51\\nOpen source software\\n\\nAmazon Web Services, 457\\nWSCs, 437\\nXen VMM, see Xen virtual \\nmachine\\n\\nIndex\\n\\n■\\n\\nI-53\\n\\nOpen Systems Interconnect (OSI)\\n\\nEthernet, F-78 to F-79\\nlayer definitions, F-82\\n\\nOperand addressing mode, Intel \\n\\n80x86, K-59, K-59 to \\nK-60\\nOperand delivery stage, Itanium 2, \\nH-42\\n\\nDSP, E-6\\nforwarding, C-19\\ninstruction set encoding, A-21 to \\n\\nA-22\\n\\nIntel 80x86, K-59\\nISA, 12\\nISA classification, A-3 to A-4\\nMIPS data types, A-34\\nMIPS pipeline, C-71\\nMIPS pipeline FP operations, C-52 \\n\\nNVIDIA GPU ISA, 298\\nper ALU instruction example, A-6\\nTMS320C55 DSP, E-6\\ntype and size, A-13 to A-14\\nVAX, K-66 to K-68, K-68\\nvector execution time, 268–269\\n\\nOperating systems (general)\\naddress translation, B-38\\nand architecture development, 2\\ncommunication performance, F-8\\ndisk access scheduling, D-44 to \\nD-45, D-45\\nmemory protection performance, \\n\\nB-58\\n\\nmiss statistics, B-59\\nmultiprocessor software \\n\\ndevelopment, 408\\n\\nand page size, B-58\\nsegmented virtual memory, B-54\\nserver benchmarks, 40\\nshared-memory workloads, \\n374–378\\n\\nstorage systems, D-35\\n\\nOperational costs\\n\\nbasic considerations, 33\\nWSCs, 434, 438, 452, 456, 472\\nOperational expenditures (OPEX)\\nWSC costs, 452–455, 454\\nWSC TCO case study, 476–478\\nOperation faults, storage systems, D-11\\nOperator dependability, disks, D-13 to \\n\\nD-15\\n\\n\\x0cI-54 ■\\n\\nIndex\\n\\nOPEX, see Operational expenditures \\n(OPEX)\\n\\nOptical media, interconnection \\n\\nnetworks, F-9\\n\\nOracle database\\n\\ncommercial workload, 368\\nmiss statistics, B-59\\nmultithreading benchmarks, 232\\nsingle-threaded benchmarks, 243\\nWSC services, 441\\n\\nOrdering, and deadlock, F-47\\nOrganization\\n\\nbuffer, switch microarchitecture, \\nF-58 to F-60\\n\\ncache, performance impact, \\nB-19\\ncache blocks, B-7 to B-8\\ncache optimization, B-19\\ncoherence extensions, 362\\ncomputer architecture, 11, 15–16\\nDRAM, 98\\nMIPS pipeline, C-37\\nmultiple-issue processor, 197, 198\\nOpteron data cache, B-12 to B-13, \\n\\npipelines, 152\\nprocessor history, 2–3\\nprocessor performance equation, \\n\\nshared-memory multiprocessors, \\n\\nB-13\\n\\n49\\n\\n346\\n\\nE-18\\n\\nTLB, B-46\\n\\nOrthogonality, compiler \\n\\nwriting-architecture \\nrelationship, A-30\\n\\nOSI, see Open Systems Interconnect \\n\\n(OSI)\\n\\nOut-of-order completion\\ndata hazards, 169\\nMIPS pipeline, C-71\\nMIPS R100000 sequential \\nconsistency, 397\\n\\nprecise exceptions, C-58\\n\\nOut-of-order execution\\n\\nand cache miss, B-2 to B-3\\ncache performance, B-21\\ndata hazards, 169–170\\nhardware-based execution, 184\\nILP, 245\\nmemory hierarchy, B-2 to B-3\\n\\nmicroarchitectural techniques case \\nstudy, 247–254\\n\\nMIPS pipeline, C-71\\nmiss penalty, B-20 to B-22\\nperformance milestones, 20\\npower/DLP issues, 322\\nprocessor comparisons, 323\\nR10000, 397\\nSMT, 246\\nTomasulo’s algorithm, 183\\n\\nOut-of-order processors\\n\\nDLP, 322\\nIntel Core i7, 236\\nmemory hierarchy history, L-11\\nmultithreading, 226\\nvector architecture, 267\\nOut-of-order write, dynamic \\nscheduling, 171\\n\\nOutput buffered switch\\nHOL blocking, F-60\\nmicroarchitecture, F-57, F-57\\norganizations, F-58 to F-59\\npipelined version, F-61\\n\\nOutput dependence\\n\\ncompiler history, L-30 to L-31\\ndefinition, 152–153\\ndynamic scheduling, 169–171, C-72\\nfinding, H-7 to H-8\\nloop-level parallelism calculations, \\n\\n320\\nMIPS scoreboarding, C-79\\n\\nmicroprocessors, 26\\nprocessor performance equation, \\n\\n52\\n\\nOverflow, integer arithmetic, J-8, J-10 \\nto J-11, J-11\\n\\nOverflow condition code, MIPS core, \\nK-9 to K-16\\n\\nOverhead\\n\\nadaptive routing, F-93 to F-94\\nAmdahl’s law, F-91\\ncommunication latency, I-4\\ninterconnection networks, F-88, \\nF-91 to F-92\\nOCNs vs.', ' servers, 433–434\\n\\nParallel processors\\n\\nareas of debate, L-56 to L-58\\nbus-based coherent multiprocessor \\n\\nhistory, L-59 to L-60\\n\\ncluster history, L-62 to L-64\\nearly computers, L-56\\n\\nsynchronization and consistency \\nmodels, L-64\\n\\nvirtual memory history, L-64\\n\\nParallel programming\\n\\ncomputation communication, I-10 \\n\\nto I-12\\n\\nwith large-scale multiprocessors, I-2\\n\\nParallel Thread Execution (PTX)\\n\\nbasic GPU thread instructions, 299\\nGPU conditional branching, \\n300–303\\n\\nGPUs vs.', ' vector architectures, 308\\nNVIDIA GPU ISA, 298–300\\nNVIDIA GPU Memory structures, \\n\\n305\\nParallel Thread Execution (PTX) \\n\\nInstruction\\n\\nCUDA Thread, 300\\ndefinition, 292, 309, 313\\nGPU conditional branching, 302–303\\nGPU terms, 308\\nNVIDIA GPU ISA, 298, 300\\n\\nsystem call performance, 141\\nXen VM, 111\\n\\nParity\\n\\ndirty bits, D-61 to D-64\\nfault detection, 58\\nmemory dependability, 104–105\\nWSC memory, 473–474\\n\\nPARSEC benchmarks\\n\\nIntel Core i7, 401–405\\nSMT on superscalar processors, \\n230–232, 231\\n\\nspeedup without SMT, 403–404\\nPartial disk failure, dirty bits, D-61 to \\n\\nD-64\\n\\nPartial store order, relaxed consistency \\nmodels, 395\\nPartitioned add operation, DSP media \\n\\nextensions, E-10\\n\\nPartitioning\\n\\nMultimedia SIMD Extensions, 282\\nvirtual memory protection, B-50\\nWSC memory hierarchy, 445\\n\\n217–218, 315–322\\n\\nParavirtualization\\n\\nPage tables\\n\\ntrace scheduling, H-19 to H-21, \\n\\n\\x0cstorage area network history, \\n\\naverage memory access time, \\n\\ninterprocessor communication, I-3 \\n\\nPascal programs\\n\\nPerformability, RAID reconstruction, \\n\\nD-55 to D-57\\nPerformance, see also Peak \\n\\nI-56 ■\\n\\nIndex\\n\\ncompiler types and classes, A-28\\ninteger division/remainder, J-12\\nPattern, disk array deconstruction, D-51\\nPayload\\n\\nmessages, F-6\\npacket format, F-7\\n\\np bits, J-21 to J-23, J-25, J-36 to J-37\\nPC, see Program counter (PC)\\nPCI bus, historical background, L-81\\nPCIe, see PCI-Express (PCIe)\\nPCI-Express (PCIe), F-29, F-63\\n\\nstorage area network history, \\n\\nF-102 to F-103\\n\\nPCI-X, F-29\\n\\nF-102\\n\\nPCI-X 2.', ' latency, 18–19\\nbenchmarks, 37–41\\nbranch penalty reduction, C-22\\nbranch schemes, C-25 to C-26\\ncache basics, B-3 to B-6\\ncache performance\\n\\nB-16 to B-20\\nbasic considerations, B-3 to \\n\\nB-6, B-16\\n\\nbasic equations, B-22\\nbasic optimizations, B-40\\nexample calculation, B-16 to \\n\\nout-of-order processors, B-20 \\n\\nto B-22\\ncompiler optimization impact, \\n\\ncost-performance\\n\\nextensive pipelining, C-80 to \\n\\nB-17\\n\\nA-27\\n\\nC-81\\n\\nWSC Flash memory, 474–475\\nWSC goals/requirements, 433\\nWSC hardware inactivity, 474\\nWSC processors, 472–473\\n\\nCUDA, 290–291\\ndesktop benchmarks, 38–40\\ndirectory-based coherence case \\n\\nstudy, 418–420\\n\\ndirty bits, D-61 to D-64\\ndisk array deconstruction, D-51 to \\n\\nD-55\\n\\ndisk deconstruction, D-48 to D-51\\nDRAM, 100–102\\nembedded computers, 9, E-13 to \\n\\nE-14\\nGoogle server benchmarks, \\n439–441\\n\\nhardward fallacies, 56\\nhigh-performance computing, 432, \\n435–436, B-10\\nhistorical milestones, 20\\nILP exploitation, 201\\n\\nF-29\\n\\nF-20\\n\\nD-40\\n\\nD-61\\n\\nD-36\\n\\nILP for realizable processors, \\n216–218\\nIntel Core i7, 239–241, 240, \\n401–405\\nIntel Core i7 memory, 122–124\\ninterconnection networks\\n\\nbandwidth considerations, F-89\\nmulti-device networks, F-25 to \\n\\nrouting/arbitration/switching \\n\\nimpact, F-52 to F-55\\n\\ntwo-device networks, F-12 to \\n\\nInternet Archive Cluster, D-38 to \\n\\nto I-6\\nI/O devices, D-15 to D-16\\nI/O subsystem design, D-59 to \\n\\nI/O system design/evaluation, \\n\\nISA, 241–243\\nItanium 2, H-43\\nlarge-scale multiprocessors\\nscientific applications\\n\\ndistributed-memory \\n\\nmultiprocessors, I-26 to \\nI-32, I-28 to I-30, I-32\\nparallel processors, I-33 to \\n\\nI-34\\n\\nsymmetric shared-memory \\nmultiprocessor, I-21 to \\nI-26, I-23 to I-25\\n\\nsynchronization, I-12 to I-16\\n\\nMapReduce, 438\\nmeasurement, reporting, \\n\\nsummarization, 36–37\\nmemory consistency models, 393\\nmemory hierarchy design, 73\\nmemory hierarchy and OS, B-58\\nmemory threads, GPUs, 332\\nMIPS FP pipeline, C-60 to C-61\\nMIPS M2000 vs.', ' scalar, 331–332\\nVMIPS on Linpack, G-17 to G-19\\nwormhole switching, F-92 to F-93\\n\\nPermanent failure, commercial \\ninterconnection \\nnetworks, F-66\\n\\nPermanent faults, storage systems, \\nD-11\\nPersonal computers\\nLANs, F-4\\nnetworks, F-2\\nPCIe, F-29\\n\\nPersonal mobile device (PMD)\\n\\ncharacteristics, 6\\nas computer class, 5\\nembedded computers, 8–9\\nFlash memory, 18\\nintegrated circuit cost trends, 28\\nISA performance and efficiency \\n\\nprediction, 241–243\\n\\nmemory hierarchy basics, 78\\nmemory hierarchy design, 72\\npower and energy, 25\\nprocessor comparison, 242\\nPetaBox GB2000, Internet Archive \\n\\nCluster, D-37\\n\\nstructure, A-26\\n\\nPhits, see Physical transfer units \\n(phits)\\n\\nPhysical addresses\\n\\naddress translation, B-46\\nAMD Opteron data cache, B-12 to \\n\\nARM Cortex-A8, 115\\ndirectory-based cache coherence \\n\\nprotocol basics, 382\\n\\nmain memory block, B-44\\nmemory hierarchy, B-48 to B-49\\nmemory hierarchy basics, 77–78\\nmemory mapping, B-52\\npaged virtual memory, B-55 to \\n\\nB-56\\n\\nscientific workloads, I-21 to \\n\\nB-13\\n\\nIndex\\n\\n■\\n\\nI-57\\n\\npage table-based mapping, B-45\\nsafe calls, B-54\\nsegmented virtual memory, B-51\\nsharing/protection, B-52\\ntranslation, B-36 to B-39\\nvirtual memory definition, B-42\\nPhysical cache, definition, B-36 to B-37\\nPhysical channels, F-47\\nPhysical layer, definition, F-82\\nPhysical memory\\n\\ncentralized shared-memory \\n\\nmultiprocessors, 347\\n\\ndirectory-based cache coherence, \\n\\n354\\nfuture GPU features, 332\\nGPU conditional branching, 303\\nmain memory block, B-44\\nmemory hierarchy basics, B-41 to \\n\\nB-42\\n\\nmultiprocessors, 345\\npaged virtual memory, B-56\\nprocessor comparison, 323\\nsegmented virtual memory, B-51\\nunified, 333\\nVirtual Machines, 110\\n\\nPhysical transfer units (phits), F-60\\nPhysical volumes, D-34\\nPID, see Process-identifier (PID) tags\\nPin-out bandwidth, topology, F-39\\nPipeline bubble, stall as, C-13\\nPipeline cycles per instruction\\n\\nbasic equation, 148\\nILP, 149\\nprocessor performance \\n\\ncalculations, 218–219\\n\\nPipelined circuit switching, F-50\\nPipelined CPUs, early versions, L-26 \\nto L-27\\n\\nPipeline delays\\n\\nARM Cortex-A8, 235\\ndefinition, 228\\nfine-grained multithreading, 227\\ninstruction set complications, C-50\\nmultiple branch speculation, 211\\nSun T1 multithreading unicore \\n\\nperformance, 227–228\\n\\nPipeline interlock\\n\\ndata dependences, 151\\ndata hazards requiring stalls, C-20\\nMIPS R4000, C-65\\nMIPS vs.', ' dynamic scheduling, 168–169\\nILP exploitation, 197\\nILP exposure, 157–161\\nmicroarchitectural techniques case \\nstudy, 247–254\\n\\nMIPS R4000, C-64\\n\\nPipeline stall cycles\\n\\nbranch cost reduction, C-26\\nbranch hazards, C-21 to C-26\\nbranch issues, C-39 to C-42\\nbranch penalty reduction, C-22 to \\n\\nbranch-prediction buffers, C-27 to \\nC-30, C-29\\nbranch scheme performance, C-25 \\n\\nC-25\\n\\nto C-26\\n\\ncache access, 82\\ncase studies, C-82 to C-88\\nclassic stages for RISC, C-6 to \\n\\nC-10\\ncompiler scheduling, L-31\\nconcept, C-2 to C-3\\ncost-performance, C-80 to C-81\\ndata hazards, C-16 to C-21\\ndefinition, C-2\\ndynamically scheduled pipelines, \\nC-70 to C-80\\n\\nexample, C-8\\nexception stopping/restarting, C-46 \\n\\nto C-47\\n\\nexception types and requirements, \\nC-43 to C-46\\n\\nexecution sequences, C-80\\nfloating-point addition speedup, \\n\\nJ-25\\n\\ngraphics pipeline history, L-51\\nhazard classes, C-11\\nhazard detection, C-38\\nimplementation difficulties, C-43 \\n\\nindependent FP operations, C-54\\ninstruction set complications, C-49 \\n\\nto C-49\\n\\nto C-51\\n\\ninterconnection networks, F-12\\nlatencies, C-87\\nMIPS, C-34 to C-36\\nMIPS control, C-36 to C-39\\nMIPS exceptions, C-48, C-48 to \\n\\nC-49\\n\\nMIPS FP performance, C-60 to \\n\\nC-61\\nMIPS multicycle operations\\n\\nbasic considerations, C-51 to \\n\\nhazards and forwarding, C-54 \\n\\nC-54\\n\\nto C-58\\n\\noverview, C-61 to C-65\\npipeline performance, C-67 to \\n\\noperations, C-54\\n\\nperformance issues, C-10 to C-11\\nperformance with stalls, C-12 to \\n\\npredicted-not-taken scheme, C-22\\nRISC instruction set, C-4 to C-5, \\n\\nsimple implementation, C-30 to \\nC-43, C-34\\n\\nsimple RISC, C-5 to C-6, C-7\\nstatic branch prediction, C-26 to \\n\\nstructural hazards, C-13 to C-16, \\n\\nC-67\\n\\nC-70\\n\\nC-13\\n\\nC-70\\n\\nC-27\\n\\nC-15\\n\\nswitch microarchitecture, F-60 to \\n\\nF-61\\nunoptimized code, C-81\\nPipe segment, definition, C-3\\nPipe stage\\n\\nbranch prediction, C-28\\ndata hazards, C-16\\ndefinition, C-3\\ndynamic scheduling, C-71\\nFP pipeline, C-66\\nintegrated instruction fetch units, \\n\\n207\\n\\nMIPS, C-34 to C-35, C-37, C-49\\nMIPS extension, C-53\\nMIPS R4000, C-62\\nout-of-order execution, 170\\npipeline stalls, C-13\\npipeling performance issues, C-10\\nregister additions, C-35\\nRISC processor, C-7\\nstopping/restarting execution, C-46\\nWAW, 153\\n\\npjbb2005 benchmark\\nIntel Core i7, 402\\nSMT on superscalar processors, \\n230–232, 231\\n\\nPLA, early computer arithmetic, J-65\\nPMD, see Personal mobile device \\n(PMD)\\n\\nPoints-to analysis, basic approach, H-9\\nPoint-to-point links\\n\\nbus replacement, D-34\\nEthernet, F-79\\nstorage systems, D-34\\nswitched-media networks, F-24\\n\\nPoint-to-point multiprocessor, \\n\\ndirectory-based coherence, 418\\ndirectory protocol, 421–422\\nSMP limitations, 363–364\\n\\nPoison bits, compiler-based \\n\\nspeculation, H-28, H-30\\n\\nPoisson, Siméon, D-28\\nPoisson distribution\\n\\nbasic equation, D-28\\nrandom variables, D-26 to D-34\\n\\nPolycyclic scheduling, L-30\\nPortable computers\\n\\ninterconnection networks, F-85\\nprocessor comparison, 242\\n\\nPort number, network interfaces, F-7\\n\\npipeline structure, C-62 to C-63\\n\\nexample, 413\\n\\nmultiple outstanding FP \\n\\nPoint-to-point networks\\n\\nbranch scheme performance, C-25\\npipeline performance, C-12 to C-13\\n\\nPipelining\\n\\nprecise exceptions, C-58 to C-60\\n\\nMIPS R4000\\n\\nFP pipeline, C-65 to C-67, \\n\\n\\x0cPosition independence, control flow \\ninstruction addressing \\nmodes, A-17\\n\\nPower\\n\\ndistribution for servers, 490\\ndistribution overview, 447\\nand DLP, 322\\nfirst-level caches, 79–80\\nGoogle server benchmarks, \\n439–441\\n\\nGoogle WSC, 465–468\\nPMDs, 6\\nreal-world server considerations, \\n\\n52–55\\n\\nWSC infrastructure, 447\\nWSC power modes, 472\\nWSC resource allocation case \\n\\nstudy, 478–479\\n\\nWSC TCO case study, 476–478\\nPower consumption, see also Energy \\nefficiency\\n\\ncache optimization, 96\\ncache size and associativity, 81\\ncase study, 63–64\\ncomputer components, 63\\nDDR3 SDRAM, 103\\ndisks, D-5\\nembedded benchmarks, E-13\\nGPUs vs.', ' datacenters, 456\\nWSC server energy efficiency, 462\\n\\nPrecise exceptions\\ndefinition, C-47\\ndynamic scheduling, 170\\nhardware-based speculation, \\n\\n187–188, 221\\ninstruction set complications, C-49\\nmaintaining, C-58 to C-60\\nMIPS exceptions, C-48\\n\\nPrecisions, floating-point arithmetic, \\n\\nJ-33 to J-34\\n\\nPredicated instructions\\n\\nexposing parallelism, H-23 to H-27\\nIA-64, H-38 to H-40\\n\\nIndex\\n\\n■\\n\\nI-59\\n\\nPredicate Registers\\ndefinition, 309\\nGPU conditional branching, 300–301\\nIA-64, H-34\\nNVIDIA GPU ISA, 298\\nvectors vs.', ' GPUs, 311\\n\\nPredication, TI TMS320C6x DSP, E-10\\nPredicted-not-taken scheme\\n\\nbranch penalty reduction, C-22, \\nC-22 to C-23\\n\\nMIPS R4000 pipeline, C-64\\nPredictions, see also Mispredictions\\naddress aliasing, 213–214, 216\\nbranch\\n\\ncorrelation, 162–164\\ncost reduction, 162–167, C-26\\ndynamic, C-27 to C-30\\nideal processor, 214\\nILP exploitation, 201\\ninstruction fetch bandwidth, 205\\nintegrated instruction fetch \\n\\nunits, 207\\n\\nIntel Core i7, 166–167, 239–241\\nstatic, C-26 to C-27\\n\\nbranch-prediction buffers, C-27 to \\nC-30, C-29\\njump prediction, 214\\nPMDs, 6\\nreturn address buffer, 207\\n2-bit scheme, C-28\\nvalue prediction, 202, 212–213\\n\\nIntel Core i7, 122, 123–124\\nItanium 2, H-42\\nMIPS core extensions, K-20\\nNVIDIA GPU Memory structures, \\n\\n208\\n\\n305\\n\\nparallel processing challenges, 351\\nPrefix, Intel 80x86 integer operations, \\n\\nK-51\\nPresentation layer, definition, F-82\\nPresent bit, IA-32 descriptor table, \\nB-52\\n\\nPrice vs.', ' cost, 32–33\\nPrice-performance ratio\\ncost trends, 28\\nDell PowerEdge servers, 53\\ndesktop comptuers, 6\\nprocessor comparisons, 55\\nWSCs, 8, 441\\n\\nefficiency ratings, 462\\n\\nPrefetching\\n\\nPower utilization effectiveness (PUE)\\n\\nintegrated instruction fetch units, \\n\\n\\x0cPA-RISC instructions, K-34 to \\n\\nProcess-identifier (PID) tags, cache \\n\\nI-60 ■\\n\\nIndex\\n\\nPrimitives\\n\\narchitect-compiler writer \\n\\nrelationship, A-30\\n\\nbasic hardware types, 387–389\\ncompiler writing-architecture \\nrelationship, A-30\\n\\nCUDA Thread, 289\\ndependent computation \\n\\nelimination, 321\\n\\nGPU vs.', ' MIMD, 329\\nlocks via coherence, 391\\noperand types and sizes, A-14 to \\n\\nA-15\\n\\nK-35\\nsynchronization, 394, L-64\\n\\nPrinciple of locality\\n\\nbidirectional MINs, F-33 to F-34\\ncache optimization, B-26\\ncache performance, B-3 to B-4\\ncoining of term, L-11\\ncommercial workload, 373\\ncomputer design principles, 45\\ndefinition, 45, B-2\\nlock accesses, 390\\nLRU, B-9\\nmemory accesses, 332, B-46\\nmemory hierarchy design, 72\\nmultilevel application, B-2\\nmultiprogramming workload, 375\\nscientific workloads on symmetric \\nshared-memory \\nmultiprocessors, I-25\\n\\nstride, 278\\nWSC bottleneck, 461\\nWSC efficiency, 450\\n\\nPrivate data\\n\\ncache protocols, 359\\ncentralized shared-memory \\nmultiprocessors, \\n351–352\\n\\ndefinition, 292, 314\\nNVIDIA GPU Memory structures, \\n\\n304\\nPrivate variables, NVIDIA GPU \\n\\nMemory, 304\\n\\nProcedure calls\\n\\ncompiler structure, A-25 to A-26\\ncontrol flow instructions, A-17, \\nA-19 to A-21\\n\\ndependence analysis, 321\\n\\nhigh-level instruction set, A-42 to \\n\\nA-43\\nIA-64 register model, H-33\\ninvocation options, A-19\\nISAs, 14\\nMIPS control flow instructions, A-38\\nreturn address predictors, 206\\nVAX, B-73 to B-74, K-71 to K-72\\nVAX vs.', ' cache performance, B-16\\nclock rate trends, 24\\ndesktop benchmarks, 38, 40\\nhistorical trends, 3, 3–4\\nmultiprocessors, 347\\nuniprocessors, 344\\n\\ncomputer design \\nprinciples, 48–52\\n\\nProcessor speed\\n\\nand clock rate, 244\\nand CPI, 244\\nsnooping cache coherence, 364\\n\\nmultithreading, 224\\nPID, B-37\\nvirtual memory-based protection, \\nB-49 to B-50\\n\\nProducer-server model, response time \\n\\nand throughput, D-16\\n\\nProductivity\\n\\nCUDA, 290–291\\nNVIDIA programmers, 289\\nsoftware development, 4\\nvirtual memory and programming, \\n\\nB-41\\n\\nWSC, 450\\n\\nProfile-based predictor, misprediction \\nrate, C-27\\n\\nProgram counter (PC)\\n\\naddressing modes, A-10\\nARM Cortex-A8, 234\\nbranch hazards, C-21\\nbranch-target buffers, 203, \\n\\n203–204, 206\\ncontrol flow instruction addressing \\nmodes, A-17\\ndynamic branch prediction, C-27 \\n\\nexception stopping/restarting, C-46 \\n\\nto C-28\\n\\nto C-47\\n\\nGPU conditional branching, 303\\nIntel Core i7, 120\\nM32R instructions, K-39\\nMIPS control flow instructions, \\n\\nA-38\\nmultithreading, 223–224\\npipeline branch issues, C-39 to \\n\\nC-41\\n\\npipe stages, C-35\\nprecise exceptions, C-59 to C-60\\nRISC classic pipeline, C-8\\nRISC instruction set, C-5\\nsimple MIPS implementation, \\n\\nC-31 to C-33\\n\\nTLP, 344\\nvirtual memory protection, 106\\nProgram counter-relative addressing\\n\\ncontrol flow instructions, A-17 to \\nA-18, A-21\\n\\ndefinition, A-10\\nMIPS instruction format, A-35\\n\\nProcess switch\\n\\nProgramming models\\n\\ndefinition, 106, B-49\\nmiss rate vs.', ' virtual addressing, \\n\\nB-37\\n\\nCUDA, 300, 310, 315\\nGPUs, 288–291\\nlatency in consistency models, 397\\n\\nPrivate Memory\\n\\nProcessor performance equation, \\n\\n\\x0cmemory consistency, 393\\nMultimedia SIMD architectures, \\n\\n285\\n\\nWAN history, F-98\\n\\nQuantitative performance measures, \\n\\ndevelopment, L-6 to L-7\\n\\nvector architectures, 280–282\\nWSCs, 436–441\\n\\nProgramming primitive, CUDA \\n\\nThread, 289\\n\\nProgram order\\n\\ncache coherence, 353\\ncontrol dependences, 154–155\\ndata hazards, 153\\ndynamic scheduling, 168–169, 174\\nhardware-based speculation, 192\\nILP exploitation, 200\\nname dependences, 152–153\\nTomasulo’s approach, 182\\n\\nProtection schemes\\n\\ncontrol dependence, 155\\ndevelopment, L-9 to L-12\\nand ISA, 112\\nnetwork interfaces, F-7\\nnetwork user access, F-86 to F-87\\nPentium vs.', ' Opteron, B-57\\nprocesses, B-50\\nsafe calls, B-54\\nsegmented virtual memory \\n\\nexample, B-51 to B-54\\n\\nVirtual Machines, 107–108\\nvirtual memory, 105–107, B-41\\n\\nProtocol deadlock, routing, F-44\\nProtocol stack\\n\\nexample, F-83\\ninternetworking, F-83\\n\\nPseudo-least recently used (LRU)\\n\\nblock replacement, B-9 to B-10\\nIntel Core i7, 118\\n\\nPSUs, see Power Supply Units (PSUs)\\nPTE, see Page Table Entry (PTE)\\nPTX, see Parallel Thread Execution \\n(PTX)\\n\\nPUE, see Power utilization \\n\\nQueue\\n\\ndefinition, D-24\\nwaiting time calculations, D-28 to \\n\\nD-29\\nQueue discipline, definition, D-26\\nQueuing locks, large-scale \\n\\nmultiprocessor \\nsynchronization, I-18 to \\nI-21\\n\\nQueuing theory\\n\\nbasic assumptions, D-30\\nLittle’s law, D-24 to D-25\\nM/M/1 model, D-31 to D-33, D-32\\noverview, D-23 to D-26\\nRAID performance prediction, \\n\\nD-57 to D-59\\n\\nsingle-server model, D-25\\nQuickpath (Intel Xeon), cache \\n\\ncoherence, 361\\n\\nR\\nRace-to-halt, definition, 26\\nRack units (U), WSC architecture, 441\\nRadio frequency amplifier, radio \\n\\nreceiver, E-23\\n\\nRadio receiver, components, E-23\\nRadio waves, wireless networks, E-21\\nRadix-2 multiplication/division, J-4 to \\n\\nJ-7, J-6, J-55\\n\\nRadix-4 multiplication/division, J-48 \\n\\nto J-49, J-49, J-56 to \\nJ-57, J-60 to J-61\\n\\nRadix-8 multiplication, J-49\\nRAID (Redundant array of \\n\\ninexpensive disks)\\n\\ndata replication, 439\\ndependability benchmarks, D-21, \\n\\nD-22\\n\\nD-48\\n\\nL-80\\n\\nD-61\\n\\nQ\\nQCDOD, L-64\\nQoS, see Quality of service (QoS)\\nQsNetII, F-63, F-76\\nQuadrics SAN, F-67, F-100 to F-101\\nQuality of service (QoS)\\n\\ndependability benchmarks, D-21\\n\\nhardware dependability, D-15\\nhistorical background, L-79 to \\n\\nI/O subsystem design, D-59 to \\n\\nlogical units, D-35\\nmemory dependability, 104\\n\\nIndex\\n\\n■\\n\\nI-61\\n\\nNetApp FAS6000 filer, D-41 to \\n\\nD-42\\noverview, D-6 to D-8, D-7\\nperformance prediction, D-57 to \\n\\nD-59\\n\\nreconstruction case study, D-55 to \\n\\nD-57\\nrow-diagonal parity, D-9\\nWSC storage, 442\\nRAID 0, definition, D-6\\nRAID 1\\n\\ndefinition, D-6\\nhistorical background, L-79\\n\\nRAID 2\\n\\nRAID 3\\n\\nRAID 4\\n\\nRAID 5\\n\\nRAID 6\\n\\ndefinition, D-6\\nhistorical background, L-79\\n\\ndefinition, D-7\\nhistorical background, L-79 to \\n\\ndefinition, D-7\\nhistorical background, L-79 to \\n\\ndefinition, D-8\\nhistorical background, L-79 to \\n\\nL-80\\n\\nL-80\\n\\nL-80\\n\\ncharacteristics, D-8 to D-9\\nhardware dependability, D-15\\n\\nRAID 10, D-8\\nRAM (random access memory), switch \\n\\nmicroarchitecture, F-57\\n\\nRAMAC-350 (Random Access \\n\\nMethod of Accounting \\nControl), L-77 to L-78, \\nL-80 to L-81\\nRandom Access Method of \\n\\nAccounting Control, \\nL-77 to L-78\\n\\nRandom variables, distribution, D-26 \\nto D-34\\n\\nRAR, see Read after read (RAR)\\nRAS, see Row access strobe (RAS)\\nRAW, see Read after write (RAW)\\nRay casting (RC)\\n\\nGPU comparisons, 329\\nthroughput computing kernel, 327\\n\\neffectiveness (PUE)\\n\\ndisk array deconstruction case \\n\\nRandom replacement\\n\\nPython language, hardware impact on \\nsoftware development, 4\\n\\nstudy, D-51, D-55\\n\\ndisk deconstruction case study, \\n\\ncache misses, B-10\\ndefinition, B-9\\n\\n\\x0cI-62 ■\\n\\nIndex\\n\\nRDMA, see Remote direct memory \\n\\nRearrangeably nonblocking, \\n\\nRedundant multiplication, integers, \\n\\naccess (RDMA)\\n\\nRead after read (RAR), absence of \\n\\ncentralized switched \\nnetworks, F-32 to F-33\\n\\ndata hazard, 154\\n\\nReceiving overhead\\n\\nRECN, see Regional explicit \\n\\nRegional explicit congestion \\n\\nF-44\\n\\nRegister addressing mode\\n\\nRead after write (RAW)\\ndata hazards, 153\\ndynamic scheduling with \\n\\nTomasulo’s algorithm, \\n170–171\\nfirst vector computers, L-45\\nhazards, stalls, C-55\\nhazards and forwarding, C-55 to \\n\\nC-57\\n\\ninstruction set complications, C-50\\nmicroarchitectural techniques case \\n\\nstudy, 253\\nMIPS FP pipeline performance, \\nC-60 to C-61\\nMIPS pipeline control, C-37 to C-38\\nMIPS pipeline FP operations, C-53\\nMIPS scoreboarding, C-74\\nROB, 192\\nTI TMS320C55 DSP, E-8\\nTomasulo’s algorithm, 182\\nunoptimized code, C-81\\n\\nAMD Opteron data cache, B-14\\ncache coherence, 357, 358, \\n359–361\\n\\ncoherence extensions, 362\\ndirectory-based cache coherence \\nprotocol example, 380, \\n382–386\\n\\nmemory hierarchy basics, 76–77\\nmemory stall clock cycles, B-4\\nmiss penalty reduction, B-35 to \\n\\nB-36\\nOpteron data cache, B-14\\nvs.', ' SANs, F-27\\ntime of flight, F-14\\n\\ncongestion notification \\n(RECN)\\nReconfiguration deadlock, routing, \\n\\nReconstruction, RAID, D-55 to D-57\\nRecovery time, vector processor, G-8\\nRecurrences\\n\\nbasic approach, H-11\\nloop-carried dependences, H-5\\n\\nRed-black Gauss-Seidel, Ocean \\n\\napplication, I-9 to I-10\\nReduced Instruction Set Computer, \\n\\nsee RISC (Reduced \\nInstruction Set \\nComputer)\\n\\nReductions\\n\\ncommercial workloads, 371\\ncost trends, 28\\nloop-level parallelism \\n\\nWSCs, 438\\n\\nRedundancy\\n\\nAmdahl’s law, 48\\nchip fabrication cost case study, \\n\\n61–62\\n\\ncomputer system power \\n\\nconsumption case study, \\n63–64\\nindex checks, B-8\\nintegrated circuit cost, 32\\nintegrated circuit failure, 35\\nsimple MIPS implementation, \\n\\nC-33\\n\\nWSC, 433, 435, 439\\nWSC bottleneck, 461\\nWSC storage, 442\\n\\nRedundant array of inexpensive disks, \\n\\nsee RAID (Redundant \\narray of inexpensive \\ndisks)\\n\\nRedundant power supplies, example \\n\\ncalculations, 35\\n\\nReference bit\\n\\nmemory hierarchy, B-52\\nvirtual memory block replacement, \\n\\nJ-48\\n\\nB-45\\n\\nnotification (RECN), \\ncongestion \\nmanagement, F-66\\n\\nMIPS, 12\\nVAX, K-67\\nRegister allocation\\n\\ncompilers, 396, A-26 to A-29\\nVAX sort, K-76\\nVAX swap, K-72\\n\\nRegister deferred addressing, VAX, \\n\\nK-67\\n\\nRegister definition, 314\\nRegister fetch (RF)\\n\\nMIPS data path, C-34\\nMIPS R4000, C-63\\npipeline branches, C-41\\nsimple MIPS implementation, \\n\\ndata hazards, C-16, C-18, C-20\\ndynamic scheduling, 172, 173, \\n\\n175, 177–178\\n\\nFermi GPU, 306\\nfield, 176\\nhardware-based speculation, 184\\nlonger latency pipelines, C-55 to \\n\\nC-57\\n\\nMIPS exceptions, C-49\\nMIPS implementation, C-31, C-33\\nMIPS R4000, C-64\\nMIPS scoreboarding, C-75\\nMultimedia SIMD Extensions, \\n282, 285\\n\\nmultiple lanes, 272, 273\\nmultithreading, 224\\nOCNs, F-3\\nprecise exceptions, C-59\\nRISC classic pipeline, C-7 to C-8\\nRISC instruction set, C-5 to C-6\\nscoreboarding, C-73, C-75\\n\\ndependences, 321\\n\\nsimple RISC implementation, C-5 \\n\\nmultiprogramming workloads, 377\\nT1 multithreading unicore \\n\\nperformance, 227\\n\\nRegister file\\n\\nC-31\\n\\nto C-6\\n\\n\\x0cRegister-memory instruction set \\n\\nRelaxed consistency models\\n\\nspeculation support, 208\\nstructural hazards, C-13\\nTomasulo’s algorithm, 180, 182\\nvector architecture, 264\\nVMIPS, 265, 308\\n\\nRegister indirect addressing mode, \\n\\nIntel 80x86, K-47\\n\\nRegister management, \\n\\nsoftware-pipelined \\nloops, H-14\\n\\narchitecture\\n\\narchitect-compiler writer \\n\\nrelationship, A-30\\n\\ndynamic scheduling, 171\\nIntel 80x86, K-52\\nISA classification, 11, A-3 to A-6\\nRegister prefetch, cache optimization, \\n\\nRegister renaming\\n\\ndynamic scheduling, 169–172\\nhardware vs.', ' VAX 8700, L-21\\nMultimedia SIMD Extensions \\nhistory, L-49 to L-50\\n\\noperations, 12\\nPA-RISC-unique, K-33 to K-35\\npipelining efficiency, C-70\\nPowerPC-unique instructions, \\n\\nK-32 to K-33\\nSanyo VPC-SX500 digital camera, \\n\\nsimple implementation, C-5 to C-6\\nsimple pipeline, C-7\\nSPARC-unique instructions, K-29 \\n\\nSun T1 multithreading, 226–227\\nSuperH-unique instructions, K-38 \\n\\nE-19\\n\\nto K-32\\n\\nto K-39\\n\\nThumb-unique instructions, K-37 \\n\\nto K-38\\nvector processor history, G-26\\nVirtual Machines ISA support, 109\\nVirtual Machines and virtual \\n\\nmemory and I/O, 110\\n\\nRISC-I, L-19 to L-20\\nRISC-II, L-19 to L-20\\nRLP, see Request-level parallelism \\n(RLP)\\nROB, see Reorder buffer (ROB)\\nRoofline model\\n\\nGPU performance, 326\\nmemory bandwidth, 332\\nMultimedia SIMD Extensions, \\n\\n285–288, 287\\n\\nRound digit, J-18\\nRounding modes, J-14, J-17 to J-19, \\nJ-18, J-20\\n\\nFP precisions, J-34\\nfused multiply-add, J-33\\n\\nRound-robin (RR)\\n\\narbitration, F-49\\nIBM 360, K-85 to K-86\\nInfiniBand, F-74\\n\\nRouters\\n\\nprediction, 241\\n\\nBARRNet, F-80\\n\\n\\x0cEthernet, F-79\\nRouting algorithm\\n\\nstorage area network history, F-103\\n\\nSaturating arithmetic, DSP media \\n\\ncommercial interconnection \\n\\nextensions, E-11\\n\\nnetworks, F-56\\n\\nSaturating operations, definition, K-18 \\n\\nfault tolerance, F-67\\nimplementation, F-57\\nIntel SCCC, F-70\\ninterconnection networks, F-21 to \\nF-22, F-27, F-44 to F-48\\n\\nmesh network, F-46\\nnetwork impact, F-52 to F-55\\nOCN history, F-104\\nand overhead, F-93 to F-94\\nSAN characteristics, F-76\\nswitched-media networks, F-24\\nswitch microarchitecture \\n\\npipelining, F-61\\nsystem area network history, F-100\\nRow access strobe (RAS), DRAM, 98\\nRow-diagonal parity\\nexample, D-9\\nRAID, D-9\\n\\nRow major order, blocking, 89\\nRR, see Round-robin (RR)\\nRS format instructions, IBM 360, \\nK-87\\n\\nRuby on Rails, hardware impact on \\n\\nsoftware development, 4\\n\\nRX format instructions, IBM 360, \\n\\nK-86 to K-87\\n\\nS\\nS3, see Amazon Simple Storage \\n\\nService (S3)\\nSaaS, see Software as a Service (SaaS)\\nSandy Bridge dies, wafter example, 31\\nSANs, see System/storage area \\nnetworks (SANs)\\n\\nSanyo digital cameras, SOC, E-20\\nSanyo VPC-SX500 digital camera, \\n\\nembedded system case \\nstudy, E-19\\n\\nSAS, see Serial Attach SCSI (SAS) \\n\\ndrive\\n\\nSASI, L-81\\nSATA (Serial Advanced Technology \\n\\nAttachment) disks\\n\\nGoogle WSC servers, 469\\nNetApp FAS6000 filer, D-42\\npower consumption, D-5\\nRAID 6, D-8\\nvs.', ' SAS drives, D-5\\n\\nto K-19\\n\\nSAXPY, GPU raw/relative \\n\\nperformance, 328\\n\\nScalability\\n\\ncloud computing, 460\\ncoherence issues, 378–379\\nFermi GPU, 295\\nJava benchmarks, 402\\nmulticore processors, 400\\nmultiprocessing, 344, 395\\nparallelism, 44\\nas server characteristic, 7\\ntransistor performance and wires, \\n\\n19–21\\n\\nWSCs, 8, 438\\nWSCs vs.', ' servers, 434\\n\\nScalable GPUs, historical background, \\nL-50 to L-51\\nScalar expansion, loop-level parallelism \\n\\ndependences, 321\\n\\nScalar Processors, see also \\n\\nSuperscalar processors\\n\\ndefinition, 292, 309\\nearly pipelined CPUs, L-26 to L-27\\nlane considerations, 273\\nMultimedia SIMD/GPU \\n\\ncomparisons, 312\\n\\nNVIDIA GPU, 291\\nprefetch units, 277\\nvs.', ' vector, 311, G-19\\nvector performance, 331–332\\n\\nScalar registers\\n\\nCray X1, G-21 to G-22\\nGPUs vs.', ' GPUs, 312\\nsample renaming code, 251\\nvector vs.', ' GPU, 311\\nvs.', ' switched-media \\n\\nnetworks, F-25\\ntransistor performance and wires, \\n\\n19–21\\n\\nVMIPS, 267\\n\\nScan Line Interleave (SLI), scalable \\n\\nGPUs, L-51\\n\\nSCCC, see Intel Single-Chip Cloud \\n\\nComputing (SCCC)\\n\\nSchorr, Herb, L-28\\nScientific applications\\nBarnes, I-8 to I-9\\nbasic characteristics, I-6 to I-7\\ncluster history, L-62\\ndistributed-memory \\n\\nmultiprocessors, I-26 to \\nI-32, I-28 to I-32\\n\\nFFT kernel, I-7\\nLU kernel, I-8\\nOcean, I-9 to I-10\\nparallel processors, I-33 to I-34\\nparallel program computation/\\n\\ncommunication, I-10 to \\nI-12, I-11\\n\\nparallel programming, I-2\\nsymmetric shared-memory \\n\\nmultiprocessors, I-21 to \\nI-26, I-23 to I-25\\n\\nARM Cortex-A8, 233, 234\\ncomponents, C-76\\ndefinition, 170\\ndynamic scheduling, 171, 175\\nand dynamic scheduling, C-71 to \\n\\nC-80\\nexample calculations, C-77\\nMIPS structure, C-73\\nNVIDIA GPU, 296\\nresults tables, C-78 to C-79\\nSIMD thread scheduler, 296\\n\\nScoreboarding\\n\\n\\x0cScripting languages, software \\n\\ndevelopment impact, 4\\n\\nsafe calls, B-54\\nsharing and protection, B-52 to \\n\\nSCSI (Small Computer System \\n\\nB-53\\n\\nBerkeley’s Tertiary Disk project, \\n\\nJ-28 to J-29\\n\\n353\\n\\nI-66 ■\\n\\nIndex\\n\\nInterface)\\n\\nD-12\\n\\ndependability benchmarks, D-21\\ndisk storage, D-4\\nhistorical background, L-80 to L-81\\nI/O subsystem design, D-59\\nRAID reconstruction, D-56\\nstorage area network history, \\n\\nF-102\\n\\nSDRAM, see Synchronous dynamic \\n\\nrandom-access memory \\n(SDRAM)\\n\\nSDRWAVE, J-62\\nSecond-level caches, see also L2 \\ncaches\\n\\nARM Cortex-A8, 114\\nILP, 245\\nIntel Core i7, 121\\ninterconnection network, F-87\\nItanium 2, H-41\\nmemory hierarchy, B-48 to B-49\\nmiss penalty calculations, B-33 to \\n\\nmiss penalty reduction, B-30 to \\n\\nmiss rate calculations, B-31 to \\n\\nB-34\\n\\nB-35\\n\\nB-35\\n\\nand relative execution time, B-34\\nspeculation, 210\\nSRAM, 99\\n\\nSecure Virtual Machine (SVM), 129\\nSeek distance\\n\\nstorage disks, D-46\\nsystem comparison, D-47\\nSeek time, storage disks, D-46\\nSegment basics\\n\\nIntel 80x86, K-50\\nvs.', ' paged, B-43\\n\\nSelf-correction, Newton’s algorithm, \\n\\nSelf-draining pipelines, L-29\\nSelf-routing, MINs, F-48\\nSemantic clash, high-level instruction \\nset, A-41\\nSemantic gap, high-level instruction \\nset, A-39\\n\\nSemiconductors\\n\\nDRAM technology, 17\\nFlash memory, 18\\nGPU vs.', ' SATA drives, D-5\\n\\nSerialization\\n\\nbarrier synchronization, I-16\\ncoherence enforcement, 354\\ndirectory-based cache coherence, \\n\\n382\\n\\nDSM multiprocessor cache \\n\\ncoherence, I-37\\nhardware primitives, 387\\nmultiprocessor cache coherency, \\n\\npage tables, 408\\nsnooping coherence protocols, 356\\nwrite invalidate protocol \\n\\nimplementation, 356\\n\\nSerpentine recording, L-77\\nServe-longest-queue (SLQ) scheme, \\n\\narbitration, F-49\\n\\nServerNet interconnection network, \\n\\nfault tolerance, F-66 to \\nF-67\\nServers, see also Warehouse-scale \\n\\ncomputers (WSCs)\\n\\nas computer class, 5\\ncost calculations, 454, 454–455\\ndefinition, D-24\\nenergy savings, 25\\nGoogle WSC, 440, 467, 468–469\\nGPU features, 324\\nmemory hierarchy design, 72\\nvs.', ' mobile GPUs, 323–330\\nmultiprocessor importance, 344\\noutage/anomaly statistics, 435\\nperformance benchmarks, 40–41\\npower calculations, 463\\npower distribution example, 490\\npower-performance benchmarks, \\n54, 439–441\\npower-performance modes, 477\\nreal-world examples, 52–55\\nRISC systems\\n\\naddressing modes and \\n\\ninstruction formats, K-5 \\nto K-6\\n\\nexamples, K-3, K-4\\ninstruction formats, K-7\\nmultimedia extensions, K-16 \\n\\nto K-19\\nsingle-server model, D-25\\nsystem characteristics, E-4\\nworkload demands, 439\\nWSC vs.', ' switched-media networks, F-24 \\n\\n418–420\\nDSM, 347–348, 348, 354–355, \\n378–380\\ninvalidate protocols, 356–357\\nSMP/DSM definition, 348\\nterminology comparison, 315\\nShared-memory communication, \\n\\nlarge-scale \\nmultiprocessors, I-5\\n\\nShared-memory multiprocessors\\nbasic considerations, 351–352\\nbasic structure, 346–347\\ncache coherence, 352–353\\ncache coherence enforcement, \\n354–355\\n\\ncache coherence example, \\n\\n357–362\\ncache coherence extensions, \\n362–363\\n\\ndata caching, 351–352\\ndefinition, L-63\\nhistorical background, L-60 to \\n\\nL-61\\n\\ninvalidate protocol \\n\\nimplementation, \\n356–357\\n\\nlimitations, 363–364\\nperformance, 366–378\\nsingle-chip multicore case study, \\n\\n412–418\\nSMP and snooping limitations, \\n363–364\\nsnooping coherence \\n\\nimplementation, \\n365–366\\n\\nIndex\\n\\n■\\n\\nI-67\\n\\nShared-memory synchronization, \\n\\nMIPS core extensions, \\nK-21\\n\\nShared state\\n\\ncache block, 357, 359\\ncache coherence, 360\\ncache miss calculations, 366–367\\ncoherence extensions, 362\\ndirectory-based cache coherence \\n\\nprotocol basics, 380, \\n385\\n\\nShear algorithms, disk array \\n\\nShifting over zeros, integer \\n\\ndeconstruction, D-51 to \\nD-52, D-52 to D-54\\n\\nmultiplication/division, \\nJ-45 to J-47\\n\\nShort-circuiting, see Forwarding\\nSI format instructions, IBM 360, K-87\\nSignals, definition, E-2\\nSignal-to-noise ratio (SNR), wireless \\n\\nnetworks, E-21\\n\\nSigned-digit representation\\n\\nexample, J-54\\ninteger multiplication, J-53\\n\\nSigned number arithmetic, J-7 to J-10\\nSign-extended offset, RISC, C-4 to \\n\\nC-5\\nSignificand, J-15\\nSign magnitude, J-7\\nSilicon Graphics 4D/240, L-59\\nSilicon Graphics Altix, F-76, L-63\\nSilicon Graphics Challenge, L-60\\nSilicon Graphics Origin, L-61, L-63\\nSilicon Graphics systems (SGI)\\neconomies of scale, 456\\nmiss statistics, B-59\\nmultiprocessor software \\n\\ndevelopment, 407–409\\n\\nvector processor history, G-27\\nSIMD (Single Instruction Stream, \\n\\nMultiple Data Stream)\\n\\ndefinition, 10\\nFermi GPU architectural \\n\\ninnovations, 305–308\\nGPU conditional branching, 301\\n\\n\\x0cI-68 ■\\n\\nIndex\\n\\nSIMD (continued)\\n\\nGPU examples, 325\\nGPU programming, 289–290\\nGPUs vs.', ' vector architectures, \\n308–309\\n\\nhistorical overview, L-55 to L-56\\nloop-level parallelism, 150\\nMapReduce, 438\\nmemory bandwidth, 332\\nmultimedia extensions, see \\n\\nMultimedia SIMD \\nExtensions\\nmultiprocessor architecture, 346\\nmultithreaded, see Multithreaded \\n\\nSIMD Processor\\n\\nNVIDIA GPU computational \\n\\nstructures, 291\\nNVIDIA GPU ISA, 300\\npower/DLP issues, 322\\nspeedup via parallelism, 263\\nsupercomputer development, L-43 \\n\\nto L-44\\n\\nsystem area network history, F-100\\nThread Block mapping, 293\\nTI 320C6x DSP, E-9\\n\\nSIMD Instruction\\n\\nCUDA Thread, 303\\ndefinition, 292, 313\\nDSP media extensions, E-10\\nfunction, 150, 291\\nGPU Memory structures, 304\\nGPUs, 300, 305\\nGrid mapping, 293\\nIBM Blue Gene/L, I-42\\nIntel AVX, 438\\nmultimedia architecture \\n\\nprogramming, 285\\n\\nmultimedia extensions, 282–285, \\n\\n312\\n\\nmultimedia instruction compilers, \\nA-31 to A-32\\n\\nMultithreaded SIMD Processor \\n\\nblock diagram, 294\\n\\nPTX, 301\\nSony PlayStation 2, E-16\\nThread of SIMD Instructions, \\n295–296\\n\\nthread scheduling, 296–297, 297, \\n\\n305\\n\\nvector architectures as superset, \\n\\n263–264\\nvector/GPU comparison, 308\\n\\nVector Registers, 309\\n\\nSIMD Lane Registers, definition, 309, \\n\\n314\\n\\nSIMD Lanes\\n\\ndefinition, 292, 296, 309\\nDLP, 322\\nFermi GPU, 305, 307\\nGPU, 296–297, 300, 324\\nGPU conditional branching, \\n302–303\\n\\nGPUs vs.', ' GPUs, 312, \\n\\n315\\n\\nmultithreaded processor, 294\\nNVIDIA GPU Memory, 304\\nsynchronization marker, 301\\nvector vs.', ' GPU, 308, 311\\n\\nSIMD Processors, see also \\n\\nMultithreaded SIMD \\nProcessor\\nblock diagram, 294\\ndefinition, 292, 309, 313–314\\ndependent computation \\n\\nelimination, 321\\n\\ndesign, 333\\nFermi GPU, 296, 305–308\\nFermi GTX 480 GPU floorplan, \\n295, 295–296\\nGPU conditional branching, 302\\nGPU vs.', ' MIMD, 329\\nGPU programming, 289–290\\nGPUs vs.', ' GPU, 312\\nmultiprocessor architecture, 346\\nNVIDIA GPU computational \\n\\nstructures, 291\\nNVIDIA GPU Memory structures, \\n\\n304–305\\nprocessor comparisons, 324\\nRoofline model, 287, 326\\nsystem area network history, F-100\\n\\nSIMD Thread\\n\\nGPU conditional branching, \\n301–302\\nGrid mapping, 293\\nMultithreaded SIMD processor, \\n\\n294\\n\\nNVIDIA GPU, 296\\nNVIDIA GPU ISA, 298\\nNVIDIA GPU Memory structures, \\n\\n305\\nscheduling example, 297\\nvector vs.', ' GPU, 308\\nvector processor, 310\\nSIMD Thread Scheduler\\ndefinition, 292, 314\\nexample, 297\\nFermi GPU, 295, 305–307, 306\\nGPU, 296\\n\\nSIMT (Single Instruction, Multiple \\nThread)\\n\\nGPU programming, 289\\nvs.', ' cache size, \\nB-33\\n\\nSingle-precision floating point\\narithmetic, J-33 to J-34\\nGPU examples, 325\\nGPU vs.', ' server GPUs, 323–324\\n\\nSmart switches, vs.', ' TCP/IP reliance, F-95\\nVirtual Machines protection, 108\\nWSC running service, 434–435\\nSolaris, RAID benchmarks, D-22, \\n\\nD-22 to D-23\\n\\nSolid-state disks (SSDs)\\n\\nprocessor performance/price/\\npower, 52\\nserver energy efficiency, 462\\nWSC cost-performance, 474–475\\nSonic Smart Interconnect, OCNs, F-3\\nSony PlayStation 2\\n\\nblock diagram, E-16\\nembedded multiprocessors, E-14\\nEmotion Engine case study, E-15 \\n\\nto E-18\\nEmotion Engine organization, \\n\\nE-18\\nSorting, case study, D-64 to D-67\\nSort primitive, GPU vs.', ' software, 221–222\\nIA-64, H-38 to H-40\\nILP studies, L-32 to L-33\\nIntel Core i7, 123–124\\nlatency hiding in consistency \\n\\nmodels, 396–397\\n\\nmemory reference, hardware \\n\\nmultithreading-based \\nperformance, 398\\n\\nSun T1 multithreading unicore \\n\\nperformance, 227, 229\\n\\nAmdahl’s law, 46–47\\nfloating-point addition, J-25 to \\n\\nJ-26\\ninteger addition\\n\\ncarry-lookahead, J-37 to J-41\\ncarry-lookahead circuit, J-38\\ncarry-lookahead tree, J-40 to \\n\\ncarry-lookahead tree adder, \\n\\nJ-41\\n\\nJ-41\\n\\nJ-42\\n\\noverview, J-37\\n\\ninteger division\\n\\nradix-2 division, J-55\\nradix-4 division, J-56\\nradix-4 SRT division, J-57\\nwith single adder, J-54 to J-58\\n\\ninteger multiplication\\n\\narray multiplier, J-50\\nBooth recoding, J-49\\neven/odd array, J-52\\nwith many adders, J-50 to J-54\\nmultipass array multiplier, \\n\\nsigned-digit addition table, \\n\\nJ-51\\n\\nJ-54\\n\\ncarry-select adder, J-43, J-43 to \\n\\nSRT division\\n\\nJ-44, J-44\\n\\ncarry-skip adder, J-41 to J43, \\n\\nIndex\\n\\n■\\n\\nI-71\\n\\nvia coherence, 389–390\\nlarge-scale multiprocessor \\nsynchronization\\nbarrier synchronization, I-16\\nexponential back-off, I-17\\n\\non superscalar \\nprocessors, 230\\n\\nSplit, GPU vs.', ' datacenter costs, 455\\nWSCs, 442–443\\n\\nDAXPY on VMIPS, G-20\\ndefinition, 292\\nmultidimensional arrays, 278\\nThread Block comparison, 294\\nvector-length registers, 274\\n\\nStore conditional\\n\\nStrip mining\\n\\nlocks via coherence, 391\\nsynchronization, 388–389\\n\\nStore-and-forward packet switching, \\n\\nStore instructions, see also Load-store \\n\\nF-51\\n\\ninstruction set \\narchitecture\\n\\ndefinition, C-4\\ninstruction execution, 186\\nISA, 11, A-3\\nMIPS, A-33, A-36\\nNVIDIA GPU ISA, 298\\nOpteron data cache, B-15\\nRISC instruction set, C-4 to C-6, \\n\\nC-10\\nvector architectures, 310\\n\\nStreaming Multiprocessor\\n\\ndefinition, 292, 313–314\\nFermi GPU, 307\\nStrecker, William, K-65\\nStrided accesses\\n\\nMultimedia SIMD Extensions, 283\\nRoofline model, 287\\nTLB interaction, 323\\n\\nStrided addressing, see also Unit stride \\naddressing\\nmultimedia instruction compiler \\nsupport, A-31 to A-32\\n\\nStrides\\n\\ngather-scatter, 280\\nhighly parallel memory systems, \\n\\nmultidimensional arrays in vector \\narchitectures, 278–279\\n\\nNVIDIA GPU ISA, 300\\nvector memory systems, G-10 to \\n\\n133\\n\\nG-11\\n\\nK-53\\n\\nDAXPY on VMIPS, G-20\\nGPU conditional branching, 303\\nGPUs vs.', ' vector architectures, 311\\nNVIDIA GPU, 291\\nvector, 275\\nVLRs, 274–275\\n\\nStrong scaling, Amdahl’s law and \\n\\nparallel computers, 407\\n\\nStructural hazards\\n\\nbasic considerations, C-13 to C-16\\ndefinition, C-11\\nMIPS pipeline, C-71\\nMIPS scoreboarding, C-78 to C-79\\npipeline stall, C-15\\nvector execution time, 268–269\\n\\nStructural stalls, MIPS R4000 \\n\\npipeline, C-68 to C-69\\nSubset property, and inclusion, 397\\nSummary overflow condition code, \\n\\nPowerPC, K-10 to K-11\\n\\nSun Microsystems\\n\\ncache optimization, B-38\\nfault detection pitfalls, 58\\nmemory dependability, 104\\nSun Microsystems Enterprise, L-60\\nSun Microsystems Niagara (T1/T2) \\n\\ncharacteristics, 227\\nCPI and IPC, 399\\nfine-grained multithreading, 224, \\n225, 226–229\\n\\nmanufacturing cost, 62\\nmulticore processor performance, \\n\\n400–401\\n\\nmultiprocessing/\\n\\nmultithreading-based \\nperformance, 398–400\\n\\nIndex\\n\\n■\\n\\nI-73\\n\\nconditional branches, K-10, \\nK-17\\n\\nconditional instructions, H-27\\nconstant extension, K-9\\nconventions, K-13\\ndata transfer instructions, K-10\\nfast traps, K-30\\nfeatures, K-44\\nFP instructions, K-23\\ninstruction list, K-31 to K-32\\ninteger arithmetic, J-12\\ninteger overflow, J-11\\nISA, A-2\\nLISP, K-30\\nMIPS core extensions, K-22 to K-23\\noverlapped integer/FP operations, \\n\\nK-31\\nprecise exceptions, C-60\\nregister windows, K-29 to K-30\\nRISC history, L-20\\nas RISC system, K-4\\nSmalltalk, K-30\\nsynchronization history, L-64\\nunique instructions, K-29 to K-32\\nSun Microsystems SPARCCenter, L-60\\nSun Microsystems SPARCstation-2, \\n\\nF-88\\n\\nSun Microsystems SPARCstation-20, \\n\\nF-88\\nSun Microsystems SPARC V8, \\n\\nfloating-point \\nprecisions, J-33\\n\\ncharacteristics, K-18\\nmultimedia support, E-11, K-18\\n\\nSun Microsystems Ultra 5, \\n\\nSPECfp2000 execution \\ntimes, 43\\nSun Microsystems UltraSPARC, L-62, \\n\\nL-73\\n\\nSun Microsystems UltraSPARC T1 \\n\\nprocessor, \\ncharacteristics, F-73\\n\\nVMIPS, 266\\n\\nString operations, Intel 80x86, K-51, \\n\\nmultithreading history, L-34\\nT1 multithreading unicore \\n\\nSun Modular Datacenter, L-74 to L-75\\nSuperblock scheduling\\n\\nStripe, disk array deconstruction, D-51\\nStriping\\n\\ndisk arrays, D-6\\nRAID, D-9\\n\\nStrip-Mined Vector Loop\\n\\nconvoys, G-5\\n\\nperformance, 227–229\\n\\nSun Microsystems SPARC\\naddressing modes, K-5\\nALU operands, A-6\\narithmetic/logical instructions, \\nK-11, K-31\\n\\nbranch conditions, A-19\\n\\nbasic process, H-21 to H-23\\ncompiler history, L-31\\nexample, H-22\\n\\nSupercomputers\\n\\ncommercial interconnection \\n\\nnetworks, F-63\\n\\ndirect network topology, F-37\\n\\nprocessors\\n\\nSun Microsystems SPARC VIS\\n\\n\\x0csubstitution logic, 251\\n\\nSwitching\\n\\nI-74 ■\\n\\nIndex\\n\\nSupercomputers (continued)\\n\\nlow-dimensional topologies, F-100\\nSAN characteristics, F-76\\nSIMD, development, L-43 to L-44\\nvs.', ' NIC, F-85 to F-86, F-86\\nprocess switch, 224, B-37, B-49 to \\n\\nB-50\\n\\nstorage systems, D-34\\nswitched-media networks, F-24\\nWSC hierarchy, 441–442, 442\\nWSC infrastructure, 446\\nWSC network bottleneck, 461\\n\\nSwitch fabric, switched-media \\n\\nnetworks, F-24\\n\\ncommercial interconnection \\n\\nnetworks, F-56\\n\\ninterconnection networks, F-22, \\n\\nF-27, F-50 to F-52\\nnetwork impact, F-52 to F-55\\nperformance considerations, F-92 \\n\\nto F-93\\nSAN characteristics, F-76\\nswitched-media networks, F-24\\nsystem area network history, F-100\\n\\nSwitch microarchitecture\\n\\nbasic microarchitecture, F-55 to \\n\\nF-58\\n\\nbuffer organizations, F-58 to F-60\\nenhancements, F-62\\nHOL blocking, F-59\\ninput-output-buffered switch, F-57\\npipelining, F-60 to F-61, F-61\\n\\nSwitch ports\\n\\ncentralized switched networks, F-30\\ninterconnection network topology, \\n\\nF-29\\nSwitch statements\\n\\ncontrol flow instruction addressing \\nmodes, A-18\\n\\nGPU, 301\\n\\nSyllable, IA-64, H-35\\nSymbolic loop unrolling, software \\npipelining, H-12 to \\nH-15, H-13\\n\\nSymmetric multiprocessors (SMP)\\n\\ncharacteristics, I-45\\ncommunication calculations, 350\\ndirectory-based cache coherence, \\n\\n354\\n\\nfirst vector computers, L-47, L-49\\nlimitations, 363–364\\nsnooping coherence protocols, \\n354–355\\n\\nsystem area network history, F-101\\nTLP, 345\\n\\nSymmetric shared-memory \\n\\nmultiprocessors, see \\nalso Centralized \\nshared-memory \\nmultiprocessors\\n\\ndata caching, 351–352\\nlimitations, 363–364\\nperformance\\n\\ncommercial workload, 367–369\\ncommercial workload \\n\\nmeasurement, 369–374\\n\\nmultiprogramming and OS \\nworkload, 374–378\\n\\noverview, 366–367\\n\\nscientific workloads, I-21 to I-26, \\nI-23 to I-25\\n\\nSynapse N + 1, L-59\\nSynchronization\\n\\nAltaVista search, 369\\nbasic considerations, 386–387\\nbasic hardware primitives, \\n\\n387–389\\nconsistency models, 395–396\\ncost, 403\\nCray X1, G-23\\ndefinition, 375\\nGPU comparisons, 329\\nGPU conditional branching, \\n300–303\\nhistorical background, L-64\\nlarge-scale multiprocessors\\n\\nbarrier synchronization, I-13 to \\nI-16, I-14, I-16\\nchallenges, I-12 to I-16\\nhardware primitives, I-18 to \\n\\nsense-reversing barrier, I-21\\nsoftware implementations, I-17 \\n\\nI-21\\n\\nto I-18\\n\\ntree-based barriers, I-19\\nlocks via coherence, 389–391\\n\\n\\x0cmessage-passing communication, \\n\\nI-5\\n\\nMIMD, 10\\nMIPS core extensions, K-21\\nprogrammer’s viewpoint, 393–394\\nPTX instruction set, 298–299\\nrelaxed consistency models, \\n394–395\\nsingle-chip multicore processor \\ncase study, 412–418\\n\\nvector vs.', ' GPU, 311\\nVLIW, 196\\nWSCs, 434\\n\\nSynchronous dynamic random-access \\n\\nmemory (SDRAM)\\n\\nARM Cortex-A8, 117\\nDRAM, 99\\nvs.', ' Flash memory, 103\\nIBM Blue Gene/L, I-42\\nIntel Core i7, 121\\nperformance, 100\\npower consumption, 102, 103\\nSDRAM timing diagram, 139\\n\\nSynchronous event, exception \\n\\nrequirements, C-44 to \\nC-45\\nSynchronous I/O, definition, D-35\\nSynonyms\\n\\naddress translation, B-38\\ndependability, 34\\nSynthetic benchmarks\\ndefinition, 37\\ntypical program fallacy, A-43\\nSystem area networks, historical \\noverview, F-100 to \\nF-102\\n\\nSystem calls\\n\\nCUDA Thread, 297\\nmultiprogrammed workload, 378\\nvirtualization/paravirtualization \\n\\nperformance, 141\\n\\nvirtual memory protection, 106\\nSystem interface controller (SIF), Intel \\nSCCC, F-70\\n\\nSystem-on-chip (SoC)\\ncell phone, E-24\\ncross-company interoperability, \\n\\nF-64\\nembedded systems, E-3\\nSanyo digital cameras, E-20\\nSanyo VPC-SX500 digital camera, \\n\\nE-19\\n\\nshared-media networks, F-23\\nSystem Performance and Evaluation \\nCooperative (SPEC), \\nsee SPEC benchmarks\\n\\nSystem Processor\\ndefinition, 309\\nDLP, 262, 322\\nFermi GPU, 306\\nGPU issues, 330\\nGPU programming, 288–289\\nNVIDIA GPU ISA, 298\\nNVIDIA GPU Memory, 305\\nprocessor comparisons, 323–324\\nsynchronization, 329\\nvector vs.', ' GPU, 311–312\\n\\nSystem response time, transactions, \\n\\nD-16, D-17\\nSystems on a chip (SOC), cost trends, \\n\\n28\\n\\nSystem/storage area networks (SANs)\\n\\ncharacteristics, F-3 to F-4\\ncommunication protocols, F-8\\ncongestion management, F-65\\ncross-company interoperability, F-64\\neffective bandwidth, F-18\\nexample system, F-72 to F-74\\nfat trees, F-34\\nfault tolerance, F-67\\nInfiniBand example, F-74 to F-77\\ninterconnection network domain \\n\\nrelationship, F-4\\n\\nLAN history, F-99\\nlatency and effective bandwidth, \\nF-26 to F-28\\nlatency vs.', ' nodes, F-27\\npacket latency, F-13, F-14 to F-16\\nrouting algorithms, F-48\\nsoftware overhead, F-91\\nTCP/IP reliance, F-95\\ntime of flight, F-13\\ntopology, F-30\\n\\nT\\nTag\\n\\nAMD Opteron data cache, B-12 to \\n\\nB-14\\n\\nARM Cortex-A8, 115\\ncache optimization, 79–80\\ndynamic scheduling, 177\\ninvalidate protocols, 357\\n\\nIndex\\n\\n■\\n\\nI-75\\n\\nmemory hierarchy basics, 74\\nmemory hierarchy basics, 77–78\\nvirtual memory fast address \\ntranslation, B-46\\n\\nwrite strategy, B-10\\n\\nTag check (TC)\\n\\nMIPS R4000, C-63\\nR4000 pipeline, B-62 to B-63\\nR4000 pipeline structure, C-63\\nwrite process, B-10\\n\\nTag fields\\n\\nblock identification, B-8\\ndynamic scheduling, 173, 175\\n\\nTail duplication, superblock \\n\\nscheduling, H-21\\n\\nTailgating, definition, G-20\\nTandem Computers\\n\\ncluster history, L-62, L-72\\nfaults, D-14\\noverview, D-12 to D-13\\n\\nTarget address\\n\\nbranch hazards, C-21, C-42\\nbranch penalty reduction, C-22 to \\n\\nC-23\\nbranch-target buffer, 206\\ncontrol flow instructions, A-17 to \\n\\nA-18\\n\\nGPU conditional branching, 301\\nIntel Core i7 branch predictor, 166\\nMIPS control flow instructions, \\n\\nA-38\\nMIPS implementation, C-32\\nMIPS pipeline, C-36, C-37\\nMIPS R4000, C-25\\npipeline branches, C-39\\nRISC instruction set, C-5\\nTarget channel adapters (TCAs), \\n\\nswitch vs.', ' NIC, F-86\\n\\nTarget instructions\\n\\nbranch delay slot scheduling, C-24\\nas branch-target buffer variation, \\n\\nGPU conditional branching, 301\\n\\nTask-level parallelism (TLP), \\n\\ndefinition, 9\\n\\nTB, see Translation buffer (TB)\\nTB-80 VME rack\\nexample, D-38\\nMTTF calculation, D-40 to D-41\\n\\nTC, see Tag check (TC)\\nTCAs, see Target channel adapters \\n(TCAs)\\n\\nSystem Virtual Machines, definition, \\n\\n206\\n\\n107\\n\\n\\x0cI-76 ■\\n\\nIndex\\n\\nTCO, see Total Cost of Ownership \\n(TCO)\\n\\nTCP, see Transmission Control \\n\\nProtocol (TCP)\\n\\nTCP/IP, see Transmission Control \\n\\nProtocol/Internet \\nProtocol (TCP/IP)\\n\\nTDMA, see Time division multiple \\n\\naccess (TDMA)\\n\\nTDP, see Thermal design power \\n(TDP)\\n\\nTechnology trends\\n\\nbasic considerations, 17–18\\nperformance, 18–19\\n\\nTeleconferencing, multimedia support, \\n\\nK-17\\n\\nTemporal locality\\n\\nblocking, 89–90\\ncache optimization, B-26\\ncoining of term, L-11\\ndefinition, 45, B-2\\nmemory hierarchy design, 72\\n\\nTERA processor, L-34\\nTerminate events\\n\\nexceptions, C-45 to C-46\\nhardware-based speculation, 188\\nloop unrolling, 161\\n\\nTertiary Disk project\\n\\nfailure statistics, D-13\\noverview, D-12\\nsystem log, D-43\\nTest-and-set operation, \\n\\nsynchronization, 388\\n\\nTexas Instruments 8847\\n\\narithmetic functions, J-58 to J-61\\nchip comparison, J-58\\nchip layout, J-59\\nTexas Instruments ASC\\n\\nfirst vector computers, L-44\\npeak performance vs.', ' start-up \\n\\noverhead, 331\\n\\nL-57 to L-58\\n\\nTFT, see Thin-film transistor (TFT)\\nThacker, Chuck, F-99\\nThermal design power (TDP), power \\ntrends, 22\\n\\nThin-film transistor (TFT), Sanyo \\nVPC-SX500 digital \\ncamera, E-19\\n\\nThinking Machines, L-44, L-56\\nThinking Multiprocessors CM-5, L-60\\n\\nThink time, transactions, D-16, D-17\\nThird-level caches, see also L3 caches\\n\\nILP, 245\\ninterconnection network, F-87\\nSRAM, 98–99\\n\\nThrash, memory hierarchy, B-25\\nThread Block\\n\\nCUDA Threads, 297, 300, 303\\ndefinition, 292, 313\\nFermi GTX 480 GPU flooplan, \\n\\n295\\nfunction, 294\\nGPU hardware levels, 296\\nGPU Memory performance, 332\\nGPU programming, 289–290\\nGrid mapping, 293\\nmapping example, 293\\nmultithreaded SIMD Processor, 294\\nNVIDIA GPU computational \\n\\nstructures, 291\\nNVIDIA GPU Memory structures, \\n\\n304\\n\\nPTX Instructions, 298\\n\\nThread Block Scheduler\\n\\ndefinition, 292, 309, 313–314\\nFermi GTX 480 GPU flooplan, 295\\nfunction, 294, 311\\nGPU, 296\\nGrid mapping, 293\\nmultithreaded SIMD Processor, 294\\n\\nThread-level parallelism (TLP)\\n\\nadvanced directory protocol case \\n\\nstudy, 420–426\\nAmdahl’s law and parallel \\n\\ncomputers, 406–407\\n\\ncentralized shared-memory \\n\\nmultiprocessors\\nbasic considerations, 351–352\\ncache coherence, 352–353\\ncache coherence enforcement, \\n\\n354–355\\n\\n357–362\\n\\n362–363\\n\\ncache coherence extensions, \\n\\ninvalidate protocol \\n\\nimplementation, \\n356–357\\n\\nSMP and snooping limitations, \\n\\n363–364\\n\\nsnooping coherence \\n\\nimplementation, 365–366\\n\\nTFLOPS, parallel processing debates, \\n\\ncache coherence example, \\n\\nsnooping coherence protocols, \\n\\n355–356\\n\\ndefinition, 9\\ndirectory-based cache coherence\\n\\ncase study, 418–420\\nprotocol basics, 380–382\\nprotocol example, 382–386\\n\\nDSM and directory-based \\n\\ncoherence, 378–380\\n\\nembedded systems, E-15\\nIBM Power7, 215\\nfrom ILP, 4–5\\ninclusion, 397–398\\nIntel Core i7 performance/energy \\n\\nefficiency, 401–405\\n\\nmemory consistency models\\n\\nbasic considerations, 392–393\\ncompiler optimization, 396\\nprogramming viewpoint, \\n\\nrelaxed consistency models, \\n\\n393–394\\n\\n394–395\\n\\nspeculation to hide latency, \\n\\n396–397\\nMIMDs, 344–345\\nmulticore processor performance, \\n\\n400–401\\nmulticore processors and SMT, \\n404–405\\n\\nmultiprocessing/\\n\\nmultithreading-based \\nperformance, 398–400\\n\\nmultiprocessor architecture, \\n346–348\\n\\nmultiprocessor cost effectiveness, 407\\nmultiprocessor performance, \\n405–406\\n\\nmultiprocessor software \\n\\ndevelopment, 407–409\\n\\nvs.', ' multithreading, 223–224\\nmultithreading history, L-34 to L-35\\nparallel processing challenges, \\n349–351\\nsingle-chip multicore processor \\ncase study, 412–418\\n\\nSun T1 multithreading, 226–229\\nsymmetric shared-memory \\n\\nmultiprocessor \\nperformance\\n\\ncommercial workload, 367–369\\ncommercial workload \\n\\nmeasurement, 369–374\\n\\n\\x0cmultiprogramming and OS \\nworkload, 374–378\\n\\noverview, 366–367\\n\\nsynchronization\\n\\nbasic considerations, 386–387\\nbasic hardware primitives, \\n\\n387–389\\n\\nlocks via coherence, 389–391\\n\\nThread Processor\\n\\ndefinition, 292, 314\\nGPU, 315\\n\\nThread Processor Registers, definition, \\n\\n292\\n\\nThread Scheduler in a Multithreaded \\n\\nCPU, definition, 292\\n\\nThread of SIMD Instructions\\ncharacteristics, 295–296\\nCUDA Thread, 303\\ndefinition, 292, 313\\nGrid mapping, 293\\nlane recognition, 300\\nscheduling example, 297\\nterminology comparison, 314\\nvector/GPU comparison, 308–309\\n\\nThread of Vector Instructions, \\n\\ndefinition, 292\\n\\nThree-dimensional space, direct \\n\\nnetworks, F-38\\n\\nThree-level cache hierarchy\\n\\ncommercial workloads, 368\\nILP, 245\\nIntel Core i7, 118, 118\\n\\nThrottling, packets, F-10\\nThroughput, see also Bandwidth\\n\\ndefinition, C-3, F-13\\ndisk storage, D-4\\nGoogle WSC, 470\\nILP, 245\\ninstruction fetch bandwidth, 202\\nIntel Core i7, 236–237\\nkernel characteristics, 327\\nmemory banks, 276\\nmultiple lanes, 271\\nparallelism, 44\\nperformance considerations, 36\\nperformance trends, 18–19\\npipelining basics, C-10\\nprecise exceptions, C-60\\nproducer-server model, D-16\\nvs.', ' sequential programs, \\n\\nUtilization\\n\\ncache protocols, 359\\ndevelopment views, 344\\nlinear speedups, 407\\nmemory hierarchy design, 73\\nmemory system coherency, 353, \\n\\n358\\n\\nmisses, 371, 373\\nmultiprogramming workload, \\n376–377\\n\\nmultithreading\\n\\nbasic considerations, 223–226\\nfine-grained on T1, 226–229\\nsimultaneous, on superscalars, \\n\\n230–232\\n\\n405–406\\n\\n344\\n\\nprocessor performance trends, 3–4, \\n\\nSISD, 10\\nsoftware development, 407–408\\n\\nUnit stride addressing\\ngather-scatter, 280\\nGPU vs.', ' MIMD with Multimedia \\nSIMD, 327\\nGPUs vs.', ' vector architectures, 310\\nmultimedia instruction compiler \\nsupport, A-31\\nNVIDIA GPU ISA, 300\\nRoofline model, 287\\n\\nUNIVAC I, L-5\\nUNIX systems\\n\\narchitecture costs, 2\\nblock servers vs.', ' filers, D-35\\ncache optimization, B-38\\nfloating point remainder, J-32\\nmiss statistics, B-59\\nmultiprocessor software \\n\\naddress translation, B-46\\nsegmented virtual memory, B-52\\nvirtual memory block replacement, \\n\\nB-45\\n\\nF-8\\n\\nUser-level communication, definition, \\n\\nUser maskable events, definition, C-45 \\n\\nto C-46\\nUser nonmaskable events, definition, \\n\\nC-45\\nUser-requested events, exception \\nrequirements, C-45\\n\\nUtility computing, 455–461, L-73 to \\n\\nL-74\\n\\nI/O system calculations, D-26\\nqueuing theory, D-25\\n\\nUTP, see Unshielded twisted pair \\n(UTP)\\n\\nV\\nValid bit\\n\\naddress translation, B-46\\nblock identification, B-7\\nOpteron data cache, B-14\\npaged virtual memory, B-56\\nsegmented virtual memory, B-52\\nsnooping, 357\\nsymmetric shared-memory \\n\\nmultiprocessors, 366\\n\\nValue prediction\\n\\ndefinition, 202\\nhardware-based speculation, 192\\nILP, 212–213, 220\\nspeculation, 208\\nVAPI, InfiniBand, F-77\\nVariable length encoding\\n\\ndevelopment, 408\\n\\ncontrol flow instruction branches, \\n\\nmultiprogramming workload, 374\\nseek distance comparison, D-47\\nvector processor history, G-26\\n\\nUnpacked decimal, A-14, J-16\\nUnshielded twisted pair (UTP), LAN \\n\\nA-18\\n\\ninstruction sets, A-22\\nISAs, 14\\n\\nVariables\\n\\nhistory, F-99\\n\\nA-29\\n\\nand compiler technology, A-27 to \\n\\n\\x0cVariables (continued)\\n\\nvector-register characteristics, G-3\\n\\nI-80 ■\\n\\nIndex\\n\\nCUDA, 289\\nFermi GPU, 306\\nISA, A-5, A-12\\nlocks via coherence, 389\\nloop-level parallelism, 316\\nmemory consistency, 392\\nNVIDIA GPU Memory, 304–305\\nprocedure invocation options, \\n\\nA-19\\n\\nrandom, distribution, D-26 to D-34\\nregister allocation, A-26 to A-27\\nin registers, A-5\\nsynchronization, 375\\nTLP programmer’s viewpoint, 394\\n\\nVCs, see Virtual channels (VCs)\\nVector architectures\\n\\ncomputer development, L-44 to L-49\\ndefinition, 9\\nDLP\\n\\nbasic considerations, 264\\ndefinition terms, 309\\ngather/scatter operations, \\n\\n279–280\\n\\nmultidimensional arrays, \\n\\n278–279\\nmultiple lanes, 271–273\\nprogramming, 280–282\\nvector execution time, 268–271\\nvector-length registers, \\n274–275\\nvector load/store unit \\n\\nbandwidth, 276–277\\n\\nvector-mask registers, 275–276\\nvector processor example, \\n\\n267–268\\n\\nVMIPS, 264–267\\n\\nGPU conditional branching, 303\\nvs.', ' GPUs, 308–312\\nmapping examples, 293\\nmemory systems, G-9 to G-11\\nmultimedia instruction compiler \\nsupport, A-31\\nvs.', ' scalar performance, 331–332\\nstart-up latency and dead time, G-8\\nstrided access-TLB interactions, \\n\\n282\\n\\n323\\n\\nVector Functional Unit\\n\\nvector add instruction, 272–273\\nvector execution time, 269\\nvector sequence chimes, 270\\nVMIPS, 264\\nVector Instruction\\n\\ndefinition, 292, 309\\nDLP, 322\\nFermi GPU, 305\\ngather-scatter, 280\\ninstruction-level parallelism, 150\\nmask registers, 275–276\\nMultimedia SIMD Extensions, 282\\nmultiple lanes, 271–273\\nThread of Vector Instructions, 292\\nvector execution time, 269\\nvector vs.', ' GPU, 308, 311\\nvector processor example, 268\\nVMIPS, 265–267, 266\\n\\nVectorizable Loop\\n\\ncharacteristics, 268\\ndefinition, 268, 292, 313\\nGrid mapping, 293\\nLivermore Fortran kernel \\n\\nperformance, 331\\n\\nmapping example, 293\\nNVIDIA GPU computational \\n\\nstructures, 291\\n\\nVectorized code\\n\\nmultimedia compiler support, A-31\\nvector architecture programming, \\n\\n280–282\\n\\nvector execution time, 271\\nVMIPS, 268\\n\\nVectorized Loop, see also Body of \\n\\nVectorized Loop\\n\\ndefinition, 309\\nGPU Memory structure, 304\\nvs.', ' Grid, 291, 308\\nmask registers, 275\\nNVIDIA GPU, 295\\nvector vs.', ' GPU, 308\\n\\nVectorizing compilers\\n\\neffectiveness, G-14 to G-15\\nFORTRAN test kernels, G-15\\nsparse matrices, G-12 to G-13\\nVector Lane Registers, definition, 292\\nVector Lanes\\n\\ncontrol processor, 311\\ndefinition, 292, 309\\nSIMD Processor, 296–297, 297\\n\\nVector-length register (VLR)\\nbasic operation, 274–275\\nperformance, G-5\\nVMIPS, 267\\nVector load/store unit\\n\\nmemory banks, 276–277\\nVMIPS, 265\\n\\nVector loops\\n\\nNVIDIA GPU, 294\\nprocessor example, 267\\nstrip-mining, 303\\nvector vs.', ' GPU, 311\\nvector-length registers, 274–275\\nvector-mask registers, 275–276\\nVector-mask control, characteristics, \\n275–276\\nVector-mask registers\\n\\nbasic operation, 275–276\\nCray X1, G-21 to G-22\\nVMIPS, 267\\nVector Processor\\ncaches, 305\\ncompiler vectorization, 281\\nCray X1\\n\\nMSP modules, G-22\\noverview, G-21 to G-23\\n\\nCray X1E, G-24\\ndefinition, 292, 309\\nDLP processors, 322\\nDSP media extensions, E-10\\nexample, 267–268\\nexecution time, G-7\\nfunctional units, 272\\ngather-scatter, 280\\nvs.', ' GPUs, 276\\nhistorical background, G-26\\nloop-level parallelism, 150\\nloop unrolling, 196\\nmeasures, G-15 to G-16\\nmemory banks, 277\\nand multiple lanes, 273, 310\\nmultiprocessor architecture, 346\\nNVIDIA GPU computational \\n\\nstructures, 291\\n\\noverview, G-25 to G-26\\npeak performance focus, 331\\nperformance, G-2 to G-7\\n\\nstart-up and multiple lanes, G-7 \\n\\nto G-9\\nperformance comparison, 58\\nperformance enhancement\\nchaining, G-11 to G-12\\n\\n\\x0cDAXPY on VMIPS, G-19 to \\n\\nsparse matrices, G-12 to G-14\\n\\nPTX, 301\\nRoofline model, 286–287, 287\\nvs.', ' SIMD Processor, 294–296\\nSony PlayStation 2 Emotion \\n\\nEngine, E-17 to E-18\\n\\nstart-up overhead, G-4\\nstride, 278\\nstrip mining, 275\\nvector execution time, 269–271\\nvector/GPU comparison, 308\\nvector kernel implementation, \\n334–336\\n\\nVMIPS, 264–265\\nVMIPS on DAXPY, G-17\\nVMIPS on Linpack, G-17 to G-19\\n\\ndefinition, 309\\nexecution time, 269, 271\\ngather-scatter, 280\\nmultimedia compiler support, A-31\\nMultimedia SIMD Extensions, 282\\nmultiple lanes, 271–273\\nNVIDIA GPU, 297\\nNVIDIA GPU ISA, 298\\nperformance/bandwidth trade-offs, \\n\\n332\\nprocessor example, 267\\nstrides, 278–279\\nvector vs.', ' GPU, 308, 311\\nVMIPS, 264–267, 266\\n\\nVector Registers\\n\\nVery-large-scale integration (VLSI)\\nearly computer arithmetic, J-63\\ninterconnection network topology, \\n\\nF-29\\n\\nRISC history, L-20\\nWallace tree, J-53\\n\\nVery Long Instruction Word (VLIW)\\n\\nclock rates, 244\\ncompiler scheduling, L-31\\nEPIC, L-32\\nIA-64, H-33 to H-34\\nILP, 193–196\\nloop-level parallelism, 315\\nM32R, K-39 to K-40\\nmultiple-issue processors, 194, \\nL-28 to L-30\\n\\nmultithreading history, L-34\\n\\nsample code, 252\\nTI 320C6x DSP, E-8 to E-10\\n\\nVGA controller, L-51\\nVideo\\n\\nAmazon Web Services, 460\\napplication trends, 4\\nPMDs, 6\\nWSCs, 8, 432, 437, 439\\n\\nVideo games, multimedia support, \\nK-17\\nVI interface, L-73\\nVirtual address\\n\\naddress translation, B-46\\nAMD64 paged virtual memory, B-55\\nAMD Opteron data cache, B-12 to \\n\\nB-13\\n\\nARM Cortex-A8, 115\\ncache optimization, B-36 to B-39\\nGPU conditional branching, 303\\nIntel Core i7, 120\\nmapping to physical, B-45\\nmemory hierarchy, B-39, B-48, \\nB-48 to B-49\\nmemory hierarchy basics, 77–78\\nmiss rate vs.', ' cache size, B-37\\nOpteron mapping, B-55\\nOpteron memory management, \\n\\nB-55 to B-56\\n\\nand page size, B-58\\npage table-based mapping, B-45\\ntranslation, B-36 to B-39\\nvirtual memory, B-42, B-49\\n\\nVirtual address space\\nexample, B-41\\nmain memory block, B-44\\n\\nVirtual caches\\n\\ndefinition, B-36 to B-37\\nissues with, B-38\\n\\nVirtual channels (VCs), F-47\\nHOL blocking, F-59\\nIntel SCCC, F-70\\nrouting comparison, F-54\\nswitching, F-51 to F-52\\nswitch microarchitecture \\n\\npipelining, F-61\\nsystem area network history, F-101\\nand throughput, F-93\\n\\nVirtual cut-through switching, F-51\\nVirtual functions, control flow \\ninstructions, A-18\\n\\nVirtualizable architecture\\nIntel 80x86 issues, 128\\n\\nIndex\\n\\n■\\n\\nI-81\\n\\nsystem call performance, 141\\nVirtual Machines support, 109\\nVMM implementation, 128–129\\n\\nVirtualizable GPUs, future \\n\\ntechnology, 333\\n\\nVirtual machine monitor (VMM)\\n\\ncharacteristics, 108\\nnonvirtualizable ISA, 126, \\n\\n128–129\\n\\nrequirements, 108–109\\nVirtual Machines ISA support, \\n109–110\\n\\nXen VM, 111\\n\\nVirtual Machines (VMs)\\n\\nAmazon Web Services, 456–457\\ncloud computing costs, 471\\nearly IBM work, L-10\\nISA support, 109–110\\nprotection, 107–108\\nprotection and ISA, 112\\nserver benchmarks, 40\\nand virtual memory and I/O, \\n110–111\\n\\nWSCs, 436\\nXen VM, 111\\n\\nVirtual memory\\n\\nbasic considerations, B-40 to B-44, \\nB-48 to B-49\\n\\nbasic questions, B-44 to B-46\\nblock identification, B-44 to B-45\\nblock placement, B-44\\nblock replacement, B-45\\nvs.', ' GPU, 308\\nvector-length registers, 274\\nvector load/store unit bandwidth, \\n\\n276\\n\\nvector performance measures, \\n\\nG-16\\nvector processor example, \\n\\n267–268\\n\\nVLR, 274\\n\\nVMM, see Virtual machine monitor \\n(VMM)\\nVMs, see Virtual Machines (VMs)\\nVoltage regulator controller (VRC), \\n\\nIntel SCCC, F-70\\n\\nVoltage regulator modules (VRMs), \\n\\nWSC server energy \\nefficiency, 462\\n\\nVolume-cost relationship, \\n\\ncomponents, 27–28\\n\\nVon Neumann, John, L-2 to L-6\\nVon Neumann computer, L-3\\nVoodoo2, L-51\\nVOQs, see Virtual output queues \\n\\n(VOQs)\\nVRC, see Voltage regulator controller \\n\\n(VRC)\\n\\nVRMs, see Voltage regulator modules \\n(VRMs)\\n\\nW\\nWafers\\n\\nexample, 31\\nintegrated circuit cost trends, \\n28–32\\n\\nWafer yield\\n\\nchip costs, 32\\ndefinition, 30\\n\\nWaiting line, definition, D-24\\nWait time, shared-media networks, \\n\\nF-23\\n\\nWallace tree\\n\\nexample, J-53, J-53\\nhistorical background, J-63\\n\\nWall-clock time\\n\\nexecution time, 36\\nscientific applications on parallel \\n\\nprocessors, I-33\\n\\nWANs, see Wide area networks \\n\\n(WANs)\\nWAR, see Write after read (WAR)\\nWarehouse-scale computers (WSCs)\\nAmazon Web Services, 456–461\\nbasic concept, 432\\ncharacteristics, 8\\ncloud computing, 455–461\\ncloud computing providers, \\n471–472\\ncluster history, L-72 to L-73\\ncomputer architecture\\narray switch, 443\\nbasic considerations, 441–442\\nmemory hierarchy, 443, \\n\\n443–446, 444\\n\\nstorage, 442–443\\nas computer class, 5\\ncomputer cluster forerunners, \\n435–436\\ncost-performance, 472–473\\ncosts, 452–455, 453–454\\n\\ndefinition, 345\\nand ECC memory, 473–474\\nefficiency measurement, 450–452\\nfacility capital costs, 472\\nFlash memory, 474–475\\nGoogle\\n\\ncontainers, 464–465\\ncooling and power, 465–468\\nmonitoring and repairing, \\n\\n469–470\\n\\nPUE, 468\\nserver, 467\\nservers, 468–469\\nMapReduce, 437–438\\nnetwork as bottleneck, 461\\nphysical infrastructure and costs, \\n\\n446–450\\npower modes, 472\\nprogramming models and \\n\\nworkloads, 436–441\\nquery response-time curve, 482\\nrelaxed consistency, 439\\nresource allocation, 478–479\\nserver energy efficiency, 462–464\\nvs.', ' ROB, 208\\nROB, 192\\nTomasulo’s advantages, 177–178\\n\\nWrite allocate\\n\\nAMD Opteron data cache, B-12\\ndefinition, B-11\\nexample calculation, B-12\\n\\nWrite-back cache\\n\\nAMD Opteron example, B-12, B-14\\ncoherence maintenance, 381\\ncoherency, 359\\ndefinition, B-11\\ndirectory-based cache coherence, \\n\\n383, 386\\nFlash memory, 474\\nFP register file, C-56\\ninvalidate protocols, 355–357, 360\\nmemory hierarchy basics, 75\\nsnooping coherence, 355, \\n\\n356–357, 359\\n\\nWorst-case execution time (WCET), \\n\\nWrite-back cycle (WB)\\n\\ndefinition, E-4\\n\\nWrite after read (WAR)\\n\\ndata hazards, 153–154, 169\\n\\nbasic MIPS pipeline, C-36\\ndata hazard stall minimization, \\n\\nC-17\\n\\nWindowing, congestion management, \\n\\nWormhole switching, F-51, F-88\\n\\n\\x0cI-84 ■\\n\\nIndex\\n\\nWrite-back cycle (continued )\\nexecution sequences, C-80\\nhazards and forwarding, C-55 to \\n\\nWrite merging\\nexample, 88\\nmiss penalty reduction, 87\\n\\nC-56\\n\\nWrite miss\\n\\nMIPS exceptions, C-49\\nMIPS pipeline, C-52\\nMIPS pipeline control, C-39\\nMIPS R4000, C-63, C-65\\nMIPS scoreboarding, C-74\\npipeline branch issues, C-40\\nRISC classic pipeline, C-7 to C-8, \\n\\nsimple MIPS implementation, \\n\\nC-10\\n\\nC-33\\n\\n356\\n\\nsimple RISC implementation, C-6\\nWrite broadcast protocol, definition, \\n\\nWrite buffer\\n\\nAMD Opteron data cache, B-14\\nIntel Core i7, 118, 121\\ninvalidate protocol, 356\\nmemory consistency, 393\\nmemory hierarchy basics, 75\\nmiss penalty reduction, 87, B-32, \\nB-35 to B-36\\n\\nwrite merging example, 88\\nwrite strategy, B-11\\n\\nWrite hit\\n\\ncache coherence, 358\\ndirectory-based coherence, 424\\nsingle-chip multicore \\n\\nmultiprocessor, 414\\n\\nsnooping coherence, 359\\nwrite process, B-11\\nWrite invalidate protocol\\n\\ndirectory-based cache coherence \\n\\nprotocol example, \\n382–383\\nexample, 359, 360\\nimplementation, 356–357\\nsnooping coherence, 355–356\\n\\nAMD Opteron data cache, B-12, \\n\\nB-14\\n\\ncache coherence, 358, 359, 360, 361\\ndefinition, 385\\ndirectory-based cache coherence, \\n\\n380–383, 385–386\\n\\nexample calculation, B-12\\nlocks via coherence, 390\\nmemory hierarchy basics, 76–77\\nmemory stall clock cycles, B-4\\nOpteron data cache, B-12, B-14\\nsnooping cache coherence, 365\\nwrite process, B-11 to B-12\\nwrite speed calculations, 393\\n\\ndata hazards, 154\\ndynamic scheduling, 174–175\\nhardware-based speculation, 192\\ninstruction steps, 175\\nROB instruction, 186\\nscoreboarding, C-74 to C-75, C-78 \\n\\nto C-80\\nstatus table examples, C-77\\nTomasulo’s algorithm, 178, 180, \\n\\nWrite result stage\\n\\n190\\nWrite serialization\\n\\nhardware primitives, 387\\nmultiprocessor cache coherency, \\n\\n353\\nsnooping coherence, 356\\nWrite stall, definition, B-11\\nWrite strategy\\n\\nmemory hierarchy considerations, \\n\\nB-6, B-10 to B-12\\nvirtual memory, B-45 to B-46\\n\\nWrite-through cache\\n\\naverage memory access time, B-16\\n\\ncoherency, 352\\ninvalidate protocol, 356\\nmemory hierarchy basics, 74–75\\nmiss penalties, B-32\\noptimization, B-35\\nsnooping coherence, 359\\nwrite process, B-11 to B-12\\n\\nWrite update protocol, definition, 356\\nWSCs, see Warehouse-scale \\n\\ncomputers (WSCs)\\n\\nX\\nXBox, L-51\\nXen Virtual Machine\\n\\nAmazon Web Services, 456–457\\ncharacteristics, 111\\n\\nXerox Palo Alto Research Center, \\n\\nLAN history, F-99\\n\\nXIMD architecture, L-34\\nXon/Xoff, interconnection networks, \\nF-10, F-17\\n\\nY\\nYahoo!, WSCs, 465\\nYield\\n\\nchip fabrication, 61–62\\ncost trends, 27–32\\nFermi GTX 480, 324\\n\\nZ\\nZ-80 microcontroller, cell phones, \\nE-24\\n\\nZero condition code, MIPS core, K-9 \\nto K-16\\n\\nZero-copy protocols\\ndefinition, F-8\\nmessage copying issues, F-91\\n\\nZero-load latency, Intel SCCC, \\nF-70\\n\\nZuse, Konrad, L-4 to L-5\\nZynga, FarmVille, 460\\n\\n\\x0cTranslation between GPU terms in book and official NVIDIA and OpenCL terms.', '\\n\\nMore Descriptive \\nName used in this \\nBook\\n\\nType\\n\\nOfficial CUDA/\\nNVIDIA Term\\n\\nVectorizable Loop\\n\\nGrid\\n\\nBody of \\nVectorized Loop\\n\\nThread Block\\n\\nSequence of\\nSIMD Lane Opera-\\ntions\\n\\nCUDA Thread\\n\\nA Thread of \\nSIMD \\nInstructions\\n\\nWarp\\n\\nBook Definition \\nand OpenCL Terms\\n\\nOfficial CUDA/NVIDIA\\nDefinition\\n\\nA vectorizable loop, executed on the GPU, made \\nup of 1 or more “Thread Blocks” (or bodies of \\nvectorized loop) that can execute in parallel.', ' \\n\\nSIMD Thread \\nScheduler\\n\\nWarp \\nScheduler\\n\\nSIMD \\nLane\\n\\nThread \\nProcessor\\n\\nGPU \\nMemory\\n\\n Private \\nMemory\\n\\nLocal \\nMemory\\n\\nGlobal \\nMemory\\n\\nLocal \\nMemory\\n\\nShared \\nMemory\\n\\nSIMD Lane \\nRegisters\\n\\nRegisters\\n\\nHardware unit that schedules and issues “Warps” \\n(threads of SIMD instructions) when they are \\nready to execute; includes a scoreboard to track \\n“Warp” (SIMD thread) execution.', '\\n\\nDRAM memory accessible by all “Streaming \\nMultiprocessors” (or multithreaded SIMD \\nprocessors) in a GPU.']\n",
            "['4> Some recent studies have defined a metric called TPUE,\\nwhich stands for “true PUE” or “total PUE.', '” TPUE is defined as PUE * SPUE.', ' Compare the PUE and TPUE in both these cases.', '  Can  you  identify  another  design  where  TPUE  is\\npotentially lower than PUE? (Hint: See Exercise 6.']\n",
            "[' Computer Architecture: A Quantitative Approach fur-\\nthers its string of firsts in presenting comprehensive architecture coverage of sig-\\nnificant new developments!”\\n\\n—John Nickolls, NVIDIA\\n\\n\\x0c“The  new  edition  of  this  now  classic  textbook  highlights  the  ascendance  of\\nexplicit parallelism (data, thread, request) by devoting a whole chapter to each\\ntype.', '  The\\n“Putting It All Together” sections of this edition include the pipeline organiza-\\ntions and memory hierarchies of the ARM Cortex A8 processor, the Intel core i7\\nprocessor,  the  NVIDIA GTX-280 and GTX-480 GPUs, and one of the Google\\nwarehouse-scale computers.', ' We decided to go\\nwith our own terms and then provide a translation between our terms and the offi-\\ncial NVIDIA terms.', ') This chapter introduces the Roofline performance model and then uses it\\nto compare the Intel Core i7 and the NVIDIA GTX 280 and GTX 480 GPUs.', '; Mary Jane Irwin, Penn State;\\n\\nxxiii\\n\\n\\x0cxxiv ■ Acknowledgments\\n\\nDavid  Kirk,  NVIDIA;  Grant  Martin,  Chief  Scientist,  Tensilica;  Gurindar  Sohi,\\nUniversity  of  Wisconsin–Madison;  Mateo  Valero,  Universidad  Politécnica  de\\nCataluña\\n\\nAppendices\\n\\nKrste  Asanovic´,  University  of  California,  Berkeley  (Appendix  G);  Thomas  M.', ' \\n\\nAdditional Material\\n\\nJohn  Nickolls,  Steve  Keckler,  and  Michael  Toksvig  of  NVIDIA  (Chapter  4\\nNVIDIA GPUs); Victor Lee, Intel (Chapter 4 comparison of Core i7 and GPU);\\nJohn Shalf, LBNL (Chapter 4 recent vector architectures); Sam Williams, LBNL\\n(Roofline  model  for  computers  in  Chapter  4);  Steve  Blackburn  of  Australian\\nNational  University  and  Kathryn  McKinley  of  University  of  Texas  at  Austin\\n(Intel  performance  and  power  measurements  in  Chapter  5);  Luiz  Barroso,  Urs\\nHölzle,  Jimmy  Clidaris,  Bob  Felderman,  and  Chris  Johnson  of  Google  (the\\nGoogle WSC in Chapter 6); James  Hamilton of Amazon Web Services (power\\ndistribution and cost model in Chapter 6)\\n\\nJason  D.', '4 Graphics Processing Units\\n\\n■ 289\\n\\nNVIDIA  decided  to  develop  a  C-like  language  and  programming  environ-\\nment  that  would  improve  the  productivity  of  GPU  programmers  by  attacking\\nboth the challenges of heterogeneous computing and of multifaceted parallelism.', ' \\n\\nNVIDIA decided that the unifying theme of all these forms of parallelism is\\nthe  CUDA  Thread.', ' Hence, NVIDIA classifies the\\nCUDA programming model as Single Instruction, Multiple Thread (SIMT).', '\\n\\nNVIDIA GPU Computational Structures\\n\\nThe  uncommon  heritage  mentioned  above  helps  explain  why  GPUs  have  their\\nown architectural style and their own terminology independent from CPUs.', ' Once we explain the GPU archi-\\ntecture in our terms, we’ll map them into the official jargon of NVIDIA GPUs.', '12 lists the more descriptive term used in this sec-\\ntion,  the  closest  term  from  mainstream  computing,  the  official  NVIDIA  GPU\\nterm in case you are interested, and then a short description of the term.', ' \\n\\nWe use NVIDIA systems as our example as they are representative of GPU\\narchitectures.', ' The Grid and Thread Block\\n\\n\\x0c292 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nMore descrip-\\ntive name\\n\\nClosest old term \\noutside of GPUs\\n\\nOfficial CUDA/\\nNVIDIA GPU term Book definition\\n\\nType\\n\\ns\\nn\\no\\ni\\nt\\nc\\na\\nr\\nt\\ns\\nb\\na\\nm\\na\\nr\\ng\\no\\nr\\nP\\n\\n \\n\\n \\n\\nt\\nc\\ne\\nj\\nb\\no\\ne\\nn\\nh\\nc\\na\\nM\\n\\ni\\n\\n \\n\\ne\\nr\\na\\nw\\nd\\nr\\na\\nh\\ng\\nn\\ni\\ns\\ns\\ne\\nc\\no\\nr\\nP\\n\\ne\\nr\\na\\nw\\nd\\nr\\na\\nh\\ny\\nr\\no\\nm\\ne\\nM\\n\\n \\n\\nVectorizable \\nLoop\\n\\nVectorizable Loop Grid\\n\\nBody of \\nVectorized Loop\\n\\n Body of a \\n(Strip-Mined) \\nVectorized Loop\\n\\nThread Block\\n\\nSequence of\\nSIMD Lane \\nOperations\\n\\nA Thread of \\nSIMD \\nInstructions\\n\\nSIMD \\nInstruction\\n\\nMultithreaded \\nSIMD \\nProcessor\\n\\nThread Block \\nScheduler\\n\\nSIMD Thread\\nScheduler\\n\\nOne iteration of \\na Scalar Loop\\n\\nCUDA Thread\\n\\nThread of Vector \\nInstructions\\n\\nWarp\\n\\nVector Instruction\\n\\nPTX Instruction\\n\\n(Multithreaded) \\nVector Processor\\n\\nStreaming \\nMultiprocessor\\n\\nScalar Processor\\n\\nGiga Thread \\nEngine\\n\\nThread scheduler \\nin a Multithreaded \\nCPU\\n\\nWarp Scheduler\\n\\nSIMD Lane\\n\\nVector Lane\\n\\nThread Processor\\n\\nGPU Memory\\n\\nMain Memory\\n\\nGlobal Memory\\n\\nA vectorizable loop, executed on the GPU, made \\nup of one or more Thread Blocks (bodies of \\nvectorized loop) that can execute in parallel.', '25 on page 314 reveal the official CUDA/NVIDIA and AMD terms and definitions along with\\nthe terms used by OpenCL.', ' However, to hedge their bets, the recent\\nNVIDIA Fermi GPU includes an L2 cache (see Section 4.', '\\n\\n\\x0c298 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nNVIDA GPU Instruction Set Architecture\\n\\nUnlike most system processors, the instruction set target of the NVIDIA compil-\\ners is an abstraction of the hardware instruction set.', '\\nNVIDIA  says  a  branch  diverges  when  this  happens.', '\\n\\n\\x0c304 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nNVIDIA GPU Memory Structures\\n\\nFigure 4.', '18 shows the memory structures of an NVIDIA GPU.', ' This view would consider the NVIDIA GTX 480 as a\\n15-core machine with hardware support for multithreading, where each core has\\n16 lanes.', ' \\n\\n\\x0cClosest CUDA/NVIDIA \\nGPU term\\n\\nComment\\n\\nType\\n\\nVector term\\n\\n \\n\\nm\\na\\nr\\ng\\no\\nr\\nP\\n\\ns Vectorized Loop\\nn\\no\\ni\\nt\\nc\\na\\nr\\nt\\ns\\nb\\na\\n\\nChime\\n\\nGrid\\n\\n--\\n\\nVector Instruction\\n\\nPTX Instruction\\n\\nGather/Scatter\\n\\nGlobal load/store\\n(ld.', '\\n\\nType\\n\\nMore \\ndescriptive \\nname used in \\nthis book\\n\\nVectorizable \\nloop\\n\\nOfficial \\nCUDA/\\nNVIDIA \\nterm\\n\\nGrid\\n\\nBody of \\nVectorized \\nloop\\n\\nThread \\nBlock\\n\\nSequence of\\nSIMD Lane \\noperations\\n\\nCUDA \\nThread\\n\\nA Thread of \\nSIMD \\ninstructions\\n\\nWarp\\n\\ns\\nn\\no\\ni\\nt\\nc\\na\\nr\\nt\\ns\\nb\\na\\nm\\na\\nr\\ng\\no\\nr\\nP\\n\\n \\n\\n \\n\\nt\\nc\\ne\\nj\\nb\\no\\ne\\nn\\nh\\nc\\na\\nM\\n\\ni\\n\\nBook definition and\\nAMD and OpenCL terms\\n\\nOfficial CUDA/NVIDIA\\ndefinition\\n\\nA vectorizable loop, executed on the \\nGPU, made up of one or more “Thread \\nBlocks” (or bodies of vectorized loop) \\nthat can execute in parallel.', '24 Conversion  from  terms  used  in  this  chapter  to  official  NVIDIA/CUDA  and  AMD  jargon.', '\\n\\n\\x0c314 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nMore \\ndescriptive \\nname used in \\nthis book\\n\\nOfficial \\nCUDA/\\nNVIDIA \\nterm\\n\\nType\\n\\nMultithreaded \\nSIMD \\nprocessor\\n\\nStreaming \\nmulti-\\nprocessor\\n\\nThread \\nblock \\nscheduler\\n\\nSIMD \\nThread \\nscheduler\\n\\nGiga \\nthread \\nengine\\n\\nWarp \\nscheduler\\n\\nSIMD \\nLane\\n\\nThread \\nprocessor\\n\\nGPU \\nMemory\\n\\nGlobal \\nMemory\\n\\nBook definition and\\nAMD and OpenCL terms\\n\\nOfficial CUDA/NVIDIA\\ndefinition\\n\\nMultithreaded SIMD Processor that executes \\nthread of SIMD instructions, independent of \\nother SIMD Processors.', '25 Conversion from terms used in this chapter to official NVIDIA/CUDA and AMD jargon.', '  NVIDIA  uses  SIMT,  single-\\ninstruction multiple-thread, rather than SIMD, to describe a streaming multiprocessor.', '\\n\\nAlthough  we’ve  used  CUDA  and  the  NVIDIA  GPU  in  this  section,  rest\\nassured that the same ideas are found in the OpenCL programming language and\\nin GPUs from other companies.', '25 match the descriptive terms and definitions of this section\\nwith the official CUDA/NVIDIA and AMD terms and definitions.', '26 lists the key characteristics of the NVIDIA Tegra 2 for mobile cli-\\nents, which is used in the LG Optimus 2X and runs Android OS, and the Fermi\\nGPU  for  servers.', '  \\n\\nThe NVIDIA Tegra 2 for mobile devices provides both the system processor\\nand the GPU in a single chip using a single physical memory.', '7\\n\\nPutting It All Together: Mobile versus Server GPUs \\nand Tesla versus Core i7\\n\\n\\x0c324 ■ Chapter Four Data-Level Parallelism in Vector, SIMD, and GPU Architectures\\n\\nMarket\\n\\nSystem processor\\n\\nSystem interface\\n\\nSystem interface \\nbandwidth\\n\\nClock rate\\n\\nSIMD multiprocessors\\n\\nSIMD lanes/SIMD \\nmultiprocessor\\n\\nMemory interface\\n\\nMemory bandwidth\\n\\nMemory capacity\\n\\nTransistors\\n\\nProcess\\n\\nDie area\\n\\nPower\\n\\nNVIDIA Tegra 2\\n\\nNVIDIA Fermi GTX 480\\n\\nMobile client\\n\\nDesktop, server\\n\\nDual-Core ARM Cortex-A9\\n\\nNot applicable\\n\\nNot applicable\\n\\nUp to 1 GHz\\n\\nUnavailable\\n\\nUnavailable\\n\\n32-bit LP-DDR2/DDR2\\n\\n2.', '\\n\\nThe NVIDIA GTX 480 in Figure 4.', '27 Intel  Core  i7-960,  NVIDIA  GTX  280,  and  GTX  480  specifications.', '4 G B/sec\\n\\nStre a m\\n\\nNVIDIA GTX280\\n\\n78 GF/sec\\n\\n= 127 G B/sec\\n\\nStrea m\\n\\n1/8\\n\\n1/4\\n\\n1/2\\n\\n1\\n\\n4\\nArithmetic intensity\\n\\n2\\n\\n8\\n\\n16\\n\\n1/8\\n\\n1/4\\n\\n1/2\\n\\n1\\n\\n4\\nArithmetic intensity\\n\\n2\\n\\n8\\n\\n16\\n\\nCore i7 920\\n(Nehalem)\\n\\n1024\\n\\nNVIDIA GTX280\\n\\n624 GF/sec\\n\\n85.', ' The NVIDIA GTX 280 has a DP FP\\npeak of 78 GFLOP/sec, SP FP peak of 624 GFLOP/sec, and 127 GBytes/sec of memory bandwidth.', '8 Fallacies and Pitfalls\\n\\n■ 331\\n\\nbeing  register-oriented  like  MIPS  as  well  as  doubling  the  address  size  to  64\\nbits—without disrupting the NVIDIA software stack.', '  NVIDIA  also  announced  Project\\nDenver, which combines an ARM scalar processor with NVIDIA GPUs in a\\nsingle address space.', ' Use the NVIDIA CUDA Tool-\\nkit  along  with  GPU-SIM  from  the  University  of  British  Columbia  (http://\\nwww.', ' miss rate, B-27\\n\\nBlock transfer engine (BLT)\\nCray Research T3D, F-87\\ninterconnection network \\n\\nprotection, F-87\\n\\nBLT, see Block transfer engine (BLT)\\nBody of Vectorized Loop\\ndefinition, 292, 313\\nGPU hardware, 295–296, 311\\nGPU Memory structure, 304\\nNVIDIA GPU, 296\\nSIMD Lane Registers, 314\\nThread Block Scheduler, 314\\n\\nBoggs, David, F-99\\nBOMB, L-4\\nBooth recoding, J-8 to J-9, J-9, J-10 to \\n\\nJ-11\\n\\nchip comparison, J-60 to J-61\\ninteger multiplication, J-49\\n\\nBose-Einstein formula, definition, 30\\nBounds checking, segmented virtual \\n\\nmemory, B-52\\n\\nBranch byte, VAX, K-71\\nBranch delay slot\\n\\ncharacteristics, C-23 to C-25\\ncontrol hazards, C-41\\nMIPS R4000, C-64\\nscheduling, C-24\\n\\ncanceling, C-24 to C-25\\nconditional branches, 300–303, \\nA-17, A-19 to A-20, \\nA-21\\n\\ncontrol flow instructions, A-16, \\n\\nA-18\\n\\ndelayed, C-23\\ndelay slot, C-65\\nIBM 360, K-86 to K-87\\ninstructions, K-25\\nMIPS control flow instructions, \\n\\nA-38\\n\\nMIPS operations, A-35\\nnullifying, C-24 to C-25\\nRISC instruction set, C-5\\nVAX, K-71 to K-72\\nWCET, E-4\\n\\nBranch folding, definition, 206\\nBranch hazards\\n\\nbasic considerations, C-21\\npenalty reduction, C-22 to C-25\\npipeline issues, C-39 to C-42\\nscheme performance, C-25 to C-26\\nstall reduction, C-42\\n\\nC-27 to C-30\\n\\nBranch offsets, control flow \\n\\ninstructions, A-18\\n\\nBranch penalty\\n\\nexamples, 205\\ninstruction fetch bandwidth, \\n203–206\\n\\nreduction, C-22 to C-25\\nsimple scheme examples, C-25\\n\\nBranch prediction\\naccuracy, C-30\\nbranch cost reduction, 162–167\\ncorrelation, 162–164\\ncost reduction, C-26\\ndynamic, C-27 to C-30\\n\\nIndex\\n\\n■\\n\\nI-7\\n\\nearly schemes, L-27 to L-28\\nideal processor, 214\\nILP exploitation, 201\\ninstruction fetch bandwidth, 205\\nintegrated instruction fetch units, \\n\\n207\\n\\nIntel Core i7, 166–167, 239–241\\nmisprediction rates on SPEC89, 166\\nstatic, C-26 to C-27\\ntrace scheduling, H-19\\ntwo-bit predictor comparison, 165\\n\\nconsiderations, C-27 to \\nC-30, C-29\\n\\nBranch registers\\nIA-64, H-34\\nPowerPC instructions, K-32 to K-33\\nBranch stalls, MIPS R4000 pipeline, \\n\\nC-67\\n\\nBranch-target address\\n\\nbranch hazards, C-42\\nMIPS control flow instructions, \\n\\nA-38\\nMIPS pipeline, C-36, C-37\\nMIPS R4000, C-25\\npipeline branches, C-39\\nRISC instruction set, C-5\\n\\nBranch-target buffers\\n\\nARM Cortex-A8, 233\\nbranch hazard stalls, C-42\\nexample, 203\\ninstruction fetch bandwidth, \\n203–206\\n\\ninstruction handling, 204\\nMIPS control flow instructions, \\n\\nA-38\\n\\nbuffers\\n\\nBrewer, Eric, L-73\\nBridges\\n\\nand bandwidth, F-78\\ndefinition, F-78\\n\\nBubbles\\n\\nand deadlock, F-47\\nrouting comparison, F-54\\nstall as, C-13\\n\\nBubble sort, code example, K-76\\nBuckets, D-26\\nBuffered crossbar switch, switch \\n\\nmicroarchitecture, F-62\\n\\nBuffered wormhole switching, \\nF-51\\n\\nBranch history table, basic scheme, \\n\\nBranch-target cache, see Branch-target \\n\\n\\x0cBuffers\\n\\nTomasulo’s algorithm, 180, 182\\n\\nbranch-prediction, C-27 to C-30, \\n\\nBypassing, see also Forwarding\\n\\nC-29\\n\\ndata hazards requiring stalls, C-19 \\n\\nI-8 ■\\n\\nIndex\\n\\nbranch-target, 203–206, 204, 233, \\nA-38, C-42\\n\\nDSM multiprocessor cache \\n\\ncoherence, I-38 to I-40\\n\\nIntel SCCC, F-70\\ninterconnection networks, F-10 to \\n\\nF-11\\n\\nmemory, 208\\nMIPS scoreboarding, C-74\\nnetwork interface functions, F-7\\nROB, 184–192, 188–189, 199, \\n\\n208–210, 238\\nswitch microarchitecture, F-58 to \\n\\nF-60\\n\\nTLB, see Translation lookaside \\nbuffer (TLB)\\n\\ntranslation buffer, B-45 to B-46\\nwrite buffer, B-11, B-14, B-32, \\n\\nB-35 to B-36\\n\\nBundles\\n\\nIA-64, H-34 to H-35, H-37\\nItanium 2, H-41\\nBurks, Arthur, L-3\\nBurroughs B5000, L-16\\nBus-based coherent multiprocessors, \\n\\nL-59 to L-60\\n\\nBuses\\n\\nbarrier synchronization, I-16\\ncache coherence, 391\\ncentralized shared-memory \\n\\nmultiprocessors, 351\\n\\ndefinition, 351\\ndynamic scheduling with \\n\\nTomasulo’s algorithm, \\n172, 175\\n\\nGoogle WSC servers, 469\\nI/O bus replacements, D-34, D-34\\nlarge-scale multiprocessor \\n\\nsynchronization, I-12 to \\nI-13\\n\\nNEWS communication, F-42\\nscientific workloads on symmetric \\nshared-memory \\nmultiprocessors, I-25\\n\\nSony PlayStation 2 Emotion \\n\\nEngine, E-18\\n\\nvs.', ' miss rate, B-27\\n\\nCache size\\n\\nIndex\\n\\n■\\n\\nI-9\\n\\nmiss rate reduction, B-28\\nmultilevel caches, B-33\\nand relative execution time, B-34\\nscientific workloads\\n\\ndistributed-memory \\n\\nmultiprocessors, I-29 to \\nI-31\\n\\nmultiprocessors, I-22 to \\nI-23, I-24\\n\\nshared-memory multiprogramming \\nworkload, 376\\nvirtually addressed, B-37\\n\\nCACTI\\n\\ncache optimization, 79–80, 81\\nmemory access times, 77\\n\\nCaller saving, control flow \\n\\ninstructions, A-19 to \\nA-20\\n\\nIA-32 segment descriptors, B-53\\nsegmented virtual memory, B-54\\n\\ncompiler structure, A-25 to A-26\\ncontrol flow instructions, A-17, \\nA-19 to A-21\\n\\nCUDA Thread, 297\\ndependence analysis, 321\\nhigh-level instruction set, A-42 to \\n\\nIntel 80x86 integer operations, \\n\\nK-51\\ninvocation options, A-19\\nISAs, 14\\nMIPS control flow instructions, \\n\\nMIPS registers, 12\\nmultiprogrammed workload, \\n\\nA-43\\n\\nA-38\\n\\n378\\n\\nNVIDIA GPU Memory structures, \\n\\n304–305\\nreturn address predictors, 206\\nshared-memory multiprocessor \\n\\nworkload, 369\\n\\nuser-to-OS gates, B-54\\nVAX, K-71 to K-72\\n\\nCanceling branch, branch delay slots, \\n\\nC-24 to C-25\\nCanonical form, AMD64 paged virtual \\n\\nmemory, B-55\\n\\nCapabilities, protection schemes, L-9 \\nto L-10\\n\\n\\x0cI-10 ■\\n\\nIndex\\n\\nCapacity misses\\n\\nblocking, 89–90\\nand cache size, B-24\\ndefinition, B-23\\nmemory hierarchy basics, 75\\nscientific workloads on symmetric \\nshared-memory \\nmultiprocessors, I-22, \\nI-23, I-24\\nshared-memory workload, 373\\nCAPEX, see Capital expenditures \\n\\n(CAPEX)\\n\\nCapital expenditures (CAPEX)\\nWSC costs, 452–455, 453\\nWSC Flash memory, 475\\nWSC TCO case study, 476–478\\n\\nCarrier sensing, shared-media \\n\\nnetworks, F-23\\n\\nCarrier signal, wireless networks, \\nE-21\\n\\nCarry condition code, MIPS core, K-9 \\n\\nto K-16\\n\\nCarry-in, carry-skip adder, J-42\\nCarry-lookahead adder (CLA)\\nchip comparison, J-60\\nearly computer arithmetic, J-63\\nexample, J-38\\ninteger addition speedup, J-37 to \\n\\nwith ripple-carry adder, J-42\\ntree, J-40 to J-41\\n\\nCarry-out\\n\\ncarry-lookahead circuit, J-38\\nfloating-point addition speedup, \\n\\nJ-41\\n\\nJ-25\\n\\nCarry-propagate adder (CPA)\\n\\ninteger multiplication, J-48, J-51\\nmultipass array multiplier, J-51\\n\\nCarry-save adder (CSA)\\n\\ninteger division, J-54 to J-55\\ninteger multiplication, J-47 to J-48, \\n\\nJ-48\\nCarry-select adder\\n\\ncharacteristics, J-43 to J-44\\nchip comparison, J-60\\nexample, J-43\\n\\nCarry-skip adder (CSA)\\n\\ncharacteristics, J-41 to J43\\nexample, J-42, J-44\\n\\ncontrol flow instruction addressing \\nmodes, A-18\\n\\nCCD, see Charge-coupled device \\n(CCD)\\n\\nreturn address predictors, 206\\n\\nC/C++ language\\n\\nCase studies\\n\\nadvanced directory protocol, \\n420–426\\ncache optimization, 131–133\\ncell phones\\n\\nblock diagram, E-23\\nNokia circuit board, E-24\\noverview, E-20\\nradio receiver, E-23\\nstandards and evolution, E-25\\nwireless communication \\nchallenges, E-21\\n\\nwireless networks, E-21 to \\n\\nE-22\\nchip fabrication cost, 61–62\\ncomputer system power \\n\\nconsumption, 63–64\\n\\ndirectory-based coherence, \\n418–420\\n\\ndirty bits, D-61 to D-64\\ndisk array deconstruction, D-51 to \\n\\nD-55, D-52 to D-55\\n\\ndisk deconstruction, D-48 to D-51, \\n\\nhighly parallel memory systems, \\n\\ninstruction set principles, A-47 to \\n\\nD-50\\n\\n133–136\\n\\nA-54\\n\\nI/O subsystem design, D-59 to D-61\\nmemory hierarchy, B-60 to B-67\\nmicroarchitectural techniques, \\n247–254\\n\\npipelining example, C-82 to C-88\\nRAID performance prediction, \\n\\nD-57 to D-59\\n\\nRAID reconstruction, D-55 to \\n\\nSanyo VPC-SX500 digital camera, \\n\\nD-57\\n\\nE-19\\n\\nsingle-chip multicore processor, \\n\\n412–418\\nSony PlayStation 2 Emotion \\n\\nEngine, E-15 to E-18\\n\\nsorting, D-64 to D-67\\nvector kernel on vector processor \\n\\nand GPU, 334–336\\n\\ndependence analysis, H-6\\nGPU computing history, L-52\\nhardware impact on software \\n\\ndevelopment, 4\\n\\ninteger division/remainder, J-12\\nloop-level parallelism \\n\\ndependences, 318, \\n320–321\\n\\nNVIDIA GPU programming, 289\\nreturn address predictors, 206\\nCDB, see Common data bus (CDB)\\nCDC, see Control Data Corporation \\n(CDC)\\nCDF, datacenter, 487\\nCDMA, see Code division multiple \\n\\naccess (CDMA)\\n\\nCedar project, L-60\\nCell, Barnes-Hut n-body algorithm, \\n\\nI-9\\n\\nCell phones\\n\\nblock diagram, E-23\\nembedded system case study\\n\\ncharacteristics, E-22 to E-24\\noverview, E-20\\nradio receiver, E-23\\nstandards and evolution, E-25\\nwireless network overview, \\nE-21 to E-22\\n\\nFlash memory, D-3\\nGPU features, 324\\nNokia circuit board, E-24\\nwireless communication \\n\\nchallenges, E-21\\n\\nwireless networks, E-22\\nCentralized shared-memory \\n\\nmultiprocessors\\n\\nbasic considerations, 351–352\\nbasic structure, 346–347, 347\\ncache coherence, 352–353\\ncache coherence enforcement, \\n354–355\\n\\ncache coherence example, \\n\\n357–362\\ncache coherence extensions, \\n362–363\\ninvalidate protocol \\n\\nimplementation, \\n356–357\\n\\nCAS, see Column access strobe (CAS)\\nCase statements\\n\\nWSC resource allocation, 478–479\\nWSC TCO, 476–478\\n\\n\\x0cSMP and snooping limitations, \\n363–364\\nsnooping coherence \\n\\nimplementation, \\n365–366\\nsnooping coherence protocols, \\n355–356\\n\\nCentralized switched networks\\n\\nexample, F-31\\nrouting algorithms, F-48\\ntopology, F-30 to F-34, F-31\\n\\nCentrally buffered switch, \\n\\nmicroarchitecture, F-57\\n\\nCentral processing unit (CPU)\\n\\nAmdahl’s law, 48\\naverage memory access time, B-17\\ncache performance, B-4\\ncoarse-grained multithreading, 224\\nearly pipelined versions, L-26 to \\n\\nL-27\\n\\nexception stopping/restarting, C-47\\nextensive pipelining, C-81\\nGoogle server usage, 440\\nGPU computing history, L-52\\nvs.', ' vector architectures, 308\\nmultiple lanes, 272\\nNVIDIA GPU computational \\n\\nstructures, 296\\n\\nvector chaining, G-12\\nvector execution time, 269, G-4\\nvector performance, G-2\\nvector sequence calculations, 270\\n\\nChip-crossing wire delay, F-70\\n\\nOCN history, F-103\\n\\nChipkill\\n\\nmemory dependability, 104–105\\nWSCs, 473\\n\\nChoke packets, congestion \\n\\nChunk\\n\\ndisk array deconstruction, D-51\\nShear algorithm, D-53\\n\\nCIFS, see Common Internet File \\n\\nC-34\\n\\nChime\\n\\nprocessor performance equation, \\n\\nSystem (CIFS)\\n\\n49–51\\n\\nCircuit switching\\n\\nprocessor performance time, 49\\n\\ncongestion management, F-64 to \\n\\nCerf, Vint, F-97\\n\\nF-65\\n\\nIndex\\n\\n■\\n\\nI-11\\n\\ninterconnected networks, F-50\\n\\nCirculating water system (CWS)\\ncooling system design, 448\\nWSCs, 448\\n\\nCISC, see Complex Instruction Set \\n\\nComputer (CISC)\\n\\nCLA, see Carry-lookahead adder \\n(CLA)\\n\\nClean block, definition, B-11\\nClimate Savers Computing Initiative, \\n\\npower supply \\nefficiencies, 462\\n\\nClock cycles\\n\\nbasic MIPS pipeline, C-34 to C-35\\nand branch penalties, 205\\ncache performance, B-4\\nFP pipeline, C-66\\nand full associativity, B-23\\nGPU conditional branching, 303\\nILP exploitation, 197, 200\\nILP exposure, 157\\ninstruction fetch bandwidth, \\n202–203\\n\\ninstruction steps, 173–175\\nIntel Core i7 branch predictor, 166\\nMIPS exceptions, C-48\\nMIPS pipeline, C-52\\nMIPS pipeline FP operations, C-52 \\n\\nto C-53\\nMIPS scoreboarding, C-77\\nmiss rate calculations, B-31 to B-32\\nmultithreading approaches, \\n225–226\\npipelining performance, C-10\\nprocessor performance equation, 49\\nRISC classic pipeline, C-7\\nSun T1 multithreading, 226–227\\nswitch microarchitecture \\n\\npipelining, F-61\\n\\nvector architectures, G-4\\nvector execution time, 269\\nvector multiple lanes, 271–273\\nVLIW processors, 195\\n\\naddressing modes, A-10\\nARM Cortex-A8, 235\\nbranch schemes, C-25 to C-26, \\n\\nC-26\\n\\ncache behavior impact, B-18 to \\n\\nB-19\\ncache hit calculation, B-5\\ndata hazards requiring stalls, C-20\\n\\nmanagement, F-65\\n\\nClock cycles per instruction (CPI)\\n\\n\\x0ccalculations, 218–219\\n\\nCloud computing\\n\\nI-12 ■\\n\\nIndex\\n\\nClock cycles per instruction (continued)\\n\\nextensive pipelining, C-81\\nfloating-point calculations, 50–52\\nILP concepts, 148–149, 149\\nILP exploitation, 192\\nIntel Core i7, 124, 240, 240–241\\nmicroprocessor advances, L-33\\nMIPS R4000 performance, C-69\\nmiss penalty reduction, B-32\\nmultiprocessing/\\n\\nmultithreading-based \\nperformance, 398–400\\n\\nmultiprocessor communication \\n\\ncalculations, 350\\npipeline branch issues, C-41\\npipeline with stalls, C-12 to C-13\\npipeline structural hazards, C-15 to \\n\\nC-16\\n\\npipelining concept, C-3\\nprocessor performance \\n\\nprocessor performance time, 49–51\\nand processor speed, 244\\nRISC history, L-21\\nshared-memory workloads, \\n369–370\\nsimple MIPS implementation, \\n\\nC-33 to C-34\\nstructural hazards, C-13\\nSun T1 multithreading unicore \\n\\nperformance, 229\\n\\nSun T1 processor, 399\\nTomasulo’s algorithm, 181\\nVAX 8700 vs.', ' vector architectures, \\n\\nCyclic redundancy check (CRC)\\nIBM Blue Gene/L 3D torus \\n\\nnetwork, F-73\\n\\nnetwork interface, F-8\\nCydrome Cydra 6, L-30, L-32\\n\\n310\\n\\nNVIDIA GPU programming, \\n\\n289\\nPTX, 298, 300\\nsample program, 289–290\\nSIMD instructions, 297\\nterminology, 313–315\\n\\nCUDA Thread\\n\\nCUDA programming model, 300, \\n\\n315\\n\\ndefinition, 292, 313\\ndefinitions and terms, 314\\nGPU data addresses, 310\\nGPU Memory structures, 304\\nNVIDIA parallelism, 289–290\\nvs.', ' WSCs, 436\\nData dependences\\n\\nconditional instructions, H-24\\ndata hazards, 167–168\\n\\nIndex\\n\\n■\\n\\nI-17\\n\\ndynamically scheduling with \\n\\nscoreboard, C-71\\n\\nexample calculations, H-3 to H-4\\nhazards, 153–154\\nILP, 150–152\\nILP hardware model, 214–215\\nILP limitation studies, 220\\nvector execution time, 269\\n\\nData fetching\\n\\nARM Cortex-A8, 234\\ndirectory-based cache coherence \\n\\nprotocol example, \\n382–383\\n\\ndynamically scheduled pipelines, \\nC-70 to C-71\\n\\nILP, instruction bandwidth\\n\\nbasic considerations, 202–203\\nbranch-target buffers, 203–206\\nreturn address predictors, \\n\\n206–207\\nMIPS R4000, C-63\\nsnooping coherence protocols, \\n355–356\\n\\nData flow\\n\\ncontrol dependence, 154–156\\ndynamic scheduling, 168\\nglobal code scheduling, H-17\\nILP limitation studies, 220\\nlimit, L-33\\n\\nData flow execution, hardware-based \\n\\nspeculation, 184\\n\\nDatagrams, see Packets\\nData hazards\\n\\nARM Cortex-A8, 235\\nbasic considerations, C-16\\ndefinition, C-11\\ndependences, 152–154\\ndynamic scheduling, 167–176\\nbasic concept, 168–170\\nexamples, 176–178\\nTomasulo’s algorithm, \\n\\n170–176, 178–179\\n\\nTomasulo’s algorithm \\n\\nloop-based example, \\n179–181\\n\\nILP limitation studies, 220\\ninstruction set complications, C-50 \\n\\nto C-51\\n\\nmicroarchitectural techniques case \\nstudy, 247–254\\n\\nMIPS pipeline, C-71\\nRAW, C-57 to C-58\\n\\n\\x0cI-18 ■\\n\\nIndex\\n\\nData hazards\\n\\nstall minimization by forwarding, \\n\\nC-16 to C-19, C-18\\n\\nstall requirements, C-19 to C-21\\nVMIPS, 264\\n\\nData-level parallelism (DLP)\\n\\ndefinition, 9\\nGPUs\\n\\nbasic considerations, 288\\nbasic PTX thread instructions, \\n\\n299\\n\\nconditional branching, 300–303\\ncoprocessor relationship, \\n\\n330–331\\nFermi GPU architecture \\n\\ninnovations, 305–308\\n\\nFermi GTX 480 floorplan, 295\\nmapping examples, 293\\nMultimedia SIMD comparison, \\n\\n312\\n\\nmultithreaded SIMD Processor \\nblock diagram, 294\\nNVIDIA computational \\n\\nstructures, 291–297\\nNVIDIA/CUDA and AMD \\n\\nterminology, 313–315\\nNVIDIA GPU ISA, 298–300\\nNVIDIA GPU Memory \\n\\nprogramming, 288–291\\nSIMD thread scheduling, 297\\nterminology, 292\\nvs.', ' access time, D-3\\ncost trends, 27\\nCray X1, G-22\\nCUDA, 290\\ndependability, 104\\ndisk storage, D-3 to D-4\\nembedded benchmarks, E-13\\nerrors and faults, D-11\\nfirst vector computers, L-45, L-47\\nFlash memory, 103–104\\nGoogle WSC servers, 468–469\\nGPU SIMD instructions, 296\\nIBM Blue Gene/L, I-43 to I-44\\nimprovement over time, 17\\nintegrated circuit costs, 28\\nIntel Core i7, 121\\ninternal organization, 98\\nmagnetic storage history, L-78\\nmemory hierarchy design, 73, 73\\nmemory performance, 100–102\\nmultibanked caches, 86\\nNVIDIA GPU Memory structures, \\n\\nperformance milestones, 20\\npower consumption, 63\\nreal-world server considerations, \\n\\n305\\n\\n52–55\\n\\nRoofline model, 286\\nserver energy savings, 25\\nSony PlayStation 2, E-16, E-17\\nspeed trends, 99\\ntechnology trends, 17\\nvector memory systems, G-9\\nvector processor, G-25\\nWSC efficiency measurement, 450\\n\\nWSC memory costs, 473–474\\nWSC memory hierarchy, 444–445\\nWSC power modes, 472\\nyield, 32\\n\\nDynamic scheduling\\nfirst use, L-27\\nILP\\n\\nbasic concept, 168–169\\ndefinition, 168\\nexample and algorithms, \\n176–178\\nwith multiple issue and \\n\\nspeculation, 197–202\\n\\novercoming data hazards, \\n\\n167–176\\n\\nTomasulo’s algorithm, 170–176, \\n178–179, 181–183\\n\\nMIPS scoreboarding, C-79\\nSMT on superscalar processors, 230\\nand unoptimized code, C-81\\nDynamic voltage-frequency scaling \\n(DVFS)\\n\\nenergy efficiency, 25\\nGoogle WSC, 467\\nprocessor performance equation, \\n\\n52\\nDynamo (Amazon), 438, 452\\n\\nE\\nEarly restart, miss penalty reduction, \\n\\n86\\n\\nEarth Simulator, L-46, L-48, L-63\\nEBS, see Elastic Block Storage (EBS)\\nEC2, see Amazon Elastic Computer \\n\\nCloud (EC2)\\n\\nECC, see Error-Correcting Code \\n(ECC)\\n\\nEckert, J.', ' vector \\n\\narchitectures, 310\\n\\nExponential back-off\\n\\nlarge-scale multiprocessor \\n\\nsynchronization, I-17\\n\\nspin lock, I-17\\n\\nExponential distribution, definition, \\n\\nD-27\\n\\nExtended accumulator\\n\\nflawed architectures, A-44\\nISA classification, A-3\\n\\nF\\nFacebook, 460\\nFailures, see also Mean time between \\nfailures (MTBF); Mean \\ntime to failure (MTTF)\\n\\nAmdahl’s law, 56\\nBerkeley’s Tertiary Disk project, \\n\\nD-12\\n\\ncloud computing, 455\\ndefinition, D-10\\ndependability, 33–35\\ndirty bits, D-61 to D-64\\nDRAM, 473\\nexample calculation, 48\\nGoogle WSC networking, 469–470\\npower failure, C-43 to C-44, C-46\\npower utilities, 435\\nRAID reconstruction, D-55 to \\n\\nD-57\\n\\nRAID row-diagonal parity, D-9\\nrate calculations, 48\\nservers, 7, 434\\nSLA states, 34\\nstorage system components, D-43\\nstorage systems, D-6 to D-10\\nTDP, 22\\nTertiary Disk, D-13\\nWSC running service, 434–435\\nWSCs, 8, 438–439\\nWSC storage, 442–443\\n\\nFalse sharing\\n\\ndefinition, 366–367\\nshared-memory workload, 373\\n\\nFarmVille, 460\\nFast Fourier transformation (FFT)\\n\\ncharacteristics, I-7\\ndistributed-memory \\n\\nmultiprocessor, I-32\\n\\nexample calculations, I-27 to I-29\\nsymmetric shared-memory \\n\\nmultiprocessors, I-22, \\nI-23, I-25\\nFast traps, SPARC instructions, K-30\\nFat trees\\n\\ndefinition, F-34\\nNEWS communication, F-43\\nrouting algorithms, F-48\\nSAN characteristics, F-76\\ntopology, F-38 to F-39\\ntorus topology interconnections, \\nF-36 to F-38\\n\\nFault detection, pitfalls, 57–58\\nFault-induced deadlock, routing, F-44\\nFaulting prefetches, cache \\n\\noptimization, 92\\n\\nFaults, see also Exceptions; Page \\nfaults\\n\\naddress fault, B-42\\ndefinition, D-10\\nand dependability, 33\\ndependability benchmarks, D-21\\nprogramming mistakes, D-11\\nstorage systems, D-6 to D-10\\nTandem Computers, D-12 to D-13\\nVAX systems, C-44\\n\\nFault tolerance\\n\\nand adaptive routing, F-94\\ncommercial interconnection \\n\\nnetworks, F-66 to F-69\\n\\nDECstation 5000 reboots, F-69\\ndependability benchmarks, D-21\\n\\nRAID, D-7\\nSAN example, F-74\\nWSC memory, 473–474\\nWSC network, 461\\n\\nFault-tolerant routing, commercial \\n\\ninterconnection \\nnetworks, F-66 to F-67\\n\\nFC, see Fibre Channel (FC)\\nFC-AL, see Fibre Channel Arbitrated \\n\\nLoop (FC-AL)\\n\\nFC-SW, see Fibre Channel Switched \\n(FC-SW)\\n\\nFeature size\\n\\ndependability, 33\\nintegrated circuits, 19–21\\nFEC, see Forward error correction \\n(FEC)\\n\\nFederal Communications Commission \\n\\n(FCC), telephone \\ncompany outages, D-15\\n\\nFermi GPU\\n\\narchitectural innovations, 305–308\\nfuture features, 333\\nGrid mapping, 293\\nmultithreaded SIMD Processor, \\n\\n307\\n\\nNVIDIA, 291, 305\\nSIMD, 296–297\\nSIMD Thread Scheduler, 306\\nFermi Tesla, GPU computing history, \\n\\nL-52\\n\\nFermi Tesla GTX 280\\n\\nGPU comparison, 324–325, 325\\nmemory bandwidth, 328\\nraw/relative GPU performance, \\n\\n328\\n\\nsynchronization, 329\\nweaknesses, 330\\nFermi Tesla GTX 480\\nfloorplan, 295\\nGPU comparisons, 323–330, 325\\n\\nFetch-and-increment\\n\\nlarge-scale multiprocessor \\n\\nsynchronization, I-20 to \\nI-21\\n\\nsense-reversing barrier, I-21\\nsynchronization, 388\\nFetching, see Data fetching\\nFetch stage, TI 320C55 DSP, E-7\\nFFT, see Fast Fourier transformation \\n\\n(FFT)\\n\\nFibre Channel (FC), F-64, F-67, F-102\\n\\n\\x0c(FPGAs), WSC array \\nswitch, 443\\n\\nFlash memory\\n\\nfile system benchmarking, D-20\\nNetApp FAS6000 filer, D-42\\nFibre Channel Arbitrated Loop \\n\\n(FC-AL), F-102\\n\\nblock servers vs.', ' window size, 217\\n\\n\\x0cI-28 ■\\n\\nIndex\\n\\nFloating-point operations (continued)\\npipeline hazards and forwarding, \\nC-55 to C-57\\npipeline structural hazards, C-16\\nprecisions, J-33 to J-34\\nremainder, J-31 to J-32\\nROB commit, 187\\nSMT, 398–400\\nSPARC, K-31\\nSPEC benchmarks, 39\\nspecial values, J-14 to J-15\\nstalls from RAW hazards, C-55\\nstatic branch prediction, C-26 to \\n\\nC-27\\nTomasulo’s algorithm, 185\\nunderflow, J-36 to J-37, J-62\\nVAX, B-73\\nvector chaining, G-11\\nvector sequence chimes, 270\\nVLIW processors, 195\\nVMIPS, 264\\n\\nFloating-point registers (FPRs)\\n\\nIA-64, H-34\\nIBM Blue Gene/L, I-42\\nMIPS data transfers, A-34\\nMIPS operations, A-36\\nMIPS64 architecture, A-34\\nwrite-back, C-56\\n\\ncalculation, 47–48\\nCPI calculations, 50–51\\n\\nFloating Point Systems AP-120B, \\nL-28\\nFloppy disks, L-78\\nFlow-balanced state, D-23\\nFlow control\\n\\nand arbitration, F-21\\ncongestion management, F-65\\ndirect networks, F-38 to F-39\\nformat, F-58\\ninterconnection networks, F-10 to \\n\\nsystem area network history, F-100 \\n\\nF-11\\n\\nto F-101\\n\\ndependence analysis, H-6\\ninteger division/remainder, J-12\\nloop-level parallelism \\n\\ndependences, 320–321\\n\\nMIPS scoreboarding, C-77\\nperformance measurement history, \\n\\nL-6\\n\\nreturn address predictors, 206\\nForward error correction (FEC), DSP, \\nE-5 to E-7\\n\\nFull adders, J-2, J-3\\nFully associative cache\\n\\nblock placement, B-7\\nconflict misses, B-23\\ndirect-mapped cache, B-9\\nmemory hierarchy basics, 74\\n\\nFully connected topology\\n\\ndistributed switched networks, \\n\\nF-34\\n\\nNEWS communication, F-43\\n\\nForwarding, see also Bypassing\\n\\nFunctional hazards\\n\\nALUs, C-40 to C-41\\ndata hazard stall minimization, \\n\\nC-16 to C-19, C-18\\n\\ndynamically scheduled pipelines, \\nC-70 to C-71\\n\\nload instruction, C-20\\nlonger latency pipelines, C-54 to \\n\\nC-58\\noperand, C-19\\nForwarding table\\n\\nrouting implementation, F-57\\nswitch microarchitecture \\n\\npipelining, F-60\\n\\nForward path, cell phones, E-24\\nFourier-Motzkin algorithm, L-31\\nFourier transform, DSP, E-5\\nFour-way conflict misses, definition, \\n\\nFP, see Floating-point (FP) operations\\nFPGAs, see Field-programmable gate \\n\\narrays (FPGAs)\\n\\nFPRs, see Floating-point registers \\n(FPRs)\\n\\nFPSQR, see Floating-point square root \\n(FPSQR)\\n\\nFrame pointer, VAX, K-71\\nFreeze, branch penalty reduction, \\nC-22\\n\\nFrequency modulation (FM), wireless \\n\\nneworks, E-21\\n\\nFront-end stage, Itanium 2, H-42\\nFU, see Functional unit (FU)\\nFujitsu Primergy BX3000 blade \\n\\nARM Cortex-A8, 233\\nmicroarchitectural techniques case \\nstudy, 247–254\\n\\nFunctional unit (FU)\\n\\nFP operations, C-66\\ninstruction execution example, \\n\\nC-80\\n\\nIntel Core i7, 237\\nItanium 2, H-41 to H-43\\nlatencies, C-53\\nMIPS pipeline, C-52\\nMIPS scoreboarding, C-75 to C-80\\nOCNs, F-3\\nvector add instruction, 272, \\n272–273\\n\\nVMIPS, 264\\n\\nFunction calls\\n\\nGPU programming, 289\\nNVIDIA GPU Memory structures, \\n\\n304–305\\nPTX assembler, 301\\n\\nFunction pointers, control flow \\n\\ninstruction addressing \\nmodes, A-18\\n\\nFused multiply-add, floating point, \\n\\nJ-32 to J-33\\n\\nFuture file, precise exceptions, C-59\\n\\nG\\nGateways, Ethernet, F-79\\nGather-Scatter\\n\\ndefinition, 309\\nGPU comparisons, 329\\nmultimedia instruction compiler \\nsupport, A-31\\n\\nsparse matrices, G-13 to G-14\\nvector architectures, 279–280\\nGCD, see Greatest common divisor \\n\\n(GCD) test\\nGDDR, see Graphics double data rate \\n(GDDR)\\n\\nFluent, F-76, F-77\\nFlush, branch penalty reduction, C-22\\nFM, see Frequency modulation (FM)\\nForm factor, interconnection \\n\\nserver, F-85\\nFujitsu VP100, L-45, L-47\\nFujitsu VP200, L-45, L-47\\nFull access\\n\\nnetworks, F-9 to F-12\\n\\ndimension-order routing, F-47 to \\n\\nFORTRAN\\n\\ncompiler types and classes, A-28\\ncompiler vectorization, G-14, G-15\\n\\nF-48\\n\\nF-29\\n\\ninterconnection network topology, \\n\\nFloating-point square root (FPSQR)\\n\\nB-23\\n\\n\\x0cGDRAM, see Graphics dynamic \\n\\nGlobal optimizations\\n\\nrandom-access memory \\n(GDRAM)\\n\\ncompilers, A-26, A-29\\noptimization types, A-28\\n\\ncomputing history, L-52\\ndefinition, 9\\nDLP\\n\\nGE 645, L-9\\nGeneral-Purpose Computing on GPUs \\n(GPGPU), L-51 to L-52\\nGeneral-purpose electronic computers, \\n\\nhistorical background, \\nL-2 to L-4\\n\\nGeneral-purpose registers (GPRs)\\nadvantages/disadvantages, A-6\\nIA-64, H-38\\nIntel 80x86, K-48\\nISA classification, A-3 to A-5\\nMIPS data transfers, A-34\\nMIPS operations, A-36\\nMIPS64, A-34\\nVMIPS, 265\\n\\nGENI, see Global Environment for \\nNetwork Innovation \\n(GENI)\\n\\nGeometric means, example \\n\\ncalculations, 43–44\\n\\nGFS, see Google File System (GFS)\\nGibson mix, L-6\\nGiga Thread Engine, definition, 292, \\n\\n314\\nGlobal address space, segmented \\n\\nvirtual memory, B-52\\n\\nGlobal code scheduling\\nexample, H-16\\nparallelism, H-15 to H-23\\nsuperblock scheduling, H-21 to \\nH-23, H-22\\ntrace scheduling, H-19 to H-21, \\n\\nH-20\\nGlobal common subexpression \\n\\nelimination, compiler \\nstructure, A-26\\n\\nGlobal data area, and compiler \\ntechnology, A-27\\n\\nGlobal Environment for Network \\nInnovation (GENI), \\nF-98\\nGlobal load/store, definition, 309\\nGlobal Memory\\n\\ndefinition, 292, 314\\nGPU programming, 290\\nlocks via coherence, 390\\n\\nGlobal miss rate\\n\\ndefinition, B-31\\nmultilevel caches, B-33\\n\\nGlobal Positioning System, CDMA, E-25\\nGlobal predictors\\n\\nIntel Core i7, 166\\ntournament predictors, 164–166\\n\\nGlobal scheduling, ILP, VLIW \\n\\nprocessor, 194\\n\\nGlobal system for mobile \\n\\ncommunication (GSM), \\ncell phones, E-25\\n\\nGoldschmidt’s division algorithm, \\n\\nJ-29, J-61\\n\\nGoldstine, Herman, L-2 to L-3\\nGoogle\\n\\nBigtable, 438, 441\\ncloud computing, 455\\ncluster history, L-62\\ncontainers, L-74\\nMapReduce, 437, 458–459, 459\\nserver CPUs, 440\\nserver power-performance \\n\\nbenchmarks, 439–441\\n\\nWSCs, 432, 449\\n\\ncontainers, 464–465, 465\\ncooling and power, 465–468\\nmonitoring and repairing, \\n\\n469–470\\n\\nPUE, 468\\nservers, 467, 468–469\\n\\nGoogle App Engine, L-74\\nGoogle Clusters\\n\\nmemory dependability, 104\\npower consumption, F-85\\n\\nGoogle File System (GFS)\\n\\nMapReduce, 438\\nWSC storage, 442–443\\n\\nGoogle Goggles\\nPMDs, 6\\nuser experience, 4\\n\\nGoogle search\\n\\nshared-memory workloads, 369\\nworkload demands, 439\\n\\nGordon Bell Prize, L-57\\nGPGPU (General-Purpose Computing \\non GPUs), L-51 to L-52\\nGPRs, see General-purpose registers \\n(GPRs)\\n\\nGPU (Graphics Processing Unit)\\n\\nbanked and graphics memory, \\n322–323\\n\\nIndex\\n\\n■\\n\\nI-29\\n\\nbasic considerations, 288\\nbasic PTX thread instructions, \\n\\n299\\n\\nconditional branching, 300–303\\ncoprocessor relationship, \\n\\n330–331\\ndefinitions, 309\\nFermi GPU architecture \\n\\ninnovations, 305–308\\n\\nFermi GTX 480 floorplan, 295\\nGPUs vs.', ' vector architectures, \\n\\n308–312, 310\\n\\nmapping examples, 293\\nMultimedia SIMD comparison, \\n\\n312\\n\\nmultithreaded SIMD Processor \\nblock diagram, 294\\n\\nNVIDIA computational \\n\\nstructures, 291–297\\nNVIDIA/CUDA and AMD \\n\\nterminology, 313–315\\nNVIDIA GPU ISA, 298–300\\nNVIDIA GPU Memory \\n\\nstructures, 304, 304–305\\n\\nprogramming, 288–291\\nSIMD thread scheduling, 297\\nterminology, 292\\n\\nfine-grained multithreading, 224\\nfuture features, 332\\ngather/scatter operations, 280\\nhistorical background, L-50\\nloop-level parallelism, 150\\nvs.', ' vector processor operation, \\n\\n276\\n\\n\\x0cI-30 ■\\n\\nIndex\\n\\nGPU Memory\\ncaches, 306\\nCUDA program, 289\\ndefinition, 292, 309, 314\\nfuture architectures, 333\\nGPU programming, 288\\nNVIDIA, 304, 304–305\\nsplitting from main memory, 330\\n\\nGradual underflow, J-15, J-36\\nGrain size\\n\\nMIMD, 10\\nTLP, 346\\n\\nGrant phase, arbitration, F-49\\nGraph coloring, register allocation, \\n\\nA-26 to A-27\\n\\nGraphics double data rate (GDDR)\\n\\ncharacteristics, 102\\nFermi GTX 480 GPU, 295, 324\\nGraphics dynamic random-access \\nmemory (GDRAM)\\n\\nbandwidth issues, 322–323\\ncharacteristics, 102\\n\\nGraphics-intensive benchmarks, \\n\\nSIMD Processors, 295\\nThread Blocks, 295\\n\\nGrid computing, L-73 to L-74\\nGrid topology\\n\\ncharacteristics, F-36\\ndirect networks, F-37\\n\\nGSDRAM, see Graphics synchronous \\ndynamic random-access \\nmemory (GSDRAM)\\nGSM, see Global system for mobile \\ncommunication (GSM)\\n\\nGuest definition, 108\\nGuest domains, Xen VM, 111\\n\\nH\\nHadoop, WSC batch processing, 437\\nHalf adders, J-2\\nHalf words\\n\\naligned/misaligned addresses, A-8\\nmemory address interpretation, \\nA-7 to A-8\\n\\nMIPS data types, A-34\\noperand sizes/types, 12\\nas operand type, A-13 to A-14\\n\\ndesktop performance, 38\\n\\nGraphics pipelines, historical \\n\\nHandshaking, interconnection \\n\\nbackground, L-51\\n\\nGraphics Processing Unit, see GPU \\n(Graphics Processing \\nUnit)\\nGraphics synchronous dynamic \\n\\nnetworks, F-10\\n\\nHard drive, power consumption, 63\\nHard real-time systems, definition, E-3 \\n\\nto E-4\\n\\nHardware\\n\\nGraphics Synthesizer, Sony \\n\\nrandom-access memory \\n(GSDRAM), \\ncharacteristics, 102\\n\\nPlayStation 2, E-16, \\nE-16 to E-17\\n\\nGreater than condition code, \\n\\nPowerPC, K-10 to K-11\\nGreatest common divisor (GCD) test, \\n\\nloop-level parallelism \\ndependences, 319, H-7\\n\\nGrid\\n\\narithmetic intensity, 286\\nCUDA parallelism, 290\\ndefinition, 292, 309, 313\\nand GPU, 291\\nGPU Memory structures, 304\\nGPU terms, 308\\nmapping example, 293\\nNVIDIA GPU computational \\n\\nstructures, 291\\n\\nas architecture component, 15\\ncache optimization, 96\\ncompiler scheduling support, L-30 \\n\\nto L-31\\ncompiler speculation support\\nmemory references, H-32\\noverview, H-27\\npreserving exception behavior, \\n\\nH-28 to H-32\\n\\ndescription notation, K-25\\nenergy/performance fallacies, 56\\nfor exposing parallelism, H-23 to \\n\\nH-27\\n\\nILP approaches, 148, 214–215\\ninterconnection networks, F-9\\npipeline hazard detection, C-38\\nVirtual Machines protection, 108\\nWSC cost-performance, 474\\nWSC running service, 434–435\\n\\nHardware-based speculation\\nbasic algorithm, 191\\n\\ndata flow execution, 184\\nFP unit using Tomasulo’s \\nalgorithm, 185\\n\\nILP\\n\\ndata flow execution, 184\\nwith dynamic scheduling and \\nmultiple issue, 197–202\\n\\nFP unit using Tomasulo’s \\nalgorithm, 185\\n\\nkey ideas, 183–184\\nmultiple-issue processors, 198\\nreorder buffer, 184–192\\nvs.', ' software speculation, \\n221–222\\nkey ideas, 183–184\\n\\nHardware faults, storage systems, \\nD-11\\n\\nHardware prefetching\\n\\ncache optimization, 131–133\\nmiss penalty/rate reduction, 91–92\\nNVIDIA GPU Memory structures, \\n\\n305\\n\\nSPEC benchmarks, 92\\n\\nHardware primitivies\\n\\nbasic types, 387–389\\nlarge-scale multiprocessor \\n\\nsynchronization, I-18 to \\nI-21\\n\\nsynchronization mechanisms, \\n387–389\\n\\nHarvard architecture, L-4\\nHazards, see also Data hazards\\n\\nbranch hazards, C-21 to C-26, \\nC-39 to C-42, C-42\\n\\ncontrol hazards, 235, C-11\\ndetection, hardware, C-38\\ndynamically scheduled pipelines, \\nC-70 to C-71\\n\\nexecution sequences, C-80\\nfunctional hazards, 233, 247–254\\ninstruction set complications, C-50\\nlonger latency pipelines, C-54 to \\n\\nC-58\\n\\nstructural hazards, 268–269, C-11, \\n\\nC-13 to C-16, C-71, \\nC-78 to C-79\\n\\nHCAs, see Host channel adapters \\n(HCAs)\\n\\nHeader\\n\\nmessages, F-6\\npacket format, F-7\\n\\n\\x0ccharacteristics, F-20\\n\\nhistory, G-28\\n\\ninterconnection network topology, \\n\\nHP-Compaq servers\\n\\nswitch microarchitecture \\n\\nHigh-level language computer \\n\\nHops\\n\\npipelining, F-60\\n\\nTCP/IP, F-84\\n\\narchitecture (HLLCA), \\nL-18 to L-19\\n\\nHead-of-line (HOL) blocking\\n\\nHigh-level optimizations, compilers, \\n\\ncongestion management, F-64\\nswitch microarchitecture, F-58 to \\nF-59, F-59, F-60, F-62\\n\\nsystem area network history, F-101\\nvirtual channels and throughput, \\n\\nF-93\\n\\nHeap, and compiler technology, A-27 \\nto A-28\\nHEP processor, L-34\\nHeterogeneous architecture, \\ndefinition, 262\\n\\nHewlett-Packard AlphaServer, \\n\\nHewlett-Packard PA-RISC\\naddressing modes, K-5\\narithmetic/logical instructions, \\n\\ncharacteristics, K-4\\nconditional branches, K-12, K-17, \\n\\nF-100\\n\\nK-11\\n\\nK-34\\n\\nconstant extension, K-9\\nconventions, K-13\\ndata transfer instructions, K-10\\nEPIC, L-32\\nfeatures, K-44\\nfloating-point precisions, J-33\\nFP instructions, K-23\\nMIPS core extensions, K-23\\nmultimedia support, K-18, K-18, \\n\\nK-19\\n\\nunique instructions, K-33 to K-36\\n\\nHewlett-Packard PA-RISC MAX2, \\nmultimedia support, \\nE-11\\n\\nHewlett-Packard Precision \\n\\nArchitecture, integer \\narithmetic, J-12\\n\\nHewlett-Packard ProLiant BL10e G2 \\n\\nBlade server, F-85\\n\\nA-26\\n\\nHighly parallel memory systems, case \\n\\nstudies, 133–136\\n\\nHigh-order functions, control flow \\n\\ninstruction addressing \\nmodes, A-18\\n\\nHigh-performance computing (HPC)\\n\\nInfiniBand, F-74\\ninterconnection network \\n\\nF-44\\n\\nstorage area network history, F-102\\nswitch microarchitecture, F-56\\nvector processor history, G-27\\nwrite strategy, B-10\\nvs.', ' success, A-44 to A-45\\nGPR advantages/disadvantages, \\n\\nA-6\\n\\nhigh-level considerations, A-39, \\nA-41 to A-43\\n\\nhigh-level language computer \\narchitecture, L-18 to \\nL-19\\n\\nIA-64\\n\\ninstruction formats, H-39\\ninstructions, H-35 to H-37\\ninstruction set basics, H-38\\noverview, H-32 to H-33\\npredication and speculation, \\nH-38 to H-40\\nIBM 360, K-85 to K-88\\nimmediate addressing mode, A-10 \\n\\nliteral addressing mode, A-10 to \\n\\nto A-11\\n\\nA-11\\n\\nIndex\\n\\n■\\n\\nI-35\\n\\nregisters, A-34\\nusage, A-39\\nMIPS64, 14, A-40\\nmultimedia instruction compiler \\nsupport, A-31 to A-32\\n\\nNVIDIA GPU, 298–300\\noperand locations, A-4\\noperands per ALU instruction, A-6\\noperand type and size, A-13 to \\n\\nA-14\\noperations, A-14 to A-16\\noperator categories, A-15\\noverview, K-2\\nperformance and efficiency \\n\\nprediction, 241–243\\n\\nand protection, 112\\nRISC code size, A-23 to A-24\\nRISC history, L-19 to L-22, L-21\\nstack architectures, L-16 to L-17\\ntop 80x86 instructions, A-16\\n“typical” program fallacy, A-43\\nVirtual Machines protection, \\n107–108\\n\\nVirtual Machines support, \\n\\n109–110\\n\\nVMIPS, 264–265\\nVMM implementation, 128–129\\n\\nInstructions per clock (IPC)\\nARM Cortex-A8, 236\\nflawless architecture design, A-45\\nILP for realizable processors, \\n216–218\\n\\nMIPS scoreboarding, C-72\\nmultiprocessing/\\n\\nmultithreading-based \\nperformance, 398–400\\n\\nmemory addressing, A-11 to A-13\\nmemory address interpretation, \\nA-7 to A-8\\n\\nprocessor performance time, 49\\nSun T1 multithreading unicore \\n\\nperformance, 229\\n\\ncontrol flow instructions, A-37 \\n\\naddition speedup\\n\\naddressing modes for data \\ntransfer, A-34\\nbasic considerations, A-32 to \\n\\nA-33\\n\\nto A-38\\n\\ndata types, A-34\\ndynamic instruction mix, A-41 \\n\\nto A-42, A-42\\nFP operations, A-38 to A-39\\ninstruction format, A-35\\nMIPS operations, A-35 to A-37\\n\\nSun T1 processor, 399\\n\\nInstruction status\\n\\ndynamic scheduling, 177\\nMIPS scoreboarding, C-75\\n\\nInteger arithmetic\\n\\ncarry-lookahead, J-37 to J-41\\ncarry-lookahead circuit, J-38\\ncarry-lookahead tree, J-40\\ncarry-lookahead tree adder, \\n\\ncarry-select adder, J-43, J-43 to \\n\\nJ-41\\n\\nJ-44, J-44\\n\\ncompiler register allocation, A-26 \\n\\nMIPS\\n\\n\\x0cI-36 ■\\n\\nIndex\\n\\nInteger arithmetic (continued )\\n\\ncarry-skip adder, J-41 to J43, \\n\\nJ-42\\n\\noverview, J-37\\n\\ndivision\\n\\nradix-2 division, J-55\\nradix-4 division, J-56\\nradix-4 SRT division, J-57\\nwith single adder, J-54 to J-58\\n\\nFP conversions, J-62\\nlanguage comparison, J-12\\nmultiplication\\n\\narray multiplier, J-50\\nBooth recoding, J-49\\neven/odd array, J-52\\nwith many adders, J-50 to J-54\\nmultipass array multiplier, J-51\\nsigned-digit addition table, \\n\\nJ-54\\n\\nwith single adder, J-47 to J-49, \\n\\nJ-48\\nWallace tree, J-53\\n\\nmultiplication/division, shifting \\n\\nover zeros, J-45 to J-47\\n\\noverflow, J-11\\nRadix-2 multiplication/division, \\nJ-4, J-4 to J-7\\n\\nrestoring/nonrestoring division, \\n\\nripply-carry addition, J-2 to J-3, \\n\\nJ-6\\n\\nJ-3\\n\\nsigned numbers, J-7 to J-10\\nSRT division, J-45 to J-47, J-46\\nsystems issues, J-10 to J-13\\n\\nInteger operand\\n\\nflawed architecture, A-44\\nGCD, 319\\ngraph coloring, A-27\\ninstruction set encoding, A-23\\nMIPS data types, A-34\\nas operand type, 12, A-13 to A-14\\n\\nInteger operations\\n\\naddressing modes, A-11\\nALUs, A-12, C-54\\nARM Cortex-A8, 116, 232, 235, \\n\\n236\\nbenchmarks, 167, C-69\\nbranches, A-18 to A-20, A-20\\ncache misses, 83–84\\ndata access distribution, A-15\\ndata dependences, 151\\ndependences, 322\\n\\ndesktop benchmarks, 38–39\\ndisplacement values, A-12\\nexceptions, C-43, C-45\\nhardware ILP model, 215\\nhardware vs.', ' vector architectures, 282\\n\\nIntel Teraflops processors, OCNs, F-3\\nIntel Thunder Tiger 4 QsNetII, F-63, \\n\\nF-76\\n\\nIntel VT-x, 129\\nIntel x86\\n\\nAmazon Web Services, 456\\nAVX instructions, 284\\nclock rates, 244\\ncomputer architecture, 15\\nconditional instructions, H-27\\nGPUs as coprocessors, 330–331\\nIntel Core i7, 237–238\\nMultimedia SIMD Extensions, \\n282–283\\n\\nNVIDIA GPU ISA, 298\\nparallelism, 262–263\\nperformance and energy \\nefficiency, 241\\n\\nvs.', ' A9, 236\\nARM Cortex-A8 example, 117\\ncache optimization, B-31 to B-33\\ncase study examples, B-60, B-63 to \\n\\nB-64\\n\\ndirectory-based coherence, 418\\nFermi GPU, 306\\nhardware prefetching, 91\\nhit time/power reduction, 79–80\\ninclusion, 397–398, B-34 to B-35\\nIntel Core i7, 118–119, 121–122, \\n123, 124, 124, 239, 241\\ninvalidate protocol, 355, 356–357\\nmemory consistency, 392\\nmemory hierarchy, B-39\\nmiss rates, 376–377\\nmultiprocessor cache coherence, \\n\\n352\\n\\nmultiprogramming workload, 374\\nnonblocking cache, 85\\nNVIDIA GPU Memory, 304\\nOpteron memory, B-57\\nprocessor comparison, 242\\nspeculative execution, 223\\nT1 multithreading unicore \\n\\nperformance, 228\\n\\nvirtual memory, B-48 to B-49\\nL2 caches, see also Second-level \\ncaches\\n\\nARM Cortex-A8, 114, 115–116, \\n\\n235–236\\nARM Cortex-A8 example, 117\\ncache optimization, B-31 to B-33, \\n\\nB-34\\n\\ncase study example, B-63 to B-64\\ncoherency, 352\\ncommercial workloads, 373\\ndirectory-based coherence, 379, \\n\\n418–420, 422, 424\\n\\nfault detection, 58\\nFermi GPU, 296, 306, 308\\nhardware prefetching, 91\\nIBM Blue Gene/L, I-42\\ninclusion, 397–398, B-35\\nIntel Core i7, 118, 120–122, 124, \\n\\n124–125, 239, 241\\n\\ninvalidation protocol, 355, 356–357\\nand ISA, 241\\nmemory consistency, 392\\nmemory hierarchy, B-39, B-48, \\n\\nB-57\\n\\n\\x0cL2 caches (continued )\\n\\nmultithreading, 225, 228\\nnonblocking cache, 85\\nNVIDIA GPU Memory, 304\\nprocessor comparison, 242\\nsnooping coherence, 359–361\\nspeculation, 223\\n\\nL3 caches, see also Third-level caches\\n\\nAlpha 21164 hierarchy, 368\\ncoherence, 352\\ncommercial workloads, 370, 371, \\n\\n374\\n\\ndirectory-based coherence, 379, 384\\nIBM Blue Gene/L, I-42\\nIBM Power processors, 247\\ninclusion, 398\\nIntel Core i7, 118, 121, 124, \\n\\n124–125, 239, 241, \\n403–404\\n\\ninvalidation protocol, 355, \\n\\n356–357, 360\\n\\nmemory access cycle shift, 372\\nmiss rates, 373\\nmulticore processors, 400–401\\nmultithreading, 225\\nnonblocking cache, 83\\nperformance/price/power \\n\\nconsiderations, 52\\n\\nsnooping coherence, 359, 361, 363\\n\\nLabVIEW, embedded benchmarks, \\n\\nE-13\\n\\nLampson, Butler, F-99\\nLanes\\n\\nGPUs vs.', ' multiple thread \\n\\nexecutions, 228\\n\\nmultimedia instruction compiler \\nsupport, A-31\\nNVIDIA GPU Memory structures, \\n\\nOCNs vs.', ' datacenters, 456\\n\\n\\x0cLayer 3 network, array and Internet \\n\\nVMIPS performance, G-17 to \\n\\nLayer 3 network, WSC memory \\n\\nLinux operating systems\\n\\nG-19\\n\\nI-42 ■\\n\\nIndex\\n\\nlinkage, 445\\n\\nhierarchy, 445\\n\\nLCA, see Least common ancestor \\n(LCA)\\nLCD, see Liquid crystal display \\n(LCD)\\n\\nLearning curve, cost trends, 27\\nLeast common ancestor (LCA), \\n\\nrouting algorithms, F-48\\n\\nLeast recently used (LRU)\\n\\nAMD Opteron data cache, B-12, \\n\\nLISP\\n\\nblock replacement, B-9\\nmemory hierarchy history, L-11\\nvirtual memory block replacement, \\n\\nLisp\\n\\nB-14\\n\\nB-45\\n\\n460\\n\\nLess than condition code, PowerPC, \\n\\nLiteral addressing mode, basic \\n\\nK-10 to K-11\\n\\nLevel 3, as Content Delivery Network, \\n\\nconsiderations, A-10 to \\nA-11\\n\\nAmazon Web Services, 456–457\\narchitecture costs, 2\\nprotection and ISA, 112\\nRAID benchmarks, D-22, D-22 to \\n\\nD-23\\n\\nWSC services, 441\\n\\nLiquid crystal display (LCD), Sanyo \\n\\nVPC-SX500 digital \\ncamera, E-19\\n\\nRISC history, L-20\\nSPARC instructions, K-30\\n\\nILP, 215\\nas MapReduce inspiration, 437\\n\\nLittle Endian\\n\\nIntel 80x86, K-49\\ninterconnection networks, F-12\\nmemory address interpretation, \\n\\nA-7\\n\\nMIPS core extensions, K-20 to K-21\\nMIPS data transfers, A-34\\n\\nLivelock, network routing, F-44\\nLiveness, control dependence, 156\\nLivermore Fortran kernels, \\n\\nperformance, 331, L-6\\nLMD, see Load memory data (LMD)\\nLoad instructions\\n\\ncontrol dependences, 155\\ndata hazards requiring stalls, C-20\\ndynamic scheduling, 177\\nILP, 199, 201\\nloop-level parallelism, 318\\nmemory port conflict, C-14\\npipelined cache access, 82\\nRISC instruction set, C-4 to C-5\\nTomasulo’s algorithm, 182\\nVLIW sample code, 252\\n\\nLoad interlocks\\n\\ndefinition, C-37 to C-39\\ndetection logic, C-39\\n\\nLoad linked\\n\\nsynchronization, 388–389\\nLoad locked, synchronization, \\n\\n388–389\\nLoad memory data (LMD), simple \\n\\nMIPS implementation, \\nC-32 to C-33\\n\\nLoad stalls, MIPS R4000 pipeline, \\nC-67\\n\\nLoad-store instruction set architecture\\n\\nbasic concept, C-4 to C-5\\nIBM 360, K-87\\nIntel Core i7, 124\\nIntel 80x86 operations, K-62\\nas ISA, 11\\nISA classification, A-5\\nMIPS nonaligned data transfers, \\nK-24, K-26\\nMIPS operations, A-35 to A-36, \\n\\nA-36\\nPowerPC, K-33\\nRISC history, L-19\\nsimple MIPS implementation, C-32\\nVMIPS, 265\\nLoad/store unit\\n\\nFermi GPU, 305\\nILP hardware model, 215\\nmultiple lanes, 273\\nTomasulo’s algorithm, 171–173, \\n\\n182, 197\\n\\noperations, A-37\\n\\nLocal address space, segmented \\n\\nvirtual memory, B-52\\n\\nLocal area networks (LANs)\\n\\ncharacteristics, F-4\\ncross-company interoperability, F-64\\neffective bandwidth, F-18\\nEthernet as, F-77 to F-79\\nfault tolerance calculations, F-68\\nhistorical overview, F-99 to F-100\\nInfiniBand, F-74\\ninterconnection network domain \\n\\nrelationship, F-4\\n\\nlatency and effective bandwidth, \\nF-26 to F-28\\n\\noffload engines, F-8\\npacket latency, F-13, F-14 to F-16\\nrouters/gateways, F-79\\nshared-media networks, F-23\\nstorage area network history, \\n\\nF-102 to F-103\\n\\nLimit field, IA-32 descriptor table, \\nB-52\\nLine, memory hierarchy basics, 74\\nLinear speedup\\n\\ncost effectiveness, 407\\nIBM eServer p5 multiprocessor, \\n\\nmulticore processors, 400, 402\\nperformance, 405–406\\n\\nLine locking, embedded systems, E-4 \\n\\nto E-5\\n\\nLink injection bandwidth\\ncalculation, F-17\\ninterconnection networks, F-89\\n\\nLink pipelining, definition, F-16\\nLink reception bandwidth, calculation, \\n\\nF-17\\n\\nLink register\\n\\nMIPS control flow instructions, \\nA-37 to A-38\\n\\nPowerPC instructions, K-32 to \\n\\nprocedure invocation options, \\n\\nK-33\\n\\nA-19\\n\\nsynchronization, 389\\n\\nLinpack benchmark\\n\\ncluster history, L-63\\nparallel processing debates, L-58\\nvector processor example, \\n\\n408\\n\\nLittle’s law\\n\\ndefinition, D-24 to D-25\\nserver utilization calculation, D-29\\n\\nvector units, 265, 276–277\\n\\nLoad upper immediate (LUI), MIPS \\n\\n267–268\\n\\nlocks via coherence, 391\\n\\n\\x0cswitches, F-29\\nTCP/IP reliance, F-95\\ntime of flight, F-13\\ntopology, F-30\\n\\nLocality, see Principle of locality\\nLocal Memory\\n\\ncentralized shared-memory \\narchitectures, 351\\n\\ndefinition, 292, 314\\ndistributed shared-memory, 379\\nFermi GPU, 306\\nGrid mapping, 293\\nmultiprocessor architecture, 348\\nNVIDIA GPU Memory structures, \\n304, 304–305\\n\\nSIMD, 315\\nsymmetric shared-memory \\nmultiprocessors, \\n363–364\\n\\nLocal miss rate, definition, B-31\\nLocal node, directory-based cache \\ncoherence protocol \\nbasics, 382\\nLocal optimizations, compilers, A-26\\nLocal predictors, tournament \\n\\npredictors, 164–166\\n\\nLocal scheduling, ILP, VLIW \\n\\nprocessor, 194–195\\n\\nLocks\\n\\nvia coherence, 389–391\\nhardware primitives, 387\\nlarge-scale multiprocessor \\n\\nsynchronization, I-18 to \\nI-21\\nmultiprocessor software \\n\\ndevelopment, 409\\n\\nLock-up free cache, 83\\nLogical units, D-34\\n\\nstorage systems, D-34 to D-35\\n\\nLogical volumes, D-34\\nLong displacement addressing, VAX, \\n\\nK-67\\n\\nLong-haul networks, see Wide area \\n\\nnetworks (WANs)\\n\\nCUDA, 290\\ndefinition, 315–316\\ndependence distance, H-6\\ndependent computation \\n\\nelimination, 321\\n\\nexample calculations, H-4 to H-5\\nGCD, 319\\nloop-level parallelism, H-3\\nas recurrence, 318\\nrecurrence form, H-5\\nVMIPS, 268\\n\\nLoop exit predictor, Intel Core i7, 166\\nLoop interchange, compiler \\n\\noptimizations, 88–89\\n\\nLoop-level parallelism\\ndefinition, 149–150\\ndetection and enhancement\\n\\nbasic approach, 315–318\\ndependence analysis, H-6 to \\n\\nH-10\\n\\ndependence computation \\n\\nelimination, 321–322\\n\\ndependences, locating, \\n318–321\\ndependent computation \\n\\nelimination, H-10 to \\nH-12\\noverview, H-2 to H-6\\n\\nhistory, L-30 to L-31\\nILP in perfect processor, 215\\nILP for realizable processors, \\n217–218\\nLoop stream detection, Intel Core i7 \\n\\nmicro-op buffer, 238\\n\\nLoop unrolling\\n\\nbasic considerations, 161–162\\nILP exposure, 157–161\\nILP limitation studies, 220\\nrecurrences, H-12\\nsoftware pipelining, H-12 to H-15, \\nH-13, H-15\\n\\nTomasulo’s algorithm, 179, \\n181–183\\n\\nVLIW processors, 195\\n\\nLong Instruction Word (LIW)\\n\\nLossless networks\\n\\nEPIC, L-32\\nmultiple-issue processors, L-28, \\n\\nL-30\\n\\nLong integer\\n\\noperand sizes/types, 12\\nSPEC benchmarks, A-14\\n\\nLoop-carried dependences\\n\\ndefinition, F-11 to F-12\\nswitch buffer organizations, F-59\\nLossy networks, definition, F-11 to \\n\\nF-12\\n\\nLRU, see Least recently used (LRU)\\nLucas\\n\\ncompiler optimizations, A-29\\n\\nIndex\\n\\n■\\n\\nI-43\\n\\ndata cache misses, B-10\\n\\nLUI, see Load upper immediate (LUI)\\nLU kernel\\n\\ncharacteristics, I-8\\ndistributed-memory \\n\\nmultiprocessor, I-32\\n\\nsymmetric shared-memory \\n\\nmultiprocessors, I-22, \\nI-23, I-25\\n\\nM\\nMAC, see Multiply-accumulate \\n\\n(MAC)\\n\\nMachine language programmer, L-17 \\nto L-18\\nMachine memory, Virtual Machines, \\n\\n110\\nMacro-op fusion, Intel Core i7, \\n\\n237–238\\n\\nMagnetic storage\\n\\naccess time, D-3\\ncost vs.', ' operation cost, 33\\n\\nMapReduce\\n\\ncloud computing, 455\\ncost calculations, 458–460, 459\\nGoogle usage, 437\\nreductions, 321\\nWSC batch processing, 437–438\\nWSC cost-performance, 474\\n\\nMark-I, L-3 to L-4, L-6\\nMark-II, L-4\\nMark-III, L-4\\nMark-IV, L-4\\nMask Registers\\n\\nbasic operation, 275–276\\ndefinition, 309\\nMultimedia SIMD, 283\\nNVIDIA GPU computational \\n\\nstructures, 291\\n\\nvector compilers, 303\\nvector vs.', ' PMD, 72\\nsystem call virtualization/\\n\\nparavirtualization \\nperformance, 141\\n\\nvirtual machine monitor, 108–109\\nVirtual Machines ISA support, \\n109–110\\nVirtual Machines protection, \\n107–108\\nVirtual Machines and virtual \\nmemory and I/O, \\n110–111\\nvirtual memory protection, \\n105–107\\nVMM on nonvirtualizable ISA, \\n128–129\\n\\nXen VM example, 111\\n\\nMemory Interface Unit\\n\\nNVIDIA GPU ISA, 300\\nvector processor example, 310\\n\\nMemoryless, definition, D-28\\nMemory mapping\\n\\nmemory hierarchy, B-48 to B-49\\nsegmented virtual memory, B-52\\nTLBs, 323\\nvirtual memory definition, B-42\\n\\nMemory-memory instruction set \\n\\narchitecture, ISA \\nclassification, A-3, A-5\\n\\nMemory protection\\n\\ncontrol dependence, 155\\nPentium vs.', ' vector architectures, 310, \\n\\n310–311\\nGrid mapping, 293\\nNVIDIA GPU computational \\n\\nstructures, 291\\nNVIDIA GPU Memory structures, \\n304, 304–305\\n\\nRoofline model, 326\\n\\nMultithreaded vector processor\\n\\ndefinition, 292\\nFermi GPU comparison, 305\\n\\nMultithreading\\n\\ncoarse-grained, 224–226\\ndefinition and types, 223–225\\nfine-grained, 224–226\\nGPU programming, 289\\nhistorical background, L-34 to \\n\\nL-35\\n\\nILP, 223–232\\nmemory hierarchy basics, 75–76\\nparallel benchmarks, 231, 231–232\\nfor performance gains, 398–400\\nSMT, see Simultaneous \\n\\nmultithreading (SMT)\\n\\nSun T1 effectiveness, 226–229\\n\\nMVAPICH, F-77\\nMVL, see Maximum vector length \\n(MVL)\\nMXP processor, components, E-14\\nMyrinet SAN, F-67\\n\\ncharacteristics, F-76\\ncluster history, L-62 to L-63, L-73\\nrouting algorithms, F-48\\nswitch vs.', ' NIC, F-86\\n\\nNetwork technology, see also \\nInterconnection \\nnetworks\\n\\nGoogle WSC, 469\\nperformance trends, 19–20\\npersonal computers, F-2\\ntrends, 18\\nWSC bottleneck, 461\\nWSC goals/requirements, 433\\nNetwork of Workstations, L-62, L-73\\nNEWS communication, see \\n\\nNewton’s iteration, J-27 to J-30\\nNFS, see Network File System (NFS)\\nNIC, see Network interface card (NIC)\\nNicely, Thomas, J-64\\nNMOS, DRAM, 99\\nNoC, see Network on chip (NoC)\\nNodes\\n\\ncoherence maintenance, 381\\ncommunication bandwidth, I-3\\ndirect network topology, F-37\\ndirectory-based cache coherence, \\n\\n380\\n\\ndistributed switched networks, \\n\\nF-34 to F-36\\n\\nIBM Blue Gene/L, I-42 to I-44\\nIBM Blue Gene/L 3D torus \\n\\nvector processor, 310, 310–311, \\n\\nG-25\\n\\nNorth-East-West-South \\n\\ncommunication, \\nnetwork topology \\ncalculations, F-41 to \\nF-43\\n\\nNorth-last routing, F-48\\nNot a Number (NaN), J-14, J-16, J-21, \\n\\nJ-34\\nNotifications, interconnection \\n\\nnetworks, F-10\\n\\nNOW project, L-73\\nNo-write allocate\\n\\ndefinition, B-11\\nexample calculation, B-12\\n\\nnetwork, F-73\\nnetwork topology performance and \\n\\nNSFNET, F-98\\nNTSC/PAL encoder, Sanyo \\n\\ncosts, F-40\\n\\nin parallel, 336\\npoints-to analysis, H-9\\n\\nNokia cell phone, circuit board, E-24\\nNonaligned data transfers, MIPS64, \\n\\nK-24 to K-26\\n\\nNonatomic operations\\n\\ncache coherence, 361\\ndirectory protocol, 386\\nNonbinding prefetch, cache \\n\\noptimization, 93\\n\\nNonblocking caches\\n\\ncache optimization, 83–85, \\n131–133\\n\\neffectiveness, 84\\nILP speculative execution, \\n\\n222–223\\n\\nIntel Core i7, 118\\nmemory hierarchy history, L-11\\nNonblocking crossbar, centralized \\n\\nswitched networks, F-32 \\nto F-33\\n\\nNonfaulting prefetches, cache \\noptimization, 92\\nNonrestoring division, J-5, J-6\\nNonuniform memory access \\n\\n(NUMA)\\n\\nDSM as, 348\\nlarge-scale multiprocessor history, \\n\\nL-61\\n\\nsnooping limitations, 363–364\\n\\nNon-unit strides\\n\\nVPC-SX500 digital \\ncamera, E-19\\n\\nNullification, PA-RISC instructions, \\n\\nNullifying branch, branch delay slots, \\n\\nK-33 to K-34\\n\\nC-24 to C-25\\n\\nNUMA, see Nonuniform memory \\n\\naccess (NUMA)\\n\\nNVIDIA GeForce, L-51\\nNVIDIA systems\\n\\nfine-grained multithreading, 224\\nGPU comparisons, 323–330, \\n\\n325\\n\\nGPU computational structures, \\n291–297\\nGPU computing history, L-52\\nGPU ISA, 298–300\\nGPU Memory structures, 304, \\n304–305\\n\\nGPU programming, 289\\ngraphics pipeline history, L-51\\nscalable GPUs, L-51\\nterminology, 313–315\\n\\nN-way set associative\\n\\nblock placement, B-7\\nconflict misses, B-23\\nmemory hierarchy basics, 74\\nTLBs, B-49\\n\\nNYU Ultracomputer, L-60\\n\\nO\\nObserved performance, fallacies, 57\\nOccupancy, communication \\nbandwidth, I-3\\n\\nNorth-East-West-South \\ncommunication\\n\\nmultidimensional arrays in vector \\narchitectures, 278–279\\n\\n\\x0cOcean application\\n\\ncharacteristics, I-9 to I-10\\ndistributed-memory \\n\\nmultiprocessor, I-32\\n\\noptimization, 79\\nSRAM, 98–99\\n\\nOn-chip memory, embedded systems, \\nE-4 to E-5\\n\\ndistributed-memory \\n\\nOn-chip networks (OCNs)\\n\\nmultiprocessors, I-30\\n\\nexample calculations, I-11 to I-12\\nmiss rates, I-28\\nsymmetric shared-memory \\n\\nbasic considerations, F-3\\ncommercial implementations, F-73\\ncommercial interconnection \\n\\nnetworks, F-63\\n\\nOperands\\n\\nmultiprocessors, I-23\\n\\ncross-company interoperability, \\n\\nOCNs, see On-chip networks (OCNs)\\nOffline reconstruction, RAID, D-55\\nOffload engines\\n\\nnetwork interfaces, F-8\\nTCP/IP reliance, F-95\\n\\nOffset\\n\\nF-64\\n\\nDOR, F-46\\neffective bandwidth, F-18, \\nF-28\\n\\nexample system, F-70 to F-72\\nhistorical overview, F-103 to \\n\\ninterconnection network domain \\n\\nrelationship, F-4\\n\\ninterconnection network speed, \\n\\nto C-53\\n\\nF-104\\n\\nF-88\\n\\naddressing modes, 12\\nAMD64 paged virtual memory, \\n\\nB-55\\n\\nblock identification, B-7 to B-8\\ncache optimization, B-38\\ncall gates, B-54\\ncontrol flow instructions, A-18\\ndirectory-based cache coherence \\n\\nprotocols, 381–382\\n\\nexample, B-9\\ngather-scatter, 280\\nIA-32 segment, B-53\\ninstruction decode, C-5 to C-6\\nmain memory, B-44\\nmemory mapping, B-52\\nMIPS, C-32\\nMIPS control flow instructions, \\nA-37 to A-38\\n\\nmisaligned addresses, A-8\\nOpteron data cache, B-13 to B-14\\npipelining, C-42\\nPTX instructions, 300\\nRISC, C-4 to C-6\\nRISC instruction set, C-4\\nTLB, B-46\\nTomasulo’s approach, 176\\nvirtual memory, B-43 to B-44, \\nB-49, B-55 to B-56\\nOLTP, see On-Line Transaction \\nProcessing (OLTP)\\n\\nOmega\\n\\nexample, F-31\\npacket blocking, F-32\\ntopology, F-30\\n\\nOMNETPP, Intel Core i7, 240–241\\nOn-chip cache\\n\\nlatency and effective bandwidth, \\nF-26 to F-28\\nlatency vs.', ' nodes, F-27\\nlink bandwidth, F-89\\npacket latency, F-13, F-14 to F-16\\nswitch microarchitecture, F-57\\ntime of flight, F-13\\ntopology, F-30\\nwormhole switching, F-51\\n\\nOne’s complement, J-7\\nOne-way conflict misses, definition, \\n\\nB-23\\nOnline reconstruction, RAID, D-55\\nOn-Line Transaction Processing \\n\\n(OLTP)\\n\\ncommercial workload, 369, 371\\nserver benchmarks, 41\\nshared-memory workloads, \\n\\n368–370, 373–374\\n\\nstorage system benchmarks, D-18\\n\\nOpenCL\\n\\nGPU programming, 289\\nGPU terminology, 292, 313–315\\nNVIDIA terminology, 291\\nprocessor comparisons, 323\\n\\nOpenGL, L-51\\nOpen source software\\n\\nAmazon Web Services, 457\\nWSCs, 437\\nXen VMM, see Xen virtual \\nmachine\\n\\nIndex\\n\\n■\\n\\nI-53\\n\\nOpen Systems Interconnect (OSI)\\n\\nEthernet, F-78 to F-79\\nlayer definitions, F-82\\n\\nOperand addressing mode, Intel \\n\\n80x86, K-59, K-59 to \\nK-60\\nOperand delivery stage, Itanium 2, \\nH-42\\n\\nDSP, E-6\\nforwarding, C-19\\ninstruction set encoding, A-21 to \\n\\nA-22\\n\\nIntel 80x86, K-59\\nISA, 12\\nISA classification, A-3 to A-4\\nMIPS data types, A-34\\nMIPS pipeline, C-71\\nMIPS pipeline FP operations, C-52 \\n\\nNVIDIA GPU ISA, 298\\nper ALU instruction example, A-6\\nTMS320C55 DSP, E-6\\ntype and size, A-13 to A-14\\nVAX, K-66 to K-68, K-68\\nvector execution time, 268–269\\n\\nOperating systems (general)\\naddress translation, B-38\\nand architecture development, 2\\ncommunication performance, F-8\\ndisk access scheduling, D-44 to \\nD-45, D-45\\nmemory protection performance, \\n\\nB-58\\n\\nmiss statistics, B-59\\nmultiprocessor software \\n\\ndevelopment, 408\\n\\nand page size, B-58\\nsegmented virtual memory, B-54\\nserver benchmarks, 40\\nshared-memory workloads, \\n374–378\\n\\nstorage systems, D-35\\n\\nOperational costs\\n\\nbasic considerations, 33\\nWSCs, 434, 438, 452, 456, 472\\nOperational expenditures (OPEX)\\nWSC costs, 452–455, 454\\nWSC TCO case study, 476–478\\nOperation faults, storage systems, D-11\\nOperator dependability, disks, D-13 to \\n\\nD-15\\n\\n\\x0cI-54 ■\\n\\nIndex\\n\\nOPEX, see Operational expenditures \\n(OPEX)\\n\\nOptical media, interconnection \\n\\nnetworks, F-9\\n\\nOracle database\\n\\ncommercial workload, 368\\nmiss statistics, B-59\\nmultithreading benchmarks, 232\\nsingle-threaded benchmarks, 243\\nWSC services, 441\\n\\nOrdering, and deadlock, F-47\\nOrganization\\n\\nbuffer, switch microarchitecture, \\nF-58 to F-60\\n\\ncache, performance impact, \\nB-19\\ncache blocks, B-7 to B-8\\ncache optimization, B-19\\ncoherence extensions, 362\\ncomputer architecture, 11, 15–16\\nDRAM, 98\\nMIPS pipeline, C-37\\nmultiple-issue processor, 197, 198\\nOpteron data cache, B-12 to B-13, \\n\\npipelines, 152\\nprocessor history, 2–3\\nprocessor performance equation, \\n\\nshared-memory multiprocessors, \\n\\nB-13\\n\\n49\\n\\n346\\n\\nE-18\\n\\nTLB, B-46\\n\\nOrthogonality, compiler \\n\\nwriting-architecture \\nrelationship, A-30\\n\\nOSI, see Open Systems Interconnect \\n\\n(OSI)\\n\\nOut-of-order completion\\ndata hazards, 169\\nMIPS pipeline, C-71\\nMIPS R100000 sequential \\nconsistency, 397\\n\\nprecise exceptions, C-58\\n\\nOut-of-order execution\\n\\nand cache miss, B-2 to B-3\\ncache performance, B-21\\ndata hazards, 169–170\\nhardware-based execution, 184\\nILP, 245\\nmemory hierarchy, B-2 to B-3\\n\\nmicroarchitectural techniques case \\nstudy, 247–254\\n\\nMIPS pipeline, C-71\\nmiss penalty, B-20 to B-22\\nperformance milestones, 20\\npower/DLP issues, 322\\nprocessor comparisons, 323\\nR10000, 397\\nSMT, 246\\nTomasulo’s algorithm, 183\\n\\nOut-of-order processors\\n\\nDLP, 322\\nIntel Core i7, 236\\nmemory hierarchy history, L-11\\nmultithreading, 226\\nvector architecture, 267\\nOut-of-order write, dynamic \\nscheduling, 171\\n\\nOutput buffered switch\\nHOL blocking, F-60\\nmicroarchitecture, F-57, F-57\\norganizations, F-58 to F-59\\npipelined version, F-61\\n\\nOutput dependence\\n\\ncompiler history, L-30 to L-31\\ndefinition, 152–153\\ndynamic scheduling, 169–171, C-72\\nfinding, H-7 to H-8\\nloop-level parallelism calculations, \\n\\n320\\nMIPS scoreboarding, C-79\\n\\nmicroprocessors, 26\\nprocessor performance equation, \\n\\n52\\n\\nOverflow, integer arithmetic, J-8, J-10 \\nto J-11, J-11\\n\\nOverflow condition code, MIPS core, \\nK-9 to K-16\\n\\nOverhead\\n\\nadaptive routing, F-93 to F-94\\nAmdahl’s law, F-91\\ncommunication latency, I-4\\ninterconnection networks, F-88, \\nF-91 to F-92\\nOCNs vs.', ' vector architectures, 308\\nNVIDIA GPU ISA, 298–300\\nNVIDIA GPU Memory structures, \\n\\n305\\nParallel Thread Execution (PTX) \\n\\nInstruction\\n\\nCUDA Thread, 300\\ndefinition, 292, 309, 313\\nGPU conditional branching, 302–303\\nGPU terms, 308\\nNVIDIA GPU ISA, 298, 300\\n\\nsystem call performance, 141\\nXen VM, 111\\n\\nParity\\n\\ndirty bits, D-61 to D-64\\nfault detection, 58\\nmemory dependability, 104–105\\nWSC memory, 473–474\\n\\nPARSEC benchmarks\\n\\nIntel Core i7, 401–405\\nSMT on superscalar processors, \\n230–232, 231\\n\\nspeedup without SMT, 403–404\\nPartial disk failure, dirty bits, D-61 to \\n\\nD-64\\n\\nPartial store order, relaxed consistency \\nmodels, 395\\nPartitioned add operation, DSP media \\n\\nextensions, E-10\\n\\nPartitioning\\n\\nMultimedia SIMD Extensions, 282\\nvirtual memory protection, B-50\\nWSC memory hierarchy, 445\\n\\n217–218, 315–322\\n\\nParavirtualization\\n\\nPage tables\\n\\ntrace scheduling, H-19 to H-21, \\n\\n\\x0cstorage area network history, \\n\\naverage memory access time, \\n\\ninterprocessor communication, I-3 \\n\\nPascal programs\\n\\nPerformability, RAID reconstruction, \\n\\nD-55 to D-57\\nPerformance, see also Peak \\n\\nI-56 ■\\n\\nIndex\\n\\ncompiler types and classes, A-28\\ninteger division/remainder, J-12\\nPattern, disk array deconstruction, D-51\\nPayload\\n\\nmessages, F-6\\npacket format, F-7\\n\\np bits, J-21 to J-23, J-25, J-36 to J-37\\nPC, see Program counter (PC)\\nPCI bus, historical background, L-81\\nPCIe, see PCI-Express (PCIe)\\nPCI-Express (PCIe), F-29, F-63\\n\\nstorage area network history, \\n\\nF-102 to F-103\\n\\nPCI-X, F-29\\n\\nF-102\\n\\nPCI-X 2.', ' datacenters, 456\\nWSC server energy efficiency, 462\\n\\nPrecise exceptions\\ndefinition, C-47\\ndynamic scheduling, 170\\nhardware-based speculation, \\n\\n187–188, 221\\ninstruction set complications, C-49\\nmaintaining, C-58 to C-60\\nMIPS exceptions, C-48\\n\\nPrecisions, floating-point arithmetic, \\n\\nJ-33 to J-34\\n\\nPredicated instructions\\n\\nexposing parallelism, H-23 to H-27\\nIA-64, H-38 to H-40\\n\\nIndex\\n\\n■\\n\\nI-59\\n\\nPredicate Registers\\ndefinition, 309\\nGPU conditional branching, 300–301\\nIA-64, H-34\\nNVIDIA GPU ISA, 298\\nvectors vs.', ' GPUs, 311\\n\\nPredication, TI TMS320C6x DSP, E-10\\nPredicted-not-taken scheme\\n\\nbranch penalty reduction, C-22, \\nC-22 to C-23\\n\\nMIPS R4000 pipeline, C-64\\nPredictions, see also Mispredictions\\naddress aliasing, 213–214, 216\\nbranch\\n\\ncorrelation, 162–164\\ncost reduction, 162–167, C-26\\ndynamic, C-27 to C-30\\nideal processor, 214\\nILP exploitation, 201\\ninstruction fetch bandwidth, 205\\nintegrated instruction fetch \\n\\nunits, 207\\n\\nIntel Core i7, 166–167, 239–241\\nstatic, C-26 to C-27\\n\\nbranch-prediction buffers, C-27 to \\nC-30, C-29\\njump prediction, 214\\nPMDs, 6\\nreturn address buffer, 207\\n2-bit scheme, C-28\\nvalue prediction, 202, 212–213\\n\\nIntel Core i7, 122, 123–124\\nItanium 2, H-42\\nMIPS core extensions, K-20\\nNVIDIA GPU Memory structures, \\n\\n208\\n\\n305\\n\\nparallel processing challenges, 351\\nPrefix, Intel 80x86 integer operations, \\n\\nK-51\\nPresentation layer, definition, F-82\\nPresent bit, IA-32 descriptor table, \\nB-52\\n\\nPrice vs.', ' MIMD, 329\\nlocks via coherence, 391\\noperand types and sizes, A-14 to \\n\\nA-15\\n\\nK-35\\nsynchronization, 394, L-64\\n\\nPrinciple of locality\\n\\nbidirectional MINs, F-33 to F-34\\ncache optimization, B-26\\ncache performance, B-3 to B-4\\ncoining of term, L-11\\ncommercial workload, 373\\ncomputer design principles, 45\\ndefinition, 45, B-2\\nlock accesses, 390\\nLRU, B-9\\nmemory accesses, 332, B-46\\nmemory hierarchy design, 72\\nmultilevel application, B-2\\nmultiprogramming workload, 375\\nscientific workloads on symmetric \\nshared-memory \\nmultiprocessors, I-25\\n\\nstride, 278\\nWSC bottleneck, 461\\nWSC efficiency, 450\\n\\nPrivate data\\n\\ncache protocols, 359\\ncentralized shared-memory \\nmultiprocessors, \\n351–352\\n\\ndefinition, 292, 314\\nNVIDIA GPU Memory structures, \\n\\n304\\nPrivate variables, NVIDIA GPU \\n\\nMemory, 304\\n\\nProcedure calls\\n\\ncompiler structure, A-25 to A-26\\ncontrol flow instructions, A-17, \\nA-19 to A-21\\n\\ndependence analysis, 321\\n\\nhigh-level instruction set, A-42 to \\n\\nA-43\\nIA-64 register model, H-33\\ninvocation options, A-19\\nISAs, 14\\nMIPS control flow instructions, A-38\\nreturn address predictors, 206\\nVAX, B-73 to B-74, K-71 to K-72\\nVAX vs.', ' cache performance, B-16\\nclock rate trends, 24\\ndesktop benchmarks, 38, 40\\nhistorical trends, 3, 3–4\\nmultiprocessors, 347\\nuniprocessors, 344\\n\\ncomputer design \\nprinciples, 48–52\\n\\nProcessor speed\\n\\nand clock rate, 244\\nand CPI, 244\\nsnooping cache coherence, 364\\n\\nmultithreading, 224\\nPID, B-37\\nvirtual memory-based protection, \\nB-49 to B-50\\n\\nProducer-server model, response time \\n\\nand throughput, D-16\\n\\nProductivity\\n\\nCUDA, 290–291\\nNVIDIA programmers, 289\\nsoftware development, 4\\nvirtual memory and programming, \\n\\nB-41\\n\\nWSC, 450\\n\\nProfile-based predictor, misprediction \\nrate, C-27\\n\\nProgram counter (PC)\\n\\naddressing modes, A-10\\nARM Cortex-A8, 234\\nbranch hazards, C-21\\nbranch-target buffers, 203, \\n\\n203–204, 206\\ncontrol flow instruction addressing \\nmodes, A-17\\ndynamic branch prediction, C-27 \\n\\nexception stopping/restarting, C-46 \\n\\nto C-28\\n\\nto C-47\\n\\nGPU conditional branching, 303\\nIntel Core i7, 120\\nM32R instructions, K-39\\nMIPS control flow instructions, \\n\\nA-38\\nmultithreading, 223–224\\npipeline branch issues, C-39 to \\n\\nC-41\\n\\npipe stages, C-35\\nprecise exceptions, C-59 to C-60\\nRISC classic pipeline, C-8\\nRISC instruction set, C-5\\nsimple MIPS implementation, \\n\\nC-31 to C-33\\n\\nTLP, 344\\nvirtual memory protection, 106\\nProgram counter-relative addressing\\n\\ncontrol flow instructions, A-17 to \\nA-18, A-21\\n\\ndefinition, A-10\\nMIPS instruction format, A-35\\n\\nProcess switch\\n\\nProgramming models\\n\\ndefinition, 106, B-49\\nmiss rate vs.', ' servers, 434\\n\\nScalable GPUs, historical background, \\nL-50 to L-51\\nScalar expansion, loop-level parallelism \\n\\ndependences, 321\\n\\nScalar Processors, see also \\n\\nSuperscalar processors\\n\\ndefinition, 292, 309\\nearly pipelined CPUs, L-26 to L-27\\nlane considerations, 273\\nMultimedia SIMD/GPU \\n\\ncomparisons, 312\\n\\nNVIDIA GPU, 291\\nprefetch units, 277\\nvs.', ' switched-media \\n\\nnetworks, F-25\\ntransistor performance and wires, \\n\\n19–21\\n\\nVMIPS, 267\\n\\nScan Line Interleave (SLI), scalable \\n\\nGPUs, L-51\\n\\nSCCC, see Intel Single-Chip Cloud \\n\\nComputing (SCCC)\\n\\nSchorr, Herb, L-28\\nScientific applications\\nBarnes, I-8 to I-9\\nbasic characteristics, I-6 to I-7\\ncluster history, L-62\\ndistributed-memory \\n\\nmultiprocessors, I-26 to \\nI-32, I-28 to I-32\\n\\nFFT kernel, I-7\\nLU kernel, I-8\\nOcean, I-9 to I-10\\nparallel processors, I-33 to I-34\\nparallel program computation/\\n\\ncommunication, I-10 to \\nI-12, I-11\\n\\nparallel programming, I-2\\nsymmetric shared-memory \\n\\nmultiprocessors, I-21 to \\nI-26, I-23 to I-25\\n\\nARM Cortex-A8, 233, 234\\ncomponents, C-76\\ndefinition, 170\\ndynamic scheduling, 171, 175\\nand dynamic scheduling, C-71 to \\n\\nC-80\\nexample calculations, C-77\\nMIPS structure, C-73\\nNVIDIA GPU, 296\\nresults tables, C-78 to C-79\\nSIMD thread scheduler, 296\\n\\nScoreboarding\\n\\n\\x0cScripting languages, software \\n\\ndevelopment impact, 4\\n\\nsafe calls, B-54\\nsharing and protection, B-52 to \\n\\nSCSI (Small Computer System \\n\\nB-53\\n\\nBerkeley’s Tertiary Disk project, \\n\\nJ-28 to J-29\\n\\n353\\n\\nI-66 ■\\n\\nIndex\\n\\nInterface)\\n\\nD-12\\n\\ndependability benchmarks, D-21\\ndisk storage, D-4\\nhistorical background, L-80 to L-81\\nI/O subsystem design, D-59\\nRAID reconstruction, D-56\\nstorage area network history, \\n\\nF-102\\n\\nSDRAM, see Synchronous dynamic \\n\\nrandom-access memory \\n(SDRAM)\\n\\nSDRWAVE, J-62\\nSecond-level caches, see also L2 \\ncaches\\n\\nARM Cortex-A8, 114\\nILP, 245\\nIntel Core i7, 121\\ninterconnection network, F-87\\nItanium 2, H-41\\nmemory hierarchy, B-48 to B-49\\nmiss penalty calculations, B-33 to \\n\\nmiss penalty reduction, B-30 to \\n\\nmiss rate calculations, B-31 to \\n\\nB-34\\n\\nB-35\\n\\nB-35\\n\\nand relative execution time, B-34\\nspeculation, 210\\nSRAM, 99\\n\\nSecure Virtual Machine (SVM), 129\\nSeek distance\\n\\nstorage disks, D-46\\nsystem comparison, D-47\\nSeek time, storage disks, D-46\\nSegment basics\\n\\nIntel 80x86, K-50\\nvs.', ' vector architectures, \\n308–309\\n\\nhistorical overview, L-55 to L-56\\nloop-level parallelism, 150\\nMapReduce, 438\\nmemory bandwidth, 332\\nmultimedia extensions, see \\n\\nMultimedia SIMD \\nExtensions\\nmultiprocessor architecture, 346\\nmultithreaded, see Multithreaded \\n\\nSIMD Processor\\n\\nNVIDIA GPU computational \\n\\nstructures, 291\\nNVIDIA GPU ISA, 300\\npower/DLP issues, 322\\nspeedup via parallelism, 263\\nsupercomputer development, L-43 \\n\\nto L-44\\n\\nsystem area network history, F-100\\nThread Block mapping, 293\\nTI 320C6x DSP, E-9\\n\\nSIMD Instruction\\n\\nCUDA Thread, 303\\ndefinition, 292, 313\\nDSP media extensions, E-10\\nfunction, 150, 291\\nGPU Memory structures, 304\\nGPUs, 300, 305\\nGrid mapping, 293\\nIBM Blue Gene/L, I-42\\nIntel AVX, 438\\nmultimedia architecture \\n\\nprogramming, 285\\n\\nmultimedia extensions, 282–285, \\n\\n312\\n\\nmultimedia instruction compilers, \\nA-31 to A-32\\n\\nMultithreaded SIMD Processor \\n\\nblock diagram, 294\\n\\nPTX, 301\\nSony PlayStation 2, E-16\\nThread of SIMD Instructions, \\n295–296\\n\\nthread scheduling, 296–297, 297, \\n\\n305\\n\\nvector architectures as superset, \\n\\n263–264\\nvector/GPU comparison, 308\\n\\nVector Registers, 309\\n\\nSIMD Lane Registers, definition, 309, \\n\\n314\\n\\nSIMD Lanes\\n\\ndefinition, 292, 296, 309\\nDLP, 322\\nFermi GPU, 305, 307\\nGPU, 296–297, 300, 324\\nGPU conditional branching, \\n302–303\\n\\nGPUs vs.', ' GPUs, 312, \\n\\n315\\n\\nmultithreaded processor, 294\\nNVIDIA GPU Memory, 304\\nsynchronization marker, 301\\nvector vs.', ' GPU, 312\\nmultiprocessor architecture, 346\\nNVIDIA GPU computational \\n\\nstructures, 291\\nNVIDIA GPU Memory structures, \\n\\n304–305\\nprocessor comparisons, 324\\nRoofline model, 287, 326\\nsystem area network history, F-100\\n\\nSIMD Thread\\n\\nGPU conditional branching, \\n301–302\\nGrid mapping, 293\\nMultithreaded SIMD processor, \\n\\n294\\n\\nNVIDIA GPU, 296\\nNVIDIA GPU ISA, 298\\nNVIDIA GPU Memory structures, \\n\\n305\\nscheduling example, 297\\nvector vs.', ' datacenter costs, 455\\nWSCs, 442–443\\n\\nDAXPY on VMIPS, G-20\\ndefinition, 292\\nmultidimensional arrays, 278\\nThread Block comparison, 294\\nvector-length registers, 274\\n\\nStore conditional\\n\\nStrip mining\\n\\nlocks via coherence, 391\\nsynchronization, 388–389\\n\\nStore-and-forward packet switching, \\n\\nStore instructions, see also Load-store \\n\\nF-51\\n\\ninstruction set \\narchitecture\\n\\ndefinition, C-4\\ninstruction execution, 186\\nISA, 11, A-3\\nMIPS, A-33, A-36\\nNVIDIA GPU ISA, 298\\nOpteron data cache, B-15\\nRISC instruction set, C-4 to C-6, \\n\\nC-10\\nvector architectures, 310\\n\\nStreaming Multiprocessor\\n\\ndefinition, 292, 313–314\\nFermi GPU, 307\\nStrecker, William, K-65\\nStrided accesses\\n\\nMultimedia SIMD Extensions, 283\\nRoofline model, 287\\nTLB interaction, 323\\n\\nStrided addressing, see also Unit stride \\naddressing\\nmultimedia instruction compiler \\nsupport, A-31 to A-32\\n\\nStrides\\n\\ngather-scatter, 280\\nhighly parallel memory systems, \\n\\nmultidimensional arrays in vector \\narchitectures, 278–279\\n\\nNVIDIA GPU ISA, 300\\nvector memory systems, G-10 to \\n\\n133\\n\\nG-11\\n\\nK-53\\n\\nDAXPY on VMIPS, G-20\\nGPU conditional branching, 303\\nGPUs vs.', ' vector architectures, 311\\nNVIDIA GPU, 291\\nvector, 275\\nVLRs, 274–275\\n\\nStrong scaling, Amdahl’s law and \\n\\nparallel computers, 407\\n\\nStructural hazards\\n\\nbasic considerations, C-13 to C-16\\ndefinition, C-11\\nMIPS pipeline, C-71\\nMIPS scoreboarding, C-78 to C-79\\npipeline stall, C-15\\nvector execution time, 268–269\\n\\nStructural stalls, MIPS R4000 \\n\\npipeline, C-68 to C-69\\nSubset property, and inclusion, 397\\nSummary overflow condition code, \\n\\nPowerPC, K-10 to K-11\\n\\nSun Microsystems\\n\\ncache optimization, B-38\\nfault detection pitfalls, 58\\nmemory dependability, 104\\nSun Microsystems Enterprise, L-60\\nSun Microsystems Niagara (T1/T2) \\n\\ncharacteristics, 227\\nCPI and IPC, 399\\nfine-grained multithreading, 224, \\n225, 226–229\\n\\nmanufacturing cost, 62\\nmulticore processor performance, \\n\\n400–401\\n\\nmultiprocessing/\\n\\nmultithreading-based \\nperformance, 398–400\\n\\nIndex\\n\\n■\\n\\nI-73\\n\\nconditional branches, K-10, \\nK-17\\n\\nconditional instructions, H-27\\nconstant extension, K-9\\nconventions, K-13\\ndata transfer instructions, K-10\\nfast traps, K-30\\nfeatures, K-44\\nFP instructions, K-23\\ninstruction list, K-31 to K-32\\ninteger arithmetic, J-12\\ninteger overflow, J-11\\nISA, A-2\\nLISP, K-30\\nMIPS core extensions, K-22 to K-23\\noverlapped integer/FP operations, \\n\\nK-31\\nprecise exceptions, C-60\\nregister windows, K-29 to K-30\\nRISC history, L-20\\nas RISC system, K-4\\nSmalltalk, K-30\\nsynchronization history, L-64\\nunique instructions, K-29 to K-32\\nSun Microsystems SPARCCenter, L-60\\nSun Microsystems SPARCstation-2, \\n\\nF-88\\n\\nSun Microsystems SPARCstation-20, \\n\\nF-88\\nSun Microsystems SPARC V8, \\n\\nfloating-point \\nprecisions, J-33\\n\\ncharacteristics, K-18\\nmultimedia support, E-11, K-18\\n\\nSun Microsystems Ultra 5, \\n\\nSPECfp2000 execution \\ntimes, 43\\nSun Microsystems UltraSPARC, L-62, \\n\\nL-73\\n\\nSun Microsystems UltraSPARC T1 \\n\\nprocessor, \\ncharacteristics, F-73\\n\\nVMIPS, 266\\n\\nString operations, Intel 80x86, K-51, \\n\\nmultithreading history, L-34\\nT1 multithreading unicore \\n\\nSun Modular Datacenter, L-74 to L-75\\nSuperblock scheduling\\n\\nStripe, disk array deconstruction, D-51\\nStriping\\n\\ndisk arrays, D-6\\nRAID, D-9\\n\\nStrip-Mined Vector Loop\\n\\nconvoys, G-5\\n\\nperformance, 227–229\\n\\nSun Microsystems SPARC\\naddressing modes, K-5\\nALU operands, A-6\\narithmetic/logical instructions, \\nK-11, K-31\\n\\nbranch conditions, A-19\\n\\nbasic process, H-21 to H-23\\ncompiler history, L-31\\nexample, H-22\\n\\nSupercomputers\\n\\ncommercial interconnection \\n\\nnetworks, F-63\\n\\ndirect network topology, F-37\\n\\nprocessors\\n\\nSun Microsystems SPARC VIS\\n\\n\\x0csubstitution logic, 251\\n\\nSwitching\\n\\nI-74 ■\\n\\nIndex\\n\\nSupercomputers (continued)\\n\\nlow-dimensional topologies, F-100\\nSAN characteristics, F-76\\nSIMD, development, L-43 to L-44\\nvs.', ' Flash memory, 103\\nIBM Blue Gene/L, I-42\\nIntel Core i7, 121\\nperformance, 100\\npower consumption, 102, 103\\nSDRAM timing diagram, 139\\n\\nSynchronous event, exception \\n\\nrequirements, C-44 to \\nC-45\\nSynchronous I/O, definition, D-35\\nSynonyms\\n\\naddress translation, B-38\\ndependability, 34\\nSynthetic benchmarks\\ndefinition, 37\\ntypical program fallacy, A-43\\nSystem area networks, historical \\noverview, F-100 to \\nF-102\\n\\nSystem calls\\n\\nCUDA Thread, 297\\nmultiprogrammed workload, 378\\nvirtualization/paravirtualization \\n\\nperformance, 141\\n\\nvirtual memory protection, 106\\nSystem interface controller (SIF), Intel \\nSCCC, F-70\\n\\nSystem-on-chip (SoC)\\ncell phone, E-24\\ncross-company interoperability, \\n\\nF-64\\nembedded systems, E-3\\nSanyo digital cameras, E-20\\nSanyo VPC-SX500 digital camera, \\n\\nE-19\\n\\nshared-media networks, F-23\\nSystem Performance and Evaluation \\nCooperative (SPEC), \\nsee SPEC benchmarks\\n\\nSystem Processor\\ndefinition, 309\\nDLP, 262, 322\\nFermi GPU, 306\\nGPU issues, 330\\nGPU programming, 288–289\\nNVIDIA GPU ISA, 298\\nNVIDIA GPU Memory, 305\\nprocessor comparisons, 323–324\\nsynchronization, 329\\nvector vs.', ' start-up \\n\\noverhead, 331\\n\\nL-57 to L-58\\n\\nTFT, see Thin-film transistor (TFT)\\nThacker, Chuck, F-99\\nThermal design power (TDP), power \\ntrends, 22\\n\\nThin-film transistor (TFT), Sanyo \\nVPC-SX500 digital \\ncamera, E-19\\n\\nThinking Machines, L-44, L-56\\nThinking Multiprocessors CM-5, L-60\\n\\nThink time, transactions, D-16, D-17\\nThird-level caches, see also L3 caches\\n\\nILP, 245\\ninterconnection network, F-87\\nSRAM, 98–99\\n\\nThrash, memory hierarchy, B-25\\nThread Block\\n\\nCUDA Threads, 297, 300, 303\\ndefinition, 292, 313\\nFermi GTX 480 GPU flooplan, \\n\\n295\\nfunction, 294\\nGPU hardware levels, 296\\nGPU Memory performance, 332\\nGPU programming, 289–290\\nGrid mapping, 293\\nmapping example, 293\\nmultithreaded SIMD Processor, 294\\nNVIDIA GPU computational \\n\\nstructures, 291\\nNVIDIA GPU Memory structures, \\n\\n304\\n\\nPTX Instructions, 298\\n\\nThread Block Scheduler\\n\\ndefinition, 292, 309, 313–314\\nFermi GTX 480 GPU flooplan, 295\\nfunction, 294, 311\\nGPU, 296\\nGrid mapping, 293\\nmultithreaded SIMD Processor, 294\\n\\nThread-level parallelism (TLP)\\n\\nadvanced directory protocol case \\n\\nstudy, 420–426\\nAmdahl’s law and parallel \\n\\ncomputers, 406–407\\n\\ncentralized shared-memory \\n\\nmultiprocessors\\nbasic considerations, 351–352\\ncache coherence, 352–353\\ncache coherence enforcement, \\n\\n354–355\\n\\n357–362\\n\\n362–363\\n\\ncache coherence extensions, \\n\\ninvalidate protocol \\n\\nimplementation, \\n356–357\\n\\nSMP and snooping limitations, \\n\\n363–364\\n\\nsnooping coherence \\n\\nimplementation, 365–366\\n\\nTFLOPS, parallel processing debates, \\n\\ncache coherence example, \\n\\nsnooping coherence protocols, \\n\\n355–356\\n\\ndefinition, 9\\ndirectory-based cache coherence\\n\\ncase study, 418–420\\nprotocol basics, 380–382\\nprotocol example, 382–386\\n\\nDSM and directory-based \\n\\ncoherence, 378–380\\n\\nembedded systems, E-15\\nIBM Power7, 215\\nfrom ILP, 4–5\\ninclusion, 397–398\\nIntel Core i7 performance/energy \\n\\nefficiency, 401–405\\n\\nmemory consistency models\\n\\nbasic considerations, 392–393\\ncompiler optimization, 396\\nprogramming viewpoint, \\n\\nrelaxed consistency models, \\n\\n393–394\\n\\n394–395\\n\\nspeculation to hide latency, \\n\\n396–397\\nMIMDs, 344–345\\nmulticore processor performance, \\n\\n400–401\\nmulticore processors and SMT, \\n404–405\\n\\nmultiprocessing/\\n\\nmultithreading-based \\nperformance, 398–400\\n\\nmultiprocessor architecture, \\n346–348\\n\\nmultiprocessor cost effectiveness, 407\\nmultiprocessor performance, \\n405–406\\n\\nmultiprocessor software \\n\\ndevelopment, 407–409\\n\\nvs.', ' vector architectures, 310\\nmultimedia instruction compiler \\nsupport, A-31\\nNVIDIA GPU ISA, 300\\nRoofline model, 287\\n\\nUNIVAC I, L-5\\nUNIX systems\\n\\narchitecture costs, 2\\nblock servers vs.', ' filers, D-35\\ncache optimization, B-38\\nfloating point remainder, J-32\\nmiss statistics, B-59\\nmultiprocessor software \\n\\naddress translation, B-46\\nsegmented virtual memory, B-52\\nvirtual memory block replacement, \\n\\nB-45\\n\\nF-8\\n\\nUser-level communication, definition, \\n\\nUser maskable events, definition, C-45 \\n\\nto C-46\\nUser nonmaskable events, definition, \\n\\nC-45\\nUser-requested events, exception \\nrequirements, C-45\\n\\nUtility computing, 455–461, L-73 to \\n\\nL-74\\n\\nI/O system calculations, D-26\\nqueuing theory, D-25\\n\\nUTP, see Unshielded twisted pair \\n(UTP)\\n\\nV\\nValid bit\\n\\naddress translation, B-46\\nblock identification, B-7\\nOpteron data cache, B-14\\npaged virtual memory, B-56\\nsegmented virtual memory, B-52\\nsnooping, 357\\nsymmetric shared-memory \\n\\nmultiprocessors, 366\\n\\nValue prediction\\n\\ndefinition, 202\\nhardware-based speculation, 192\\nILP, 212–213, 220\\nspeculation, 208\\nVAPI, InfiniBand, F-77\\nVariable length encoding\\n\\ndevelopment, 408\\n\\ncontrol flow instruction branches, \\n\\nmultiprogramming workload, 374\\nseek distance comparison, D-47\\nvector processor history, G-26\\n\\nUnpacked decimal, A-14, J-16\\nUnshielded twisted pair (UTP), LAN \\n\\nA-18\\n\\ninstruction sets, A-22\\nISAs, 14\\n\\nVariables\\n\\nhistory, F-99\\n\\nA-29\\n\\nand compiler technology, A-27 to \\n\\n\\x0cVariables (continued)\\n\\nvector-register characteristics, G-3\\n\\nI-80 ■\\n\\nIndex\\n\\nCUDA, 289\\nFermi GPU, 306\\nISA, A-5, A-12\\nlocks via coherence, 389\\nloop-level parallelism, 316\\nmemory consistency, 392\\nNVIDIA GPU Memory, 304–305\\nprocedure invocation options, \\n\\nA-19\\n\\nrandom, distribution, D-26 to D-34\\nregister allocation, A-26 to A-27\\nin registers, A-5\\nsynchronization, 375\\nTLP programmer’s viewpoint, 394\\n\\nVCs, see Virtual channels (VCs)\\nVector architectures\\n\\ncomputer development, L-44 to L-49\\ndefinition, 9\\nDLP\\n\\nbasic considerations, 264\\ndefinition terms, 309\\ngather/scatter operations, \\n\\n279–280\\n\\nmultidimensional arrays, \\n\\n278–279\\nmultiple lanes, 271–273\\nprogramming, 280–282\\nvector execution time, 268–271\\nvector-length registers, \\n274–275\\nvector load/store unit \\n\\nbandwidth, 276–277\\n\\nvector-mask registers, 275–276\\nvector processor example, \\n\\n267–268\\n\\nVMIPS, 264–267\\n\\nGPU conditional branching, 303\\nvs.', ' GPU, 308, 311\\nvector processor example, 268\\nVMIPS, 265–267, 266\\n\\nVectorizable Loop\\n\\ncharacteristics, 268\\ndefinition, 268, 292, 313\\nGrid mapping, 293\\nLivermore Fortran kernel \\n\\nperformance, 331\\n\\nmapping example, 293\\nNVIDIA GPU computational \\n\\nstructures, 291\\n\\nVectorized code\\n\\nmultimedia compiler support, A-31\\nvector architecture programming, \\n\\n280–282\\n\\nvector execution time, 271\\nVMIPS, 268\\n\\nVectorized Loop, see also Body of \\n\\nVectorized Loop\\n\\ndefinition, 309\\nGPU Memory structure, 304\\nvs.', ' Grid, 291, 308\\nmask registers, 275\\nNVIDIA GPU, 295\\nvector vs.', ' GPU, 308\\n\\nVectorizing compilers\\n\\neffectiveness, G-14 to G-15\\nFORTRAN test kernels, G-15\\nsparse matrices, G-12 to G-13\\nVector Lane Registers, definition, 292\\nVector Lanes\\n\\ncontrol processor, 311\\ndefinition, 292, 309\\nSIMD Processor, 296–297, 297\\n\\nVector-length register (VLR)\\nbasic operation, 274–275\\nperformance, G-5\\nVMIPS, 267\\nVector load/store unit\\n\\nmemory banks, 276–277\\nVMIPS, 265\\n\\nVector loops\\n\\nNVIDIA GPU, 294\\nprocessor example, 267\\nstrip-mining, 303\\nvector vs.', ' GPUs, 276\\nhistorical background, G-26\\nloop-level parallelism, 150\\nloop unrolling, 196\\nmeasures, G-15 to G-16\\nmemory banks, 277\\nand multiple lanes, 273, 310\\nmultiprocessor architecture, 346\\nNVIDIA GPU computational \\n\\nstructures, 291\\n\\noverview, G-25 to G-26\\npeak performance focus, 331\\nperformance, G-2 to G-7\\n\\nstart-up and multiple lanes, G-7 \\n\\nto G-9\\nperformance comparison, 58\\nperformance enhancement\\nchaining, G-11 to G-12\\n\\n\\x0cDAXPY on VMIPS, G-19 to \\n\\nsparse matrices, G-12 to G-14\\n\\nPTX, 301\\nRoofline model, 286–287, 287\\nvs.', ' SIMD Processor, 294–296\\nSony PlayStation 2 Emotion \\n\\nEngine, E-17 to E-18\\n\\nstart-up overhead, G-4\\nstride, 278\\nstrip mining, 275\\nvector execution time, 269–271\\nvector/GPU comparison, 308\\nvector kernel implementation, \\n334–336\\n\\nVMIPS, 264–265\\nVMIPS on DAXPY, G-17\\nVMIPS on Linpack, G-17 to G-19\\n\\ndefinition, 309\\nexecution time, 269, 271\\ngather-scatter, 280\\nmultimedia compiler support, A-31\\nMultimedia SIMD Extensions, 282\\nmultiple lanes, 271–273\\nNVIDIA GPU, 297\\nNVIDIA GPU ISA, 298\\nperformance/bandwidth trade-offs, \\n\\n332\\nprocessor example, 267\\nstrides, 278–279\\nvector vs.', ' ROB, 208\\nROB, 192\\nTomasulo’s advantages, 177–178\\n\\nWrite allocate\\n\\nAMD Opteron data cache, B-12\\ndefinition, B-11\\nexample calculation, B-12\\n\\nWrite-back cache\\n\\nAMD Opteron example, B-12, B-14\\ncoherence maintenance, 381\\ncoherency, 359\\ndefinition, B-11\\ndirectory-based cache coherence, \\n\\n383, 386\\nFlash memory, 474\\nFP register file, C-56\\ninvalidate protocols, 355–357, 360\\nmemory hierarchy basics, 75\\nsnooping coherence, 355, \\n\\n356–357, 359\\n\\nWorst-case execution time (WCET), \\n\\nWrite-back cycle (WB)\\n\\ndefinition, E-4\\n\\nWrite after read (WAR)\\n\\ndata hazards, 153–154, 169\\n\\nbasic MIPS pipeline, C-36\\ndata hazard stall minimization, \\n\\nC-17\\n\\nWindowing, congestion management, \\n\\nWormhole switching, F-51, F-88\\n\\n\\x0cI-84 ■\\n\\nIndex\\n\\nWrite-back cycle (continued )\\nexecution sequences, C-80\\nhazards and forwarding, C-55 to \\n\\nWrite merging\\nexample, 88\\nmiss penalty reduction, 87\\n\\nC-56\\n\\nWrite miss\\n\\nMIPS exceptions, C-49\\nMIPS pipeline, C-52\\nMIPS pipeline control, C-39\\nMIPS R4000, C-63, C-65\\nMIPS scoreboarding, C-74\\npipeline branch issues, C-40\\nRISC classic pipeline, C-7 to C-8, \\n\\nsimple MIPS implementation, \\n\\nC-10\\n\\nC-33\\n\\n356\\n\\nsimple RISC implementation, C-6\\nWrite broadcast protocol, definition, \\n\\nWrite buffer\\n\\nAMD Opteron data cache, B-14\\nIntel Core i7, 118, 121\\ninvalidate protocol, 356\\nmemory consistency, 393\\nmemory hierarchy basics, 75\\nmiss penalty reduction, 87, B-32, \\nB-35 to B-36\\n\\nwrite merging example, 88\\nwrite strategy, B-11\\n\\nWrite hit\\n\\ncache coherence, 358\\ndirectory-based coherence, 424\\nsingle-chip multicore \\n\\nmultiprocessor, 414\\n\\nsnooping coherence, 359\\nwrite process, B-11\\nWrite invalidate protocol\\n\\ndirectory-based cache coherence \\n\\nprotocol example, \\n382–383\\nexample, 359, 360\\nimplementation, 356–357\\nsnooping coherence, 355–356\\n\\nAMD Opteron data cache, B-12, \\n\\nB-14\\n\\ncache coherence, 358, 359, 360, 361\\ndefinition, 385\\ndirectory-based cache coherence, \\n\\n380–383, 385–386\\n\\nexample calculation, B-12\\nlocks via coherence, 390\\nmemory hierarchy basics, 76–77\\nmemory stall clock cycles, B-4\\nOpteron data cache, B-12, B-14\\nsnooping cache coherence, 365\\nwrite process, B-11 to B-12\\nwrite speed calculations, 393\\n\\ndata hazards, 154\\ndynamic scheduling, 174–175\\nhardware-based speculation, 192\\ninstruction steps, 175\\nROB instruction, 186\\nscoreboarding, C-74 to C-75, C-78 \\n\\nto C-80\\nstatus table examples, C-77\\nTomasulo’s algorithm, 178, 180, \\n\\nWrite result stage\\n\\n190\\nWrite serialization\\n\\nhardware primitives, 387\\nmultiprocessor cache coherency, \\n\\n353\\nsnooping coherence, 356\\nWrite stall, definition, B-11\\nWrite strategy\\n\\nmemory hierarchy considerations, \\n\\nB-6, B-10 to B-12\\nvirtual memory, B-45 to B-46\\n\\nWrite-through cache\\n\\naverage memory access time, B-16\\n\\ncoherency, 352\\ninvalidate protocol, 356\\nmemory hierarchy basics, 74–75\\nmiss penalties, B-32\\noptimization, B-35\\nsnooping coherence, 359\\nwrite process, B-11 to B-12\\n\\nWrite update protocol, definition, 356\\nWSCs, see Warehouse-scale \\n\\ncomputers (WSCs)\\n\\nX\\nXBox, L-51\\nXen Virtual Machine\\n\\nAmazon Web Services, 456–457\\ncharacteristics, 111\\n\\nXerox Palo Alto Research Center, \\n\\nLAN history, F-99\\n\\nXIMD architecture, L-34\\nXon/Xoff, interconnection networks, \\nF-10, F-17\\n\\nY\\nYahoo!, WSCs, 465\\nYield\\n\\nchip fabrication, 61–62\\ncost trends, 27–32\\nFermi GTX 480, 324\\n\\nZ\\nZ-80 microcontroller, cell phones, \\nE-24\\n\\nZero condition code, MIPS core, K-9 \\nto K-16\\n\\nZero-copy protocols\\ndefinition, F-8\\nmessage copying issues, F-91\\n\\nZero-load latency, Intel SCCC, \\nF-70\\n\\nZuse, Konrad, L-4 to L-5\\nZynga, FarmVille, 460\\n\\n\\x0cTranslation between GPU terms in book and official NVIDIA and OpenCL terms.', '\\n\\nMore Descriptive \\nName used in this \\nBook\\n\\nType\\n\\nOfficial CUDA/\\nNVIDIA Term\\n\\nVectorizable Loop\\n\\nGrid\\n\\nBody of \\nVectorized Loop\\n\\nThread Block\\n\\nSequence of\\nSIMD Lane Opera-\\ntions\\n\\nCUDA Thread\\n\\nA Thread of \\nSIMD \\nInstructions\\n\\nWarp\\n\\nBook Definition \\nand OpenCL Terms\\n\\nOfficial CUDA/NVIDIA\\nDefinition\\n\\nA vectorizable loop, executed on the GPU, made \\nup of 1 or more “Thread Blocks” (or bodies of \\nvectorized loop) that can execute in parallel.']\n",
            "---\n",
            "Looking into Long short term memory\n",
            "['hochreit@informatik.tu-muenchen.de', 'juergen@idsia.ch']\n",
            "['hochreit@informatik.tu-muenchen.de', 'juergen@idsia.ch']\n",
            "['informatik.tu-muenchen.de', 'www7.informatik.tu-muenchen.de', 'www7.informatik.tu-muenchen.de']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Binarized Neural Networks\n",
            "['itayh@technion.ac.il', 'matthieu.courbariaux@gmail.com', 'daniel.soudry@gmail.com', 'rani@cs.technion.ac.il', 'yoshua.umontreal@gmail.com']\n",
            "['itayh@technion.ac.il', 'matthieu.courbariaux@gmail.com', 'daniel.soudry@gmail.com', 'rani@cs.technion.ac.il', 'yoshua.umontreal@gmail.com']\n",
            "[]\n",
            "[' Last but not least, we wrote a binary matrix multiplication GPU kernel\\nwith which it is possible to run our MNIST BNN 7 times faster than with an\\nunoptimized GPU kernel, without suffering any loss in classiﬁcation accuracy.', ' Today, DNNs are almost exclusively trained on one or many very\\nfast and power-hungry Graphic Processing Units (GPUs) (Coates et al.', '\\n\\n• Last but not least, we programed a binary matrix multiplication GPU kernel with which it is\\npossible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without\\nsuffering any loss in classiﬁcation accuracy (see Section 4).', '\\n\\n4 Seven Times Faster on GPU at Run-Time\\n\\nIt is possible to speed up GPU implementations of BNNs, by using a method sometimes called\\nSIMD (single instruction, multiple data) within a register (SWAR).', '\\nThose 3 instructions (accumulation, popcount, xnor) take 1 + 4 + 1 = 6 clock cycles on recent\\n\\nand w32b\\n\\n1\\n\\n0\\n\\n6\\n\\n\\x0cNvidia GPUs (and if they were to become a fused instruction, it would only take a single clock cycle).', '\\nConsequently, we obtain a theoretical Nvidia GPU speed-up of factor of 32/6 ≈ 5.', '\\n\\nIn order to validate those theoretical results, we\\nprogramed two GPU kernels:\\n\\n• The ﬁrst kernel (baseline) is an unoptimized\\n\\nmatrix multiplication kernel.', '\\n\\nThe two GPU kernels return identical outputs\\nwhen their inputs are constrained to −1 or +1\\n(but not otherwise).', '\\n\\n5 Discussion and Related Work\\n\\nFigure 2: The ﬁrst three columns represent the\\ntime it takes to perform a 8192 × 8192 × 8192 (bi-\\nnary) matrix multiplication on a GTX750 Nvidia\\nGPU, depending on which kernel is used.', ' In terms of speed, we programed a binary matrix\\nmultiplication GPU kernel that enabled running MLP over the MNIST datset 7 times faster (than\\nwith an unoptimized GPU kernel) without suffering any accuracy degradation (see Section 4).', ', 2012), a Python library which allowed us to easily\\ndevelop a fast and optimized code for GPU.', ' We thank Yuxin Wu for helping us compare our GPU kernels with cuBLAS.', ' Theano: a CPU and GPU math expression compiler.']\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Collecting a large-scale dataset of fine-grained cars.\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Convolutional deep belief networks on cifar-10.\n",
            "['kriz@cs.toronto.edu']\n",
            "['kriz@cs.toronto.edu']\n",
            "[]\n",
            "['\\n\\nThe most computationally-intensive networks that we describe here take 45 hours to pre-train and 36\\nhours to (cid:28)ne-tune on an Nvidia GTX 280 GPU.', '\\n3128 happens to be a good number to use on Nvidia GPUs because it leads to matrices whose dimensions are divisible by\\n\\n64.']\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Learning multiple layers of features from tiny images.\n",
            "[]\n",
            "[]\n",
            "['latency at 0.091ms']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Deep learning for classical japanese literature.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Microsoft coco: Common objects in context.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into All-optical machine learning using diffractive deep neural networks.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Deep transfer learning with joint adaptation networks.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Distinctive image features from scale-invariant keypoints.\n",
            "['lowe@cs.ubc.ca']\n",
            "['lowe@cs.ubc.ca']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Lasso-type recovery of sparse representations for high- dimensional data.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Bayesian deep convolutional networks with many channels are gaussian processes\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into In Proceedings of the 40th annual meeting on association for computational linguistics,\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Fisher kernels on visual vocabularies for image categorization.\n",
            "['Firstname.Lastname@xrce.xerox.com']\n",
            "['Firstname.Lastname@xrce.xerox.com']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Efficient neural architecture search via parameters sharing\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into A study of complex deep learning networks on high-performance, neuromorphic, and quantum computers\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Squad: 100,000+ questions for machine comprehension of text.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Do imagenet classifiers generalize to imagenet?\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Imagenet large scale visual recognition challenge\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into The evolved transformer\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Theoretical insights into the optimization landscape of over-parameterized shallow neural networks.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Energy and policy considerations for deep learning in nlp\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Outlier-robust spatial percep- tion: Hardness, general-purpose algorithms, and guarantees\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Ai feynman: A physics-inspired method for symbolic regression\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into The inaturalist species classification and detection dataset.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Locality-constrained linear coding for image classification\n",
            "['†{jjwang, ygong}@akiira.com, ‡{jyang29, huang}@ifp.uiuc.edu, §{kyu, flv}@sv.nec-labs.com']\n",
            "[]\n",
            "['j.patrec.2009.09.004']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Coordinating filters for faster deep neural networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Lite transformer with long- short range attention\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Neural-symbolic vqa: Disentangling reasoning from vision and language understanding\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Image classification using super- vector coding of local image descriptors.\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Revisiting resnets: Improved training and scaling strategies\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Are we done with imagenet?\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into End-to-end object detection with transformers\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Big self- supervised models are strong semi-supervised learners.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into On the relationship between self-attention and convolutional layers\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Bootstrap your own latent: A new approach to self- supervised learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Scaling laws for autoregressive generative modeling.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Scaling up visual and vision-language representation learning with noisy text supervision\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Scaling laws for neural language models.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Big Transfer (BiT): General Visual Representation Learning.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Settransformer:A framework for attention-based permutation-invariant neural networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Gshard: Scaling giant models with conditional computation and automatic sharding.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Meta pseudo labels\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Adafactor: Adaptive learning rates with sublinear memory cost.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Bottleneck transformers for visual recognition\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Training data-efficient image transformers & distillation through attention\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Scaling local self-attention for parameter efficient visual backbones.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Tokens-to-token vit: Training vision transformers from scratch on imagenet.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into S4l: Self-supervised semi-supervised learning.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into A large-scale study of representation learning with the visual task adaptation benchmark.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Backward feature correction: How deep learning performs deep learning.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Training deeper neural ma- chine translation models with transparent attention.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into The university of edinburgh submissions to the wmt19 news translation task\n",
            "['rachel.bawden@ed.ac.uk']\n",
            "['rachel.bawden@ed.ac.uk']\n",
            "['data.statmt.org', 'translation.15']\n",
            "['\\n\\n7We were unable to translate all available monolingual\\n\\ndata due to time constraints and limits to GPU resources.', '\\nWe use mini-batches dynamically ﬁtted into 48GB\\nof GPU memory on 4 GPUs and delay gradient\\nupdates to every second iteration, which results in\\nmini-batches of 1-1.']\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Analysis Methods in Neural Language Processing: A Survey\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into The best of both worlds: Combining recent advances in neural ma- chine translation\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Robust neural machine translation with doubly ad- versarial inputs.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Very deep convolutional net- works for text classification\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Understanding back-translation at scale.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Convolutional sequence to sequence learning.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Microsoft translator at wmt 2019: Towards large-scale document-level neural machine translation\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into On the variance of the adaptive learning rate and beyond.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Understanding the difficulty of training transformers\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Adversarial training for large neural lan- guage models\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Deep Learning Based Text Classification: A Comprehensive Review\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into A tool for holistic comparison of language genera- tion systems.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Facebook FAIR’s WMT19 News Translation Task Submission\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Transform- ers without tears: Improving the normalization of self-attention.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Fairseq: A fast, extensi- ble toolkit for sequence modeling.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Scaling neural machine trans- lation.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into A call for clarity in reporting BLEU scores.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Learning deep transformer models for ma- chine translation.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Understanding and improving layer normalization.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Contextual string embeddings for sequence labeling.\n",
            "['{firstname.lastname}@zalando.de']\n",
            "[]\n",
            "['creativecommons.org', 'gradients at 0.25']\n",
            "[', 2017), but require the training\\nof massive word-level language models: their best conﬁguration uses a language model that was trained\\nfor 5 weeks on 32 GPUs (Jozefowicz et al.', ' distinct words) and are thus signiﬁcantly easier to train and deploy in applications: the LM\\nparameter-count scales according to nlayers n2\\nhidden (not true for word-level LM since these contain many\\ninputs) and may be trained in 1 week on 1 GPU (e.', ' signiﬁcantly less than the 5 weeks on 32 GPUs\\nused by Peters et al.']\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Cloze-driven pretraining of self-attention networks.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Designing neural network architec- tures using reinforcement learning.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Enriching Word Vectors with Subword Information\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Semi-supervised se- quence modeling with cross-view training\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Unsupervised cross-lingual representation learning at scale\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Deep biaffine attention for neural dependency pars- ing.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Simpler but more accurate semantic dependency parsing.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Simple and efficient architecture search for convolutional neural networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Neural architecture search: A survey\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Transition-based semantic dependency parsing with pointer networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Nas-fpn: Learning scalable feature pyramid architec- ture for object detection.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Establishing strong baselines for the new decade: Sequence tagging, syntactic and semantic parsing with bert.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Densely connected con- volutional networks.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Adam: A method for stochastic optimization.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into 75 lan- guages, 1 model: Parsing Universal Dependencies universally.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into  Random search and reproducibility for neural architecture search.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Exploiting BERT for end-to-end aspect-based sentiment analysis.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Global greedy dependency parsing.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Auto-deeplab: Hierarchical neural ar- chitecture search for semantic image segmentation.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Hierarchical representations for efficient architecture search.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Parsing tweets into Universal Dependencies\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into GCDT: A global context enhanced deep transition architecture for se- quence labeling.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Roberta: A robustly optimized bert pretraining ap- proach.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Decoupled weight decay regularization.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Exploring cross-sentence contexts for named entity recogni- tion with BERT.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Stack- pointer networks for dependency parsing\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Rethinking self-attention: Towards inter- pretability in neural parsing.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into A pre-trained language model for English Tweets.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Deep contextualized word rep- resentations.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into How multilingual is multilingual BERT?\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Regularized evolution for image classifier architecture search.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Large-scale evolution of image classifiers.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Cross-lingual alignment of con- textual word embeddings, with applications to zero- shot dependency parsing.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Neural architectures for nested NER through lineariza- tion.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into A genetic programming ap- proach to designing convolutional neural network ar- chitectures.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Second-order semantic dependency parsing with end-to-end neural networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Structure-level knowledge distillation for multilingual sequence la- beling.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into More Embeddings, Better Sequence Labelers?\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Second-Order Neural Dependency Parsing with Message Passing and End-to-End Training\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Automated Concatenation of Embeddings for Structured Prediction\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Don’t eclipse your arts due to small discrepancies: Boundary repositioning with a pointer network for aspect extraction\n",
            "['{airzkwei, tianxianer(cid:0)}@gmail.com, zou bowei@i2r.astar.edu.sg,', 'dcdream@outlook.com, jyao@suda.edu.cn']\n",
            "['bowei@i2r.astar.edu.sg', 'dcdream@outlook.com', 'jyao@suda.edu.cn']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Deep learning architecture search by neuro-cell-based evolution with function- preserving mutations.\n",
            "['martin.wistuba@ibm.com']\n",
            "['martin.wistuba@ibm.com']\n",
            "[]\n",
            "['\\nOur proposed method ﬁnds within 12 GPU hours neural network ar-\\nchitectures that can achieve a classiﬁcation error of about 4% and 24%\\nwith only 5.', ' Since these methods still require GPU years\\nuntil this performance is reached, further work has been proposed to signiﬁcantly\\ndecrease the run time [1, 3, 15, 30, 32].', ' demonstrated in an experi-\\nment over 28 days and with 800 GPUs that neural network architectures with\\nperformances close to state-of-the-art architectures can be found.', ' In an extraordinary experiment that used 250 GPUs\\nfor almost 11 days, they showed that architectures can be found which provide\\nsimilar good results as human-crafted image classiﬁcation network architectures.', '5 GPU years in order to reach this\\nperformance.', ' Hierarchical Evolution [15] ﬁnds the best performing architecture\\n\\n\\x0cDeep Learning Architecture Search by Neuro-Cell-based Evolution\\n\\n11\\n\\nTable 1: Classiﬁcation error on CIFAR-10 and CIFAR-100 including spent search\\ntime in GPU days.', '\\n\\namong them in 300 GPU days.']\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Beto, bentz, be- cas: The surprising cross-lingual effectiveness of BERT.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Genetic cnn\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into BERT post-training for review reading comprehension and aspect-based sentiment analysis.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Dou- ble embeddings and CNN-based sequence labeling for aspect extraction\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into LUKE: Deep contextualized entity representations with entity- aware self-attention.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Xlnet: Generalized autoregressive pretraining for language understanding\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Named entity recognition as dependency parsing.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Effi- cient second-order TreeCRF for neural dependency parsing.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Practical block-wise neural network architecture generation.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Head-Driven Phrase Structure Grammar parsing on Penn Treebank\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Neural architecture search with reinforcement learning.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Learning transferable architec- tures for scalable image recognition\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Improved baselines with momentum contrastive learning.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Ex- ploring the limits of weakly supervised pretraining.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Large scale learning of general visual representations for transfe\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Do better imagenet models transfer better?\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Deformable convolutional networks\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Spinenet: Learning scale-permuted backbone for recognition and localization\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Centernet: Keypoint triplets for object detection.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Simple copy-paste is a strong data augmentation method for instance segmentation,\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Fast r-cnn\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Mask r-cnn\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Feature pyra- mid networks for object detection.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Focal loss for dense object detection\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Path Aggregation Network for Instance Segmentation\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Libra r-cnn: Towards balanced learning for ob- ject detection.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Borderdet: Border feature for dense object detection.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into You only look once: Unified, real-time object detec- tion.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Faster r-cnn: Towards real-time object detection with region proposal networks.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into An analysis of scale invariance in ob- ject detection - snip.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Sniper: Efficient multi-scale training.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into SparseR-CNN: End-to-end object detection with learnable proposals.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Efficientdet: Scal- able and efficient object detection\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Fcos: Fully convolutional one-stage object detection.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Non-local neural networks.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Scale-equalizing pyramid convolution for ob- ject detection\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into RepPoints: Point set representation for object detection.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Multi-scale context aggregation by di- lated convolutions\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Bridging the gap between anchor-based and anchor- free detection via adaptive training sample selection\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Probabilistic two-stage detection\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into De- formable convnets v2: More deformable, better results.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Deformable detr: Deformable transformers for end-to- end object detection.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Edge boxes: Locating object proposals from edges\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Rethinking pre- training and self-training\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Object detection in 20 years: A survey.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into What Does BERT Look at? An Analysis of BERT’s Attention.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Deep Joint Entity Disambiguation with Local Neural Attention.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into GAUSSIAN ERROR LINEAR UNITS (GELUS\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Span- BERT: Improving Pre-training by Representing and Predicting Spans.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Layer Normalization.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Layer Normalization.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into BART: Denoising Sequence-to-Sequence Pre- training for Natural Language Generation, Transla- tion, and Comprehension.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Learning Cross- Context Entity Representations from Text.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Commonsense Inference in Nat- ural Language Processing (COIN) - Shared Task Re- port.\n",
            "['simono@coli.uni-saarland.de', 'zsheng2@jhu.edu', 'rothml@ims.uni-stuttgart.de', 'peterc@allenai.org']\n",
            "['simono@coli.uni-saarland.de', 'zsheng2@jhu.edu', 'rothml@ims.uni-stuttgart.de', 'peterc@allenai.org']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Knowledge Enhanced Contextual Word Representations\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Exploring the Limits of Transfer Learning with a Unified Text-to- Text Transformer.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Visualizing and Measuring the Geometry of BERT.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Bidirectional Attention Flow for Machine Comprehension.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Deep Exhaustive Model for Nested Named Entity Recognition.\n",
            "['sohrab.mohammad@aist.go.jp, makoto-miwa@toyota-ti.ac.jp']\n",
            "['sohrab.mohammad@aist.go.jp', 'makoto-miwa@toyota-ti.ac.jp']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into Complex Embeddings for Simple Link Prediction\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into K-Adapter: Infusing Knowledge into Pre-trained Models with Adapters.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Pretrained Encyclope- dia: Weakly Supervised Knowledge-Pretrained Lan- guage Model.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Joint Learning of the Embedding of Words and Entities for Named En- tity Disambiguation.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Learning Distributed Representations of Texts and Entities from Knowl- edge Base.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Embedding Entities and Relations for Learning and Inference in Knowl- edge Bases.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Graph Convolution over Pruned Depen- dency Trees Improves Relation Extraction.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into Position- aware Attention and Supervised Data Improve Slot Filling\n",
            "['{yuhao, vzhong, danqi}@cs.stanford.edu', '{angeli, manning}@cs.stanford.edu']\n",
            "[]\n",
            "['statements.Your', 'statement.2', 'UnitedStates.Actors']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "---\n",
            "Looking into ERNIE: En- hanced Language Representation with Informative Entities.\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author(s)</th>\n",
              "      <th>Publication date</th>\n",
              "      <th>Reference</th>\n",
              "      <th>Link</th>\n",
              "      <th>email_1</th>\n",
              "      <th>email_2</th>\n",
              "      <th>email_3</th>\n",
              "      <th>GPU</th>\n",
              "      <th>TPU</th>\n",
              "      <th>NVIDIA</th>\n",
              "      <th>email_subject</th>\n",
              "      <th>email_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>DeepStack: Expert-Level Artificial Intelligenc...</td>\n",
              "      <td>https://arxiv.org/abs/1701.01724</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>N Shazeer, A Mirhoseini, K Maziarz, A Davis</td>\n",
              "      <td>2017-01-23</td>\n",
              "      <td>Outrageously Large Neural Networks: The Sparse...</td>\n",
              "      <td>https://arxiv.org/abs/1701.06538</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>Kaiming He, Georgia Gkioxari, Piotr Dollár, Ro...</td>\n",
              "      <td>2017-03-30</td>\n",
              "      <td>Mask R-CNN</td>\n",
              "      <td>https://arxiv.org/abs/1703.06870</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>AG Howard, M Zhu, B Chen, D Kalenichenko</td>\n",
              "      <td>2017-04-17</td>\n",
              "      <td>MobileNets: Efficient Convolutional Neural Net...</td>\n",
              "      <td>https://arxiv.org/abs/1704.04861</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>T Anthony, Z Tian, D Barber</td>\n",
              "      <td>2017-05-23</td>\n",
              "      <td>Thinking Fast and Slow with Deep Learning and ...</td>\n",
              "      <td>https://arxiv.org/abs/1705.08439</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>Bishan Yang, Scott Wen-tau Yih, Xiaodong He, J...</td>\n",
              "      <td>2020-02-13</td>\n",
              "      <td>Embedding Entities and Relations for Learning ...</td>\n",
              "      <td>https://arxiv.org/pdf/1412.6575.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianf...</td>\n",
              "      <td>2020-07-08</td>\n",
              "      <td>ReCoRD: Bridging the Gap between Human and Mac...</td>\n",
              "      <td>https://arxiv.org/pdf/1810.12885.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>748</th>\n",
              "      <td>Yuhao Zhang, Peng Qi, and Christopher D Manning</td>\n",
              "      <td>2020-07-28</td>\n",
              "      <td>Graph Convolution over Pruned Depen- dency Tre...</td>\n",
              "      <td>https://arxiv.org/pdf/1809.10185.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>749</th>\n",
              "      <td>Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor A...</td>\n",
              "      <td>2020-11-23</td>\n",
              "      <td>Position- aware Attention and Supervised Data ...</td>\n",
              "      <td>https://nlp.stanford.edu/pubs/zhang2017tacred.pdf</td>\n",
              "      <td>{yuhao, vzhong, danqi}@cs.stanford.edu;{angeli...</td>\n",
              "      <td></td>\n",
              "      <td>statements.Your;statement.2;UnitedStates.Actors</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>Trends in Machine Learning - Report your data ...</td>\n",
              "      <td>\\nDear all,\\n\\nWe are writing to you about you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750</th>\n",
              "      <td>Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor A...</td>\n",
              "      <td>2020-12-28</td>\n",
              "      <td>ERNIE: En- hanced Language Representation with...</td>\n",
              "      <td>https://arxiv.org/pdf/1905.07129.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>316 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Author(s)  ...                                         email_body\n",
              "252                                                NaN  ...                                                NaN\n",
              "253        N Shazeer, A Mirhoseini, K Maziarz, A Davis  ...                                                NaN\n",
              "254  Kaiming He, Georgia Gkioxari, Piotr Dollár, Ro...  ...                                                NaN\n",
              "255           AG Howard, M Zhu, B Chen, D Kalenichenko  ...                                                NaN\n",
              "256                        T Anthony, Z Tian, D Barber  ...                                                NaN\n",
              "..                                                 ...  ...                                                ...\n",
              "746  Bishan Yang, Scott Wen-tau Yih, Xiaodong He, J...  ...                                                NaN\n",
              "747  Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianf...  ...                                                NaN\n",
              "748    Yuhao Zhang, Peng Qi, and Christopher D Manning  ...                                                NaN\n",
              "749  Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor A...  ...  \\nDear all,\\n\\nWe are writing to you about you...\n",
              "750  Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor A...  ...                                                NaN\n",
              "\n",
              "[316 rows x 12 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWdxB_TQjPqr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}