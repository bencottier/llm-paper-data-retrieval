{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieving data from papers using GPT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /Users/bencottier/miniconda3/envs/nlp\n",
            "\n",
            "  added / updated specs:\n",
            "    - tenacity\n",
            "\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2023.5.7~ --> pkgs/main::ca-certificates-2023.01.10-hecd8cb5_0\n",
            "  certifi            conda-forge/noarch::certifi-2023.5.7-~ --> pkgs/main/osx-64::certifi-2023.5.7-py39hecd8cb5_0\n",
            "  openssl            conda-forge::openssl-1.1.1t-hfd90126_0 --> pkgs/main::openssl-1.1.1t-hca72f7f_0\n",
            "\n",
            "\n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda install -y tenacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!conda install -y openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.11.0\n",
            "  latest version: 23.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /Users/bencottier/miniconda3/envs/nlp\n",
            "\n",
            "  added / updated specs:\n",
            "    - tiktoken\n",
            "\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2023.5.7-h8857fd0_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            pkgs/main/osx-64::certifi-2023.5.7-py~ --> conda-forge/noarch::certifi-2023.5.7-pyhd8ed1ab_0\n",
            "  openssl              pkgs/main::openssl-1.1.1t-hca72f7f_0 --> conda-forge::openssl-1.1.1t-hfd90126_0\n",
            "\n",
            "\n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda install -y -c conda-forge tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ee3887XnrDa",
        "outputId": "5cde1173-289a-44e4-d6a1-c911b058c4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.11.0\n",
            "  latest version: 23.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!conda install -y -c conda-forge pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "from pdfminer.high_level import extract_text\n",
        "import re\n",
        "import requests\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff on API calls\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('output_data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Base character limit for the model.\n",
        "May be adjusted by prompt length.\n",
        "I've heard that English has about 4 chars per token on average.\n",
        "But papers may have parts with a lot of digits, which I think are one token each.\n",
        "GPT-3 token limit (including output) is 4096.\n",
        "So a base value of 4096 is safe.\n",
        "This should be subtracted by the prompt length later.\n",
        "\"\"\"\n",
        "BASE_CHAR_LIMIT = 4096\n",
        "\n",
        "\"\"\"\n",
        "The number of questions the model is asked.\n",
        "This should be updated along with the prompt.\n",
        "\"\"\"\n",
        "NUM_QUESTIONS = 2\n",
        "\n",
        "# Token limit for each model\n",
        "MAX_TOKENS = {\n",
        "    \"gpt-4\": 8192,\n",
        "    \"gpt-4-32k\": 32768,\n",
        "    \"gpt-3.5-turbo\": 4096,\n",
        "    \"text-davinci-003\": 4097,\n",
        "}\n",
        "\n",
        "# Token limit for model output\n",
        "MAX_RESPONSE_TOKENS = 100\n",
        "\n",
        "DEFAULT_CHAT_MODEL = \"gpt-4\"  # \"gpt-3.5-turbo\"\n",
        "DEFAULT_COMPLETION_MODEL = \"text-davinci-003\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_paper_text = \"\"\"\n",
        "2 Related work\n",
        "\n",
        "Language models and dialog models: Language models have attracted much attention recently thanks to their\n",
        "successes in NLP applications (e.g., [19, 20, 21, 2, 1, 22, 23, 5, 12, 24]). Our study of scaling laws with respect to\n",
        "model sizes is inspired by recent work on the scaling laws of neural language models [12, 13]. Similar to their ﬁndings,\n",
        "our results show that model scaling improves our quality (sensibleness, speciﬁcity, and interestingness), safety and\n",
        "groundedness metrics to some extent. However, ﬁne-tuning combined with scaling signiﬁcantly improves performance\n",
        "on all metrics.\n",
        "\n",
        "Our work is also closely related to recent successes in applying language models to dialog modeling (e.g., [25, 26,\n",
        "17, 18]), which built on earlier research in neural dialog modeling (e.g., [14, 15, 16, 27, 28]). One of our ﬁne-tuning\n",
        "stages requires training on dialog-only data, which is related to Wolf et al. [29], Dinan et al. [25] and Zhang et al. [30].\n",
        "Our use of ﬁne-tuning on crowdworker-annotated data to improve interestingness is comparable to Roller et al. [18].\n",
        "However, we aim to maximize the interestingness of the model’s output distinctly from its ability to engage the user in\n",
        "further interaction.\n",
        "\n",
        "Our ﬁnding that pure scaling has a limited effect on key measures of open-domain dialog model performance echoes\n",
        "that of Shuster et al. [31], who also focus on the problem of groundedness. Recent studies on scaling have found that\n",
        "performance on question-answering tasks improves with model size [32, 33], similar to our ﬁndings on pre-trained\n",
        "LaMDA prior to ﬁne-tuning.\n",
        "\n",
        "Our approach to improving model groundedness is broadly consistent with a growing literature on augmenting neural\n",
        "language models with retrieval systems. Most of the existing literature focuses on the problem of open-domain\n",
        "question-answering rather than dialog generation, and the models themselves are used to index and rank knowledge\n",
        "sources, rather than trained to use an intermediate tool. Given these differences, we note that the range of existing\n",
        "approaches to this problem include the RNNLM [34], RAG [35], REALM [36], and FiD [37] architectures. Zhu et\n",
        "al. [38] provide a survey of further recent work. See Karpukhin et al. [39] for details on the ‘dense passage retriever’\n",
        "used in RAG. Recent work in this direction has expanded and elaborated on neural models’ ability to retrieve and rank\n",
        "passages [40]. The RETRO architecture demonstrates that language models can be primed with results retrieved from\n",
        "a database as large as two trillion tokens [41]. At a broad level, our approach is also comparable to that of Byrne et\n",
        "al. [42], which ﬁne-tunes the model to use external APIs for movie ticketing dialog.\n",
        "\n",
        "Parts of our ﬁndings are similar to recent studies on dialog groundedness. Granting access to external knowledge\n",
        "bases has been shown to reduce the rate at which models hallucinate unsourced statements in dialog across a variety of\n",
        "retrieval systems and model architectures [31]. Another study ﬁnds that a question-answering system’s accuracy is\n",
        "improved by separating it into a reasoning unit and a response generator, analogous to our separation of ‘Base’ and\n",
        "‘Research’ models in our study [43]. Meanwhile, the WebGPT framework includes a language system that can interact\n",
        "with the open web via a text-only interface, and learns to imitate humans in answering questions by citing external\n",
        "sources [44]. Komeili et al. [45] compare different types of pre-trained models and retrieval methods, and reach a\n",
        "similar conclusion that augmenting language models with a search engine provides more factually grounded responses.\n",
        "They encode the input context with grounded information from search to generate the next response, while we augment\n",
        "the generated responses with information from known sources in our method. This allows us to ﬁne-tune the model for\n",
        "groundedness without sacriﬁcing gains in safety or quality from other ﬁne-tuning treatments.\n",
        "\n",
        "Dialog metrics: Deﬁning effective metrics for dialog models remains an open research topic. Our approach is\n",
        "inspired by Adiwardana et al. [17], who argued for human-like metrics, such as sensibleness and speciﬁcity. Many\n",
        "automated metrics for dialog models have been studied, including perplexity [16, 17], F1, Hits@1/N [25], USR [46],\n",
        "or BLEU/ROUGE [47, 15, 27]. However, such automated metrics may not correlate well with human judgment [48].\n",
        "More reliable metrics for dialog modeling require human evaluation [49, 50, 18, 25, 17, 51], as used in this paper.\n",
        "\n",
        "Earlier research attempted to combine multifaceted evaluations of dialog quality into a single headline metric [52]. We\n",
        "follow the pattern established in Adiwardana et al. [17] and Roller et al. [18] by considering the different components\n",
        "of our evaluations separately. In addition to sensibleness and speciﬁcity per Adiwardana et al. [17], we add new metrics:\n",
        "interestingness, safety, and groundedness. An advantage of using several different metrics is their debuggability: by\n",
        "exploring responses with low safety or groundedness scores, we have been able to develop targeted methods to improve\n",
        "them.\n",
        "\n",
        "Safety and safety of dialog models:\n",
        "Inappropriate and unsafe risks and behaviors of language models have been\n",
        "extensively discussed and studied in previous works (e.g., [53, 54]). Issues encountered include toxicity (e.g., [55, 56,\n",
        "57]), bias (e.g., [58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]), and inappropriately revealing personally\n",
        "identifying information (PII) from training data [73]. Weidinger et al. [54] identify 21 risks associated with large-scale\n",
        "\n",
        "3\n",
        "\n",
        "\flanguage models and discuss the points of origin for these risks. While many mitigation strategies have also been\n",
        "suggested (e.g., [74, 75, 76, 77, 78, 79, 80, 81, 82]), meaningfully addressing these issues remains an active research\n",
        "area.\n",
        "\n",
        "Similar issues have also been discussed speciﬁcally for dialog models [53]. For instance, examples of bias, offensiveness,\n",
        "and hate speech have been found both in training data drawn from social media, and consequently in the output of dialog\n",
        "models trained on such data [83]. Dialog models [84] can learn, and even amplify, biases in the training data. Echoing\n",
        "Gehman et al. [85], we ﬁnd ﬁne-tuning effective to augment language models for safety. The method we use in this\n",
        "paper follows previous attempts to tackle these issues by training separate layers to detect unsafe output [17, 86, 18, 79].\n",
        "Our strategy is similar to recent work that also uses ﬁne-tuning [87]. While their safety guidelines were derived from\n",
        "human rights principles, they similarly ﬁnd that increasing scale has no impact on toxicity metrics, while ﬁne-tuning on\n",
        "safety evaluations does.\n",
        "\n",
        "Groundedness metrics: Similar to other recent research into groundedness cited above, we assess groundedness\n",
        "by asking crowdworkers to judge whether the model’s output is in accordance with authoritative external sources.\n",
        "The recently-proposed Attributable to Identiﬁed Sources (AIS) framework [88] articulates a more precise approach\n",
        "to assess output of language models that pertains to the external world. It splits evaluation into two stages, where\n",
        "crowdworkers are asked: (1) if they can understand and identify the information shared in a dialog turn, and (2) if all\n",
        "of this information can be attributed to a source. Meanwhile, a recent study has reopened the question of automatic\n",
        "evaluation, with the Q2 metric showing performance comparable to human annotation [89].\n",
        "\n",
        "3 LaMDA pre-training\n",
        "\n",
        "LaMDA was pre-trained to predict the next token in a text corpus. Unlike previous dialog models trained on dialog data\n",
        "alone [17, 18], we pre-trained LaMDA on a dataset created from public dialog data and other public web documents.\n",
        "Therefore, LaMDA can be used as a general language model prior to ﬁne-tuning.\n",
        "\n",
        "The pre-training dataset consists of 2.97B documents, 1.12B dialogs, and 13.39B dialog utterances, for a total of 1.56T\n",
        "words (Appendix E). Over 90% of the pre-training dataset is in the English language. We used the SentencePiece\n",
        "library [90] to tokenize the dataset into 2.81T byte pair encoding (BPE) tokens [91], with a vocabulary of 32K tokens.\n",
        "For comparison, the total number of words in the training set for Meena [17] was 40B words, which is nearly 40x\n",
        "smaller.\n",
        "\n",
        "The largest LaMDA model has 137B non-embedding parameters, which is ~50x more parameters than Meena [17].\n",
        "We use a decoder-only Transformer [92] language model as the model architecture for LaMDA. The Transformer has\n",
        "64 layers, dmodel = 8192, df f = 65536, h = 128, dk = dv = 128, relative attention as described in T5 [11], and\n",
        "gated-GELU activation as described in Raffel et al. [93].\n",
        "\n",
        "We pre-trained LaMDA on 1024 TPU-v3 chips for a total of about 57.7 days, and 256K tokens per batch. We used\n",
        "the Lingvo framework [94] for training and achieved 123 TFLOPS/sec with 56.5% FLOPS utilization with the 2D\n",
        "sharding algorithm, as described in GSPMD [95] (see Section 10 for carbon footprint estimates). We also trained\n",
        "smaller 2B-parameter and 8B-parameter models to measure the effects of model scaling on our metrics. Hyperparameter\n",
        "details for the models of different sizes can be found in Table 27, Appendix D.\n",
        "\n",
        "Figure 2 gives an overview of the pre-training stage. We call the model before any ﬁne-tuning \"PT\", for PreTrained.\n",
        "\n",
        "Figure 2: LaMDA pre-training as a language model.\n",
        "\n",
        "4\n",
        "\n",
        "\fPT uses the same sample-and-rank strategy as Meena [17] for decoding. We ﬁrst sample 16 independent candidate\n",
        "responses using top-k (k = 40) sampling (no temperature). The ﬁnal output is the highest-scoring candidate, where the\n",
        "score is based on the candidate’s log-likelihood and its length.\n",
        "\n",
        "4 Metrics\n",
        "\n",
        "Evaluating generative models in general, and open-ended dialog models in particular, is difﬁcult. See the Related\n",
        "Work section for a general review of recent work in this area. In this section, we describe the metrics that we use for\n",
        "evaluation.\n",
        "\n",
        "4.1 Foundation metrics: Quality, Safety and Groundedness\n",
        "\n",
        "Sensibleness, Speciﬁcity, Interestingness (SSI): Our overall quality score is an average of sensibleness, speciﬁcity,\n",
        "and interestingness (SSI).\n",
        "\n",
        "Adiwardana et al. [17] propose the sensibleness and speciﬁcity average (SSA) metric to measure the quality of Meena.\n",
        "This metric is a simple average of two scores: sensibleness and speciﬁcity.\n",
        "\n",
        "The ﬁrst score, sensibleness, measures whether a model’s responses make sense in context and do not contradict\n",
        "anything that was said earlier. Humans tend to take this basic aspect of communication for granted, but generative\n",
        "models often struggle to meet this requirement. However, if sensibleness alone is used to evaluate models, we could\n",
        "inadvertently reward models for playing it safe by always producing short, generic, and boring responses. The\n",
        "GenericBot algorithm [17], which answers every question with “I don’t know” and every statement with “Ok,” scores\n",
        "70% on sensibleness, which even surpasses some large dialog models [17].\n",
        "\n",
        "The second score, speciﬁcity, is used to measure whether a response is speciﬁc to a given context. For example, if a user\n",
        "says “I love Eurovision” and the model responds “Me too,” then it would score 0 on speciﬁcity, since this response could\n",
        "be used in many different contexts. If it answers “Me too. I love Eurovision songs,” then it would score 1. Adiwardana\n",
        "et al. [17] report that Meena narrows the gap to average human performance in the SSA metric.\n",
        "\n",
        "As the model’s performance increases, however, we ﬁnd that sensibleness and speciﬁcity are not sufﬁcient to measure\n",
        "the quality of a dialog model. For example, a response to “How do I throw a ball?” could be “You can throw a ball by\n",
        "ﬁrst picking it up and then throwing it”, which makes sense and is speciﬁc to the question. An alternative deeper and\n",
        "more satisfying answer could be “One way to toss a ball is to hold it ﬁrmly in both hands and then swing your arm\n",
        "down and up again, extending your elbow and then releasing the ball upwards.”\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7H90LNEEoZZDUp9ZAaPl7cymJGMdy at 0x7f8458519130> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"| ---------------------- | --------------------------- | ------ |\\n| 1024                  | TPU-v3                      | 123 TFLOPS/sec |\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1684320545,\n",
              "  \"id\": \"cmpl-7H90LNEEoZZDUp9ZAaPl7cymJGMdy\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 29,\n",
              "    \"prompt_tokens\": 3117,\n",
              "    \"total_tokens\": 3146\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=f\"A table summarizing the training hardware from this paper:\\n\\n====\\n\\n{example_paper_text}\\n\\n====\\n\\n| Number of GPUs or TPUs | Hardware model (e.g. A100) | FLOP/s |\\n\",\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. N/A\n",
            "2. N/A\n",
            "3. 123 TFLOP/s\n"
          ]
        }
      ],
      "source": [
        "prompt_text = f\"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "1. How many GPUs or TPUs were used to train the model? Just state the number. If the number of GPUs or TPUs is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "3. What FLOP/s (AKA: FLOP/second, FLOPS) was achieved during training? Include the same units as written in the paper. If FLOP/s is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "3. 21 TFLOP/s\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "3. 21 petaflops\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "3. 127e12 FLOPS\n",
        "\n",
        "====\n",
        "\n",
        "{example_paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt_text,\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_text = \"\"\"Read the Machine Learning research paper below and answer the following questions. \n",
        "\n",
        "## Questions\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number.\n",
        "2. What model of GPU or TPU was used to train the model?\n",
        "\n",
        "## Instructions for how to answer each question\n",
        "\n",
        "1. Write the question number, e.g. \"1. \".\n",
        "2. Write \"Relevant quote: \" and copy verbatim the quote from the paper that determines your final answer.\n",
        "3. Write \"Final answer: \" and then write your final answer for the question.\n",
        "4. If the answer cannot be determined from the text, just write \"Relevant quotes: N/A. Final answer: N/A.\".\n",
        "\n",
        "## Made-up example answers\n",
        "\n",
        "1. Relevant quote: \"We pre-trained BaLM on 1024 V100 GPUs for a total of about 30 days.\" Final answer: 1024.\n",
        "2. Relevant quote: \"We pre-trained BaLM on 1024 V100 GPUs for a total of about 30 days.\" Final answer: V100.\n",
        "\n",
        "1. Relevant quote: N/A. Final answer: N/A.\n",
        "2. Relevant quote: \"SuperGAN was trained on a TPUv3 cluster, achieving a throughput of 52 TFLOPS.\" Final answer: TPUv3.\n",
        "\n",
        "## Paper\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "## Answers\n",
        "\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        # {\"role\": \"system\", \"content\": \"You are an expert in Machine Learning.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"{prompt_text.format(paper_text=example_paper_text)}\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_tokens=MAX_RESPONSE_TOKENS,\n",
        "    top_p=1,\n",
        "    frequency_penalty=-2,\n",
        "    presence_penalty=-2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Read the Machine Learning research paper below and answer the following questions. \\n\\n## Questions\\n\\n1. How many GPUs or TPUs or chips were used to train the model? Just state the number.\\n2. What model of GPU or TPU was used to train the model?\\n\\n## Instructions for how to answer each question\\n\\n1. Write the question number, e.g. \"1. \".\\n2. Write \"Relevant quotes: \" and copy verbatim any relevant quotes from the paper that inform your final answer.\\n3. Write \"Final answer: \" and then write your final answer for the question.\\n4. If the answer cannot be determined from the text, just write \"Relevant quotes: N/A. Final answer: N/A.\".\\n\\n## Made-up example answers\\n\\n1. Relevant quotes: \"We pre-trained BaLM on 1024 V100 GPUs for a total of about 30 days.\" Final answer: 1024.\\n2. Relevant quotes: \"We pre-trained BaLM on 1024 V100 GPUs for a total of about 30 days.\" Final answer: V100.\\n\\n1. Relevant quotes: N/A. Final answer: N/A.\\n2. Relevant quotes: \"SuperGAN was trained on a TPUv3 cluster, achieving a throughput of 52 TFLOPS.\" Final answer: TPUv3.\\n\\n## Paper\\n\\n\\n2 Related work\\n\\nLanguage models and dialog models: Language models have attracted much attention recently thanks to their\\nsuccesses in NLP applications (e.g., [19, 20, 21, 2, 1, 22, 23, 5, 12, 24]). Our study of scaling laws with respect to\\nmodel sizes is inspired by recent work on the scaling laws of neural language models [12, 13]. Similar to their ﬁndings,\\nour results show that model scaling improves our quality (sensibleness, speciﬁcity, and interestingness), safety and\\ngroundedness metrics to some extent. However, ﬁne-tuning combined with scaling signiﬁcantly improves performance\\non all metrics.\\n\\nOur work is also closely related to recent successes in applying language models to dialog modeling (e.g., [25, 26,\\n17, 18]), which built on earlier research in neural dialog modeling (e.g., [14, 15, 16, 27, 28]). One of our ﬁne-tuning\\nstages requires training on dialog-only data, which is related to Wolf et al. [29], Dinan et al. [25] and Zhang et al. [30].\\nOur use of ﬁne-tuning on crowdworker-annotated data to improve interestingness is comparable to Roller et al. [18].\\nHowever, we aim to maximize the interestingness of the model’s output distinctly from its ability to engage the user in\\nfurther interaction.\\n\\nOur ﬁnding that pure scaling has a limited effect on key measures of open-domain dialog model performance echoes\\nthat of Shuster et al. [31], who also focus on the problem of groundedness. Recent studies on scaling have found that\\nperformance on question-answering tasks improves with model size [32, 33], similar to our ﬁndings on pre-trained\\nLaMDA prior to ﬁne-tuning.\\n\\nOur approach to improving model groundedness is broadly consistent with a growing literature on augmenting neural\\nlanguage models with retrieval systems. Most of the existing literature focuses on the problem of open-domain\\nquestion-answering rather than dialog generation, and the models themselves are used to index and rank knowledge\\nsources, rather than trained to use an intermediate tool. Given these differences, we note that the range of existing\\napproaches to this problem include the RNNLM [34], RAG [35], REALM [36], and FiD [37] architectures. Zhu et\\nal. [38] provide a survey of further recent work. See Karpukhin et al. [39] for details on the ‘dense passage retriever’\\nused in RAG. Recent work in this direction has expanded and elaborated on neural models’ ability to retrieve and rank\\npassages [40]. The RETRO architecture demonstrates that language models can be primed with results retrieved from\\na database as large as two trillion tokens [41]. At a broad level, our approach is also comparable to that of Byrne et\\nal. [42], which ﬁne-tunes the model to use external APIs for movie ticketing dialog.\\n\\nParts of our ﬁndings are similar to recent studies on dialog groundedness. Granting access to external knowledge\\nbases has been shown to reduce the rate at which models hallucinate unsourced statements in dialog across a variety of\\nretrieval systems and model architectures [31]. Another study ﬁnds that a question-answering system’s accuracy is\\nimproved by separating it into a reasoning unit and a response generator, analogous to our separation of ‘Base’ and\\n‘Research’ models in our study [43]. Meanwhile, the WebGPT framework includes a language system that can interact\\nwith the open web via a text-only interface, and learns to imitate humans in answering questions by citing external\\nsources [44]. Komeili et al. [45] compare different types of pre-trained models and retrieval methods, and reach a\\nsimilar conclusion that augmenting language models with a search engine provides more factually grounded responses.\\nThey encode the input context with grounded information from search to generate the next response, while we augment\\nthe generated responses with information from known sources in our method. This allows us to ﬁne-tune the model for\\ngroundedness without sacriﬁcing gains in safety or quality from other ﬁne-tuning treatments.\\n\\nDialog metrics: Deﬁning effective metrics for dialog models remains an open research topic. Our approach is\\ninspired by Adiwardana et al. [17], who argued for human-like metrics, such as sensibleness and speciﬁcity. Many\\nautomated metrics for dialog models have been studied, including perplexity [16, 17], F1, Hits@1/N [25], USR [46],\\nor BLEU/ROUGE [47, 15, 27]. However, such automated metrics may not correlate well with human judgment [48].\\nMore reliable metrics for dialog modeling require human evaluation [49, 50, 18, 25, 17, 51], as used in this paper.\\n\\nEarlier research attempted to combine multifaceted evaluations of dialog quality into a single headline metric [52]. We\\nfollow the pattern established in Adiwardana et al. [17] and Roller et al. [18] by considering the different components\\nof our evaluations separately. In addition to sensibleness and speciﬁcity per Adiwardana et al. [17], we add new metrics:\\ninterestingness, safety, and groundedness. An advantage of using several different metrics is their debuggability: by\\nexploring responses with low safety or groundedness scores, we have been able to develop targeted methods to improve\\nthem.\\n\\nSafety and safety of dialog models:\\nInappropriate and unsafe risks and behaviors of language models have been\\nextensively discussed and studied in previous works (e.g., [53, 54]). Issues encountered include toxicity (e.g., [55, 56,\\n57]), bias (e.g., [58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]), and inappropriately revealing personally\\nidentifying information (PII) from training data [73]. Weidinger et al. [54] identify 21 risks associated with large-scale\\n\\n3\\n\\n\\x0clanguage models and discuss the points of origin for these risks. While many mitigation strategies have also been\\nsuggested (e.g., [74, 75, 76, 77, 78, 79, 80, 81, 82]), meaningfully addressing these issues remains an active research\\narea.\\n\\nSimilar issues have also been discussed speciﬁcally for dialog models [53]. For instance, examples of bias, offensiveness,\\nand hate speech have been found both in training data drawn from social media, and consequently in the output of dialog\\nmodels trained on such data [83]. Dialog models [84] can learn, and even amplify, biases in the training data. Echoing\\nGehman et al. [85], we ﬁnd ﬁne-tuning effective to augment language models for safety. The method we use in this\\npaper follows previous attempts to tackle these issues by training separate layers to detect unsafe output [17, 86, 18, 79].\\nOur strategy is similar to recent work that also uses ﬁne-tuning [87]. While their safety guidelines were derived from\\nhuman rights principles, they similarly ﬁnd that increasing scale has no impact on toxicity metrics, while ﬁne-tuning on\\nsafety evaluations does.\\n\\nGroundedness metrics: Similar to other recent research into groundedness cited above, we assess groundedness\\nby asking crowdworkers to judge whether the model’s output is in accordance with authoritative external sources.\\nThe recently-proposed Attributable to Identiﬁed Sources (AIS) framework [88] articulates a more precise approach\\nto assess output of language models that pertains to the external world. It splits evaluation into two stages, where\\ncrowdworkers are asked: (1) if they can understand and identify the information shared in a dialog turn, and (2) if all\\nof this information can be attributed to a source. Meanwhile, a recent study has reopened the question of automatic\\nevaluation, with the Q2 metric showing performance comparable to human annotation [89].\\n\\n3 LaMDA pre-training\\n\\nLaMDA was pre-trained to predict the next token in a text corpus. Unlike previous dialog models trained on dialog data\\nalone [17, 18], we pre-trained LaMDA on a dataset created from public dialog data and other public web documents.\\nTherefore, LaMDA can be used as a general language model prior to ﬁne-tuning.\\n\\nThe pre-training dataset consists of 2.97B documents, 1.12B dialogs, and 13.39B dialog utterances, for a total of 1.56T\\nwords (Appendix E). Over 90% of the pre-training dataset is in the English language. We used the SentencePiece\\nlibrary [90] to tokenize the dataset into 2.81T byte pair encoding (BPE) tokens [91], with a vocabulary of 32K tokens.\\nFor comparison, the total number of words in the training set for Meena [17] was 40B words, which is nearly 40x\\nsmaller.\\n\\nThe largest LaMDA model has 137B non-embedding parameters, which is ~50x more parameters than Meena [17].\\nWe use a decoder-only Transformer [92] language model as the model architecture for LaMDA. The Transformer has\\n64 layers, dmodel = 8192, df f = 65536, h = 128, dk = dv = 128, relative attention as described in T5 [11], and\\ngated-GELU activation as described in Raffel et al. [93].\\n\\nWe pre-trained LaMDA on 1024 TPU-v3 chips for a total of about 57.7 days, and 256K tokens per batch. We used\\nthe Lingvo framework [94] for training and achieved 123 TFLOPS/sec with 56.5% FLOPS utilization with the 2D\\nsharding algorithm, as described in GSPMD [95] (see Section 10 for carbon footprint estimates). We also trained\\nsmaller 2B-parameter and 8B-parameter models to measure the effects of model scaling on our metrics. Hyperparameter\\ndetails for the models of different sizes can be found in Table 27, Appendix D.\\n\\nFigure 2 gives an overview of the pre-training stage. We call the model before any ﬁne-tuning \"PT\", for PreTrained.\\n\\nFigure 2: LaMDA pre-training as a language model.\\n\\n4\\n\\n\\x0cPT uses the same sample-and-rank strategy as Meena [17] for decoding. We ﬁrst sample 16 independent candidate\\nresponses using top-k (k = 40) sampling (no temperature). The ﬁnal output is the highest-scoring candidate, where the\\nscore is based on the candidate’s log-likelihood and its length.\\n\\n4 Metrics\\n\\nEvaluating generative models in general, and open-ended dialog models in particular, is difﬁcult. See the Related\\nWork section for a general review of recent work in this area. In this section, we describe the metrics that we use for\\nevaluation.\\n\\n4.1 Foundation metrics: Quality, Safety and Groundedness\\n\\nSensibleness, Speciﬁcity, Interestingness (SSI): Our overall quality score is an average of sensibleness, speciﬁcity,\\nand interestingness (SSI).\\n\\nAdiwardana et al. [17] propose the sensibleness and speciﬁcity average (SSA) metric to measure the quality of Meena.\\nThis metric is a simple average of two scores: sensibleness and speciﬁcity.\\n\\nThe ﬁrst score, sensibleness, measures whether a model’s responses make sense in context and do not contradict\\nanything that was said earlier. Humans tend to take this basic aspect of communication for granted, but generative\\nmodels often struggle to meet this requirement. However, if sensibleness alone is used to evaluate models, we could\\ninadvertently reward models for playing it safe by always producing short, generic, and boring responses. The\\nGenericBot algorithm [17], which answers every question with “I don’t know” and every statement with “Ok,” scores\\n70% on sensibleness, which even surpasses some large dialog models [17].\\n\\nThe second score, speciﬁcity, is used to measure whether a response is speciﬁc to a given context. For example, if a user\\nsays “I love Eurovision” and the model responds “Me too,” then it would score 0 on speciﬁcity, since this response could\\nbe used in many different contexts. If it answers “Me too. I love Eurovision songs,” then it would score 1. Adiwardana\\net al. [17] report that Meena narrows the gap to average human performance in the SSA metric.\\n\\nAs the model’s performance increases, however, we ﬁnd that sensibleness and speciﬁcity are not sufﬁcient to measure\\nthe quality of a dialog model. For example, a response to “How do I throw a ball?” could be “You can throw a ball by\\nﬁrst picking it up and then throwing it”, which makes sense and is speciﬁc to the question. An alternative deeper and\\nmore satisfying answer could be “One way to toss a ball is to hold it ﬁrmly in both hands and then swing your arm\\ndown and up again, extending your elbow and then releasing the ball upwards.”\\n\\n\\n## Answers\\n'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_text.format(paper_text=example_paper_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1. Relevant quotes: \"We pre-trained LaMDA on 1024 TPU-v3 chips for a total of about 57.7 days\". Final answer: 1024. \\n2. Relevant quotes: \"We pre-trained LaMDA on 1024 TPU-v3 chips for a total of about 57.7 days.  Final answer: TPU-v3. '"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"choices\"][0][\"message\"][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1', ' Relevant quote: \"We pre-trained LaMDA on 1024 TPU-v3 chips for a total of about 57.7 days\". Final answer: 1024. ']\n",
            "['2', ' Relevant quote: \"We pre-trained LaMDA on 1024 TPU-v3 chips for a total of about 57.7 days.  Final answer: TPU-v3.']\n"
          ]
        }
      ],
      "source": [
        "answers = response[\"choices\"][0][\"message\"][\"content\"].strip().split(\"\\n\")\n",
        "# Remove \"\" from list of answers in case of multiple \"\\n\" chars\n",
        "answers = [a for a in answers if a != \"\"]\n",
        "processed_answers = list()\n",
        "for a in answers:\n",
        "    a = a.strip()\n",
        "    if a[1] == \".\":  # question numbering\n",
        "        answer_parts = a.split(\".\", maxsplit=1)[-1]\n",
        "        print(answer_parts)\n",
        "#         processed_answers.append(a.split(\".\")[-1].strip())\n",
        "#     else:\n",
        "#         processed_answers.append(a)\n",
        "# processed_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7Gt92obaFxiKh7eWGJ7x38epqSoWj at 0x7f8aa0b269f0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"1. N/A\\n2. TPU-v3\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1684259580,\n",
              "  \"id\": \"chatcmpl-7Gt92obaFxiKh7eWGJ7x38epqSoWj\",\n",
              "  \"model\": \"gpt-4-0314\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 11,\n",
              "    \"prompt_tokens\": 3072,\n",
              "    \"total_tokens\": 3083\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_text = \"\"\"\n",
        "Read the excerpt of a Machine Learning research paper below and answer the following questions.\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\"\"\"\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert in Machine Learning.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"{prompt_text}\\n\\n====\\n\\n{example_paper_text}\\n\\n====\\n\\n\"},\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_message_template = \"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some made-up example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "\n",
        "1. N/A\n",
        "2. TPUv3\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def parse_text_gpt_chat(text):\n",
        "    prompt_text = chat_message_template.format(paper_text=text)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=DEFAULT_CHAT_MODEL,\n",
        "        messages=[\n",
        "            # {\"role\": \"system\", \"content\": \"You are an expert in Machine Learning.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt_text},\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=MAX_RESPONSE_TOKENS,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def parse_gpt_chat_response(text):\n",
        "    response = parse_text_gpt_chat(text)\n",
        "    # E.g. \"1. 6144 TPUs\\n2. TPU v4\\n3. N/A\\n\"\n",
        "    # print(f\"Response: {response['choices'][0]['message']['content']}\")\n",
        "    answers = response[\"choices\"][0][\"message\"][\"content\"].strip().split(\"\\n\")\n",
        "    # Remove \"\" from list of answers in case of multiple \"\\n\" chars\n",
        "    answers = [a for a in answers if a != \"\"]\n",
        "    processed_answers = list()\n",
        "    for a in answers:\n",
        "        a = a.strip()\n",
        "        if a[1] == \".\":  # question numbering\n",
        "            processed_answers.append(a.split(\".\")[-1].strip())\n",
        "        else:\n",
        "            processed_answers.append(a)\n",
        "    return processed_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "completion_prompt_template = \"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are made-up some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "\n",
        "1. N/A\n",
        "2. TPUv3\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def parse_text_gpt(text):\n",
        "    prompt_text = completion_prompt_template.format(paper_text=text)\n",
        "    response = openai.Completion.create(\n",
        "        model=DEFAULT_COMPLETION_MODEL,\n",
        "        prompt=prompt_text,\n",
        "        temperature=0,\n",
        "        max_tokens=MAX_RESPONSE_TOKENS,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def parse_gpt_response(text):\n",
        "    response = parse_text_gpt(text)\n",
        "    # E.g. \"1. 6144 TPUs\\n2. TPU v4\\n3. N/A\\n\"\n",
        "    # print(response[\"choices\"][0][\"text\"])\n",
        "    answers = response[\"choices\"][0][\"text\"].strip().split(\"\\n\")\n",
        "    # Remove \"\" from list of answers in case of multiple \"\\n\" chars\n",
        "    answers = [a for a in answers if a != \"\"]\n",
        "    processed_answers = list()\n",
        "    for a in answers:\n",
        "        a = a.strip()\n",
        "        if a[1] == \".\":  # question numbering\n",
        "            processed_answers.append(a.split(\".\")[-1].strip())\n",
        "        else:\n",
        "            processed_answers.append(a)\n",
        "    return processed_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_answers(text, num_questions=1, mode=\"chat\"):\n",
        "    answer_fcn = parse_gpt_chat_response if mode == \"chat\" else parse_gpt_response\n",
        "    model_name = DEFAULT_CHAT_MODEL if mode == \"chat\" else DEFAULT_COMPLETION_MODEL\n",
        "    prompt_template = chat_message_template if mode == \"chat\" else completion_prompt_template\n",
        "\n",
        "    tokenizer = tiktoken.encoding_for_model(model_name)\n",
        "    encoded_text = tokenizer.encode(text, disallowed_special=())\n",
        "    print(f\"Full #tokens: {len(encoded_text)}\")\n",
        "    # The prompt text will be added to the paper text later\n",
        "    # Add constant extra tokens for a little buffer when using the chat model\n",
        "    max_tokens = MAX_TOKENS[model_name] - (len(tokenizer.encode(prompt_template)) + MAX_RESPONSE_TOKENS + 20)\n",
        "    print(f\"Max #tokens: {max_tokens}\")\n",
        "\n",
        "    text_pos = 0\n",
        "    final_answers = [None] * num_questions\n",
        "    while text_pos < len(encoded_text):\n",
        "        # Get model answers for the next chunk of the text\n",
        "        encoded_text_chunk = encoded_text[text_pos : text_pos + max_tokens]\n",
        "        print(f\"Chunk #tokens: {len(encoded_text_chunk)}\")\n",
        "        text_chunk = tokenizer.decode(encoded_text_chunk)  # the model will encode again\n",
        "        answers = answer_fcn(text_chunk)\n",
        "        # Process each answer\n",
        "        for i in range(num_questions):\n",
        "            try:\n",
        "                if final_answers[i] is None and answers[i] is not None:\n",
        "                    # Take the first answer as the final answer initially\n",
        "                    final_answers[i] = answers[i]\n",
        "                elif \"N/A\" in final_answers[i] and not (\"N/A\" in answers[i]):\n",
        "                    \"\"\"\n",
        "                    If the answer was \"N/A\" previously but there's at least \n",
        "                    one non-\"N/A\" answer for a later chunk, then use the \n",
        "                    first non-\"N/A\" answer as the final answer\n",
        "                    \"\"\"\n",
        "                    final_answers[i] = answers[i]\n",
        "            except IndexError:\n",
        "                print(f\"IndexError: index={i}, answers={answers}, final_answers={final_answers}\")\n",
        "        # Move to the next chunk of text\n",
        "        text_pos += max_tokens\n",
        "    return final_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_paper(df, i, row, keys):\n",
        "    url = row['Link']\n",
        "\n",
        "    # replace \"abs\" with \"pdf\" in arxiv url links\n",
        "    url = url.replace('abs', 'pdf')\n",
        "    print(f\"Looking into \\\"{row['Reference']}\\\"\")\n",
        "\n",
        "    # try:\n",
        "    #     response = requests.get(url)\n",
        "    # except Exception as e:\n",
        "    #     print(f\"There's something wrong with downloading: {e}\")\n",
        "    #     raise e\n",
        "\n",
        "    # file = open(\"download.pdf\", \"wb\")\n",
        "    # file.seek(0) # overwrite previous file\n",
        "    # file.write(response.content)\n",
        "    # file.close()\n",
        "\n",
        "    # try:\n",
        "    # text = extract_text('download.pdf')\n",
        "    paper_title = row['Reference'].replace(' ', '_').replace(':', '').replace('\"', '').lower()\n",
        "    with open('input_data/' + paper_title + '.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    answers = get_model_answers(text, num_questions=NUM_QUESTIONS, mode='chat')\n",
        "    print(f\"Answers: {answers}\")\n",
        "    \n",
        "    for key, answer in zip(keys, answers):\n",
        "        df.loc[i,key] = answer if answer else \"none\"\n",
        "    # except Exception as e:\n",
        "    #     print(f\"There's something wrong with extracting the text: {e}\")\n",
        "    #     raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_answers(df, answer_keys, correct_answers):\n",
        "    correct_dict = dict()\n",
        "    for key in answer_keys:\n",
        "        print(key)\n",
        "        correct = 0\n",
        "        for i, row in df.iterrows():\n",
        "            ref = row['Reference']\n",
        "            answer = row[key]\n",
        "            correct_answer = correct_answers[key][ref]\n",
        "            if answer == correct_answer:\n",
        "                correct += 1\n",
        "            else:\n",
        "                print(f\"{answer} != {correct_answer}\")\n",
        "        correct_dict[key] = correct\n",
        "    for k, v in correct_dict.items():\n",
        "        print(f\"{k}: {v}/{len(df)}\")\n",
        "    return correct_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "CORRECT_ANSWERS = {\n",
        "    'Number of hardware units': {\n",
        "        'No Language Left Behind: Scaling Human-Centered Machine Translation': 'N/A',\n",
        "        'Solving Quantitative Reasoning Problems with Language Models': '1024',\n",
        "        'Scaling Autoregressive Models for Content-Rich Text-to-Image Generation': 'N/A',\n",
        "        'A Generalist Agent': '256',\n",
        "        'OPT: Open Pre-trained Transformer Language Models': '992',\n",
        "        'PaLM: Scaling Language Modeling with Pathways': '6144',\n",
        "        'Training Compute-Optimal Large Language Models': 'N/A',\n",
        "        'Efficient Language Modeling with Sparse all-MLP': '32',\n",
        "        'Announcing GPT- NeoX- 20B': '96',\n",
        "        'LaMDA: Language Models for Dialog Applications': '1024',\n",
        "        'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale': 'N/A',\n",
        "        'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding': '1024',\n",
        "        'Generative Pretraining from Pixels': 'N/A',\n",
        "        'Once for all: Train one network and specialize it for efficient deployment.': '32',\n",
        "        'Language models are Few- Shot Learners': 'N/A',\n",
        "        'ProGen: Language Modeling for Protein Generation': '128',\n",
        "        'Turing-NLG: A 17-billion-parameter language model by Microsoft': '256',\n",
        "        'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.': '512',\n",
        "        'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures': '1',\n",
        "        'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm': '64',\n",
        "        'Progressive Neural Architecture Search': '100',\n",
        "        'Mastering the game of Go without human knowledge': '64',\n",
        "        'Dota 2 ': 'N/A',\n",
        "        'Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.': '50',\n",
        "        'Attention Is All You Need': '8',\n",
        "        'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer': '64',\n",
        "        'DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker': '20'\n",
        "    },\n",
        "    'Hardware model': {\n",
        "        'No Language Left Behind: Scaling Human-Centered Machine Translation': 'A100',\n",
        "        'Solving Quantitative Reasoning Problems with Language Models': 'TPUv4',\n",
        "        'Scaling Autoregressive Models for Content-Rich Text-to-Image Generation': 'TPUv4',\n",
        "        'A Generalist Agent': 'TPUv3',\n",
        "        'OPT: Open Pre-trained Transformer Language Models': 'A100',\n",
        "        'PaLM: Scaling Language Modeling with Pathways': 'TPUv4',\n",
        "        'Training Compute-Optimal Large Language Models': 'TPUv3, TPUv4',\n",
        "        'Efficient Language Modeling with Sparse all-MLP': 'V100',\n",
        "        'Announcing GPT- NeoX- 20B': 'A100',\n",
        "        'LaMDA: Language Models for Dialog Applications': 'TPUv3',\n",
        "        'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale': 'TPUv3',\n",
        "        'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding': 'TPUv3',\n",
        "        'Generative Pretraining from Pixels': 'V100',\n",
        "        'Once for all: Train one network and specialize it for efficient deployment.': 'V100',\n",
        "        'Language models are Few- Shot Learners': 'V100',\n",
        "        'ProGen: Language Modeling for Protein Generation': 'TPUv3',\n",
        "        'Turing-NLG: A 17-billion-parameter language model by Microsoft': 'V100',\n",
        "        'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.': 'TPUv3',\n",
        "        'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures': 'P100',\n",
        "        'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm': 'TPUv2',\n",
        "        'Progressive Neural Architecture Search': 'P100',\n",
        "        'Mastering the game of Go without human knowledge': 'N/A',\n",
        "        'Dota 2 ': 'N/A',\n",
        "        'Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.': 'K80',\n",
        "        'Attention Is All You Need': 'P100',\n",
        "        'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer': 'K40',\n",
        "        'DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker': 'N/A'\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['No Language Left Behind: Scaling Human-Centered Machine Translation',\n",
              " 'Solving Quantitative Reasoning Problems with Language Models',\n",
              " 'Scaling Autoregressive Models for Content-Rich Text-to-Image Generation',\n",
              " 'A Generalist Agent',\n",
              " 'OPT: Open Pre-trained Transformer Language Models',\n",
              " 'PaLM: Scaling Language Modeling with Pathways',\n",
              " 'Training Compute-Optimal Large Language Models',\n",
              " 'Efficient Language Modeling with Sparse all-MLP',\n",
              " 'Announcing GPT- NeoX- 20B',\n",
              " 'LaMDA: Language Models for Dialog Applications',\n",
              " 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',\n",
              " 'GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding',\n",
              " 'Generative Pretraining from Pixels',\n",
              " 'Once for all: Train one network and specialize it for efficient deployment.',\n",
              " 'Language models are Few- Shot Learners',\n",
              " 'ProGen: Language Modeling for Protein Generation',\n",
              " 'Turing-NLG: A 17-billion-parameter language model by Microsoft',\n",
              " 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.',\n",
              " 'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures',\n",
              " 'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm',\n",
              " 'Progressive Neural Architecture Search',\n",
              " 'Mastering the game of Go without human knowledge',\n",
              " 'Dota 2 ',\n",
              " 'Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.',\n",
              " 'Attention Is All You Need',\n",
              " 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer',\n",
              " 'DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paper_titles = list(CORRECT_ANSWERS['Hardware model'].keys())\n",
        "print(len(paper_titles))\n",
        "paper_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkwpFPzmWyG6",
        "outputId": "057026f6-0b56-49cd-a84e-2ba4773b4eae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/g6/w_cfz8c507j6mbt5mdv7wt_h0000gn/T/ipykernel_18770/3471291030.py:16: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking into \"Solving Quantitative Reasoning Problems with Language Models\"\n",
            "Full #tokens: 38377\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 6845\n",
            "Answers: ['N/A', 'TPUv4']\n",
            "---\n",
            "Looking into \"PaLM: Scaling Language Modeling with Pathways\"\n",
            "Full #tokens: 82875\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 4045\n",
            "Answers: ['6144', 'TPU v4']\n",
            "---\n",
            "Looking into \"Training Compute-Optimal Large Language Models\"\n",
            "Full #tokens: 30151\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 6502\n",
            "Answers: ['N/A', 'TPUv3/TPUv4']\n",
            "---\n",
            "Looking into \"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\"\n",
            "Full #tokens: 39185\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7653\n",
            "Answers: ['N/A', 'CloudTPUv4']\n",
            "---\n",
            "Looking into \"LaMDA: Language Models for Dialog Applications\"\n",
            "Full #tokens: 38884\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7352\n",
            "Answers: ['1024', 'TPU-v3']\n",
            "---\n",
            "Looking into \"Efficient Language Modeling with Sparse all-MLP\"\n",
            "Full #tokens: 18739\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 2973\n",
            "Answers: ['N/A', '32G V100']\n",
            "---\n",
            "Looking into \"Language models are Few- Shot Learners\"\n",
            "Full #tokens: 65539\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 2475\n",
            "Answers: ['N/A', 'V100']\n",
            "---\n",
            "Looking into \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\"\n",
            "Full #tokens: 30252\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 6603\n",
            "Answers: ['2048', 'TPU v3']\n",
            "---\n",
            "Looking into \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\"\n",
            "Full #tokens: 17746\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 1980\n",
            "Answers: ['N/A', 'Cloud TPU V3']\n",
            "---\n",
            "Looking into \"Once for all: Train one network and specialize it for efficient deployment.\"\n",
            "Full #tokens: 18287\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 2521\n",
            "Answers: ['N/A', 'V100']\n",
            "---\n",
            "Looking into \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\n",
            "Full #tokens: 19310\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 3544\n",
            "Answers: ['N/A', 'TPUv3']\n",
            "---\n",
            "Looking into \"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\"\n",
            "Full #tokens: 22393\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 6627\n",
            "Answers: ['N/A', 'N/A']\n",
            "---\n",
            "Looking into \"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\"\n",
            "Full #tokens: 16930\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 1164\n",
            "Answers: ['N/A', 'TPU']\n",
            "---\n",
            "Looking into \"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.\"\n",
            "Full #tokens: 16263\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 497\n",
            "Answers: ['50', 'K80']\n",
            "---\n",
            "Looking into \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\"\n",
            "Full #tokens: 17550\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 1784\n",
            "Answers: ['64', 'N/A']\n",
            "---\n",
            "Looking into \"Progressive Neural Architecture Search\"\n",
            "Full #tokens: 14421\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 6538\n",
            "Answers: ['N/A', 'P100']\n",
            "---\n",
            "Looking into \"Attention Is All You Need\"\n",
            "Full #tokens: 8302\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 419\n",
            "Answers: ['8', 'P100']\n",
            "---\n",
            "Looking into \"DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker\"\n",
            "Full #tokens: 24685\n",
            "Max #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 7883\n",
            "Chunk #tokens: 1036\n",
            "Answers: ['N/A', 'NVIDIA GeForce GTX 1080']\n",
            "---\n",
            "18\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author(s)</th>\n",
              "      <th>Publication date</th>\n",
              "      <th>Reference</th>\n",
              "      <th>Link</th>\n",
              "      <th>Number of hardware units</th>\n",
              "      <th>Hardware model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aitor Lewkowycz, Anders Andreassen, David Doha...</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>Solving Quantitative Reasoning Problems with L...</td>\n",
              "      <td>https://arxiv.org/abs/2206.14858</td>\n",
              "      <td>N/A</td>\n",
              "      <td>TPUv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aakanksha Chowdhery, Sharan Narang, Jacob Devl...</td>\n",
              "      <td>2022-04-04</td>\n",
              "      <td>PaLM: Scaling Language Modeling with Pathways</td>\n",
              "      <td>https://arxiv.org/abs/2204.02311</td>\n",
              "      <td>6144</td>\n",
              "      <td>TPU v4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
              "      <td>2022-03-29</td>\n",
              "      <td>Training Compute-Optimal Large Language Models</td>\n",
              "      <td>https://arxiv.org/abs/2203.15556</td>\n",
              "      <td>N/A</td>\n",
              "      <td>TPUv3/TPUv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...</td>\n",
              "      <td>2022-06-22</td>\n",
              "      <td>Scaling Autoregressive Models for Content-Rich...</td>\n",
              "      <td>https://arxiv.org/abs/2206.10789v1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>CloudTPUv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Romal Thoppilan, Daniel De Freitas, Jamie Hall...</td>\n",
              "      <td>2022-02-10</td>\n",
              "      <td>LaMDA: Language Models for Dialog Applications</td>\n",
              "      <td>https://arxiv.org/abs/2201.08239</td>\n",
              "      <td>1024</td>\n",
              "      <td>TPU-v3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...</td>\n",
              "      <td>2022-04-14</td>\n",
              "      <td>Efficient Language Modeling with Sparse all-MLP</td>\n",
              "      <td>https://arxiv.org/abs/2203.06850</td>\n",
              "      <td>N/A</td>\n",
              "      <td>32G V100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...</td>\n",
              "      <td>2020-05-28</td>\n",
              "      <td>Language models are Few- Shot Learners</td>\n",
              "      <td>https://arxiv.org/abs/2005.14165</td>\n",
              "      <td>N/A</td>\n",
              "      <td>V100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...</td>\n",
              "      <td>2020-06-30</td>\n",
              "      <td>GShard: Scaling Giant Models with Conditional ...</td>\n",
              "      <td>https://arxiv.org/abs/2006.16668</td>\n",
              "      <td>2048</td>\n",
              "      <td>TPU v3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...</td>\n",
              "      <td>2020-02-09</td>\n",
              "      <td>ALBERT: A Lite BERT for Self-supervised Learni...</td>\n",
              "      <td>https://arxiv.org/abs/1909.11942</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Cloud TPU V3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...</td>\n",
              "      <td>2020-04-29</td>\n",
              "      <td>Once for all: Train one network and specialize...</td>\n",
              "      <td>https://arxiv.org/abs/1908.09791</td>\n",
              "      <td>N/A</td>\n",
              "      <td>V100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...</td>\n",
              "      <td>2020-10-22</td>\n",
              "      <td>An Image is Worth 16x16 Words: Transformers fo...</td>\n",
              "      <td>https://arxiv.org/abs/2010.11929</td>\n",
              "      <td>N/A</td>\n",
              "      <td>TPUv3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...</td>\n",
              "      <td>2018-02-05</td>\n",
              "      <td>IMPALA: Scalable Distributed Deep-RL with Impo...</td>\n",
              "      <td>https://arxiv.org/abs/1802.01561</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>D Silver, T Hubert, J Schrittwieser, I Antonoglou</td>\n",
              "      <td>2017-12-05</td>\n",
              "      <td>Mastering Chess and Shogi by Self-Play with a ...</td>\n",
              "      <td>https://arxiv.org/abs/1712.01815</td>\n",
              "      <td>N/A</td>\n",
              "      <td>TPU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...</td>\n",
              "      <td>2017-08-04</td>\n",
              "      <td>Revisiting Unreasonable Effectiveness of Data ...</td>\n",
              "      <td>https://arxiv.org/abs/1707.02968</td>\n",
              "      <td>50</td>\n",
              "      <td>K80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>N Shazeer, A Mirhoseini, K Maziarz, A Davis</td>\n",
              "      <td>2017-01-23</td>\n",
              "      <td>Outrageously Large Neural Networks: The Sparse...</td>\n",
              "      <td>https://arxiv.org/abs/1701.06538</td>\n",
              "      <td>64</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>C Liu, B Zoph, M Neumann, J Shlens</td>\n",
              "      <td>2017-12-02</td>\n",
              "      <td>Progressive Neural Architecture Search</td>\n",
              "      <td>https://arxiv.org/abs/1712.00559</td>\n",
              "      <td>N/A</td>\n",
              "      <td>P100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...</td>\n",
              "      <td>2017-06-12</td>\n",
              "      <td>Attention Is All You Need</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2017/file...</td>\n",
              "      <td>8</td>\n",
              "      <td>P100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>Matej Moravčík, Martin Schmid, Neil Burch, Vil...</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>DeepStack: Expert-Level Artificial Intelligenc...</td>\n",
              "      <td>https://arxiv.org/abs/1701.01724</td>\n",
              "      <td>N/A</td>\n",
              "      <td>NVIDIA GeForce GTX 1080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Author(s) Publication date  \\\n",
              "3    Aitor Lewkowycz, Anders Andreassen, David Doha...       2022-06-29   \n",
              "4    Aakanksha Chowdhery, Sharan Narang, Jacob Devl...       2022-04-04   \n",
              "6    Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...       2022-03-29   \n",
              "7    Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...       2022-06-22   \n",
              "8    Romal Thoppilan, Daniel De Freitas, Jamie Hall...       2022-02-10   \n",
              "21   Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...       2022-04-14   \n",
              "100  Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...       2020-05-28   \n",
              "103  Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...       2020-06-30   \n",
              "108  Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...       2020-02-09   \n",
              "111  Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...       2020-04-29   \n",
              "132  Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...       2020-10-22   \n",
              "185  Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...       2018-02-05   \n",
              "208  D Silver, T Hubert, J Schrittwieser, I Antonoglou       2017-12-05   \n",
              "211  ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...       2017-08-04   \n",
              "212        N Shazeer, A Mirhoseini, K Maziarz, A Davis       2017-01-23   \n",
              "213                 C Liu, B Zoph, M Neumann, J Shlens       2017-12-02   \n",
              "214  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...       2017-06-12   \n",
              "215  Matej Moravčík, Martin Schmid, Neil Burch, Vil...       2017-01-06   \n",
              "\n",
              "                                             Reference  \\\n",
              "3    Solving Quantitative Reasoning Problems with L...   \n",
              "4        PaLM: Scaling Language Modeling with Pathways   \n",
              "6       Training Compute-Optimal Large Language Models   \n",
              "7    Scaling Autoregressive Models for Content-Rich...   \n",
              "8       LaMDA: Language Models for Dialog Applications   \n",
              "21     Efficient Language Modeling with Sparse all-MLP   \n",
              "100             Language models are Few- Shot Learners   \n",
              "103  GShard: Scaling Giant Models with Conditional ...   \n",
              "108  ALBERT: A Lite BERT for Self-supervised Learni...   \n",
              "111  Once for all: Train one network and specialize...   \n",
              "132  An Image is Worth 16x16 Words: Transformers fo...   \n",
              "185  IMPALA: Scalable Distributed Deep-RL with Impo...   \n",
              "208  Mastering Chess and Shogi by Self-Play with a ...   \n",
              "211  Revisiting Unreasonable Effectiveness of Data ...   \n",
              "212  Outrageously Large Neural Networks: The Sparse...   \n",
              "213             Progressive Neural Architecture Search   \n",
              "214                          Attention Is All You Need   \n",
              "215  DeepStack: Expert-Level Artificial Intelligenc...   \n",
              "\n",
              "                                                  Link  \\\n",
              "3                     https://arxiv.org/abs/2206.14858   \n",
              "4                     https://arxiv.org/abs/2204.02311   \n",
              "6                     https://arxiv.org/abs/2203.15556   \n",
              "7                   https://arxiv.org/abs/2206.10789v1   \n",
              "8                     https://arxiv.org/abs/2201.08239   \n",
              "21                    https://arxiv.org/abs/2203.06850   \n",
              "100                   https://arxiv.org/abs/2005.14165   \n",
              "103                   https://arxiv.org/abs/2006.16668   \n",
              "108                   https://arxiv.org/abs/1909.11942   \n",
              "111                   https://arxiv.org/abs/1908.09791   \n",
              "132                   https://arxiv.org/abs/2010.11929   \n",
              "185                   https://arxiv.org/abs/1802.01561   \n",
              "208                   https://arxiv.org/abs/1712.01815   \n",
              "211                   https://arxiv.org/abs/1707.02968   \n",
              "212                   https://arxiv.org/abs/1701.06538   \n",
              "213                   https://arxiv.org/abs/1712.00559   \n",
              "214  https://proceedings.neurips.cc/paper/2017/file...   \n",
              "215                   https://arxiv.org/abs/1701.01724   \n",
              "\n",
              "    Number of hardware units           Hardware model  \n",
              "3                        N/A                    TPUv4  \n",
              "4                       6144                   TPU v4  \n",
              "6                        N/A              TPUv3/TPUv4  \n",
              "7                        N/A               CloudTPUv4  \n",
              "8                       1024                   TPU-v3  \n",
              "21                       N/A                 32G V100  \n",
              "100                      N/A                     V100  \n",
              "103                     2048                   TPU v3  \n",
              "108                      N/A             Cloud TPU V3  \n",
              "111                      N/A                     V100  \n",
              "132                      N/A                    TPUv3  \n",
              "185                      N/A                      N/A  \n",
              "208                      N/A                      TPU  \n",
              "211                       50                      K80  \n",
              "212                       64                      N/A  \n",
              "213                      N/A                     P100  \n",
              "214                        8                     P100  \n",
              "215                      N/A  NVIDIA GeForce GTX 1080  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download dataset from the Parameters, Compute and Data Trends in ML sheet\n",
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/export?format=csv#gid=0')\n",
        "\n",
        "year_start = 2017\n",
        "\n",
        "# Recode columns\n",
        "df['Publication date'] = pd.to_datetime(df['Publication date'], errors='coerce', dayfirst=True)\n",
        "\n",
        "# Filter for papers of only the last 5 years\n",
        "df = df[df['Publication date'] > f'{year_start}-01-01']\n",
        "\n",
        "# Keep only bibliographical data\n",
        "df = df.filter(['Author(s)', 'Publication date', 'Reference', 'Link'])\n",
        "df = df[df['Link'].notna()]\n",
        "# Keep only links which forward to a pdf or an arxiv link\n",
        "df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n",
        "\n",
        "keys = ['Number of hardware units', 'Hardware model']\n",
        "\n",
        "# Enable for test running with the first ten papers\n",
        "# df = df[1:11]\n",
        "# Or a specific paper\n",
        "# idx = 4\n",
        "# df = df[idx:idx+1]\n",
        "# Or by title\n",
        "df = df[df['Reference'].isin(paper_titles)]\n",
        "df = df.drop_duplicates(subset=['Reference'], keep='first')\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    # try:\n",
        "    parse_paper(df, i, row, keys)\n",
        "    print(\"---\")\n",
        "    # except:\n",
        "    #     continue\n",
        "print(len(df))\n",
        "display(df)\n",
        "\n",
        "timestamp = datetime.datetime.now()\n",
        "df.to_csv(f'output_data/parsed_paper_data_{timestamp.strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author(s)</th>\n",
              "      <th>Publication date</th>\n",
              "      <th>Reference</th>\n",
              "      <th>Link</th>\n",
              "      <th>Number of hardware units</th>\n",
              "      <th>Hardware model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aitor Lewkowycz, Anders Andreassen, David Doha...</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>Solving Quantitative Reasoning Problems with L...</td>\n",
              "      <td>https://arxiv.org/abs/2206.14858</td>\n",
              "      <td>N/A</td>\n",
              "      <td>TPUv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aakanksha Chowdhery, Sharan Narang, Jacob Devl...</td>\n",
              "      <td>2022-04-04</td>\n",
              "      <td>PaLM: Scaling Language Modeling with Pathways</td>\n",
              "      <td>https://arxiv.org/abs/2204.02311</td>\n",
              "      <td>6144</td>\n",
              "      <td>TPU v4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
              "      <td>2022-03-29</td>\n",
              "      <td>Training Compute-Optimal Large Language Models</td>\n",
              "      <td>https://arxiv.org/abs/2203.15556</td>\n",
              "      <td>N/A</td>\n",
              "      <td>TPUv3/TPUv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...</td>\n",
              "      <td>2022-06-22</td>\n",
              "      <td>Scaling Autoregressive Models for Content-Rich...</td>\n",
              "      <td>https://arxiv.org/abs/2206.10789v1</td>\n",
              "      <td>N/A</td>\n",
              "      <td>CloudTPUv4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Romal Thoppilan, Daniel De Freitas, Jamie Hall...</td>\n",
              "      <td>2022-02-10</td>\n",
              "      <td>LaMDA: Language Models for Dialog Applications</td>\n",
              "      <td>https://arxiv.org/abs/2201.08239</td>\n",
              "      <td>1024</td>\n",
              "      <td>TPU-v3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...</td>\n",
              "      <td>2022-04-14</td>\n",
              "      <td>Efficient Language Modeling with Sparse all-MLP</td>\n",
              "      <td>https://arxiv.org/abs/2203.06850</td>\n",
              "      <td>N/A</td>\n",
              "      <td>32G V100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...</td>\n",
              "      <td>2020-05-28</td>\n",
              "      <td>Language models are Few- Shot Learners</td>\n",
              "      <td>https://arxiv.org/abs/2005.14165</td>\n",
              "      <td>N/A</td>\n",
              "      <td>V100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...</td>\n",
              "      <td>2020-06-30</td>\n",
              "      <td>GShard: Scaling Giant Models with Conditional ...</td>\n",
              "      <td>https://arxiv.org/abs/2006.16668</td>\n",
              "      <td>2048</td>\n",
              "      <td>TPU v3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...</td>\n",
              "      <td>2020-02-09</td>\n",
              "      <td>ALBERT: A Lite BERT for Self-supervised Learni...</td>\n",
              "      <td>https://arxiv.org/abs/1909.11942</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Cloud TPU V3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...</td>\n",
              "      <td>2020-04-29</td>\n",
              "      <td>Once for all: Train one network and specialize...</td>\n",
              "      <td>https://arxiv.org/abs/1908.09791</td>\n",
              "      <td>N/A</td>\n",
              "      <td>V100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...</td>\n",
              "      <td>2020-10-22</td>\n",
              "      <td>An Image is Worth 16x16 Words: Transformers fo...</td>\n",
              "      <td>https://arxiv.org/abs/2010.11929</td>\n",
              "      <td>N/A</td>\n",
              "      <td>TPUv3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...</td>\n",
              "      <td>2018-02-05</td>\n",
              "      <td>IMPALA: Scalable Distributed Deep-RL with Impo...</td>\n",
              "      <td>https://arxiv.org/abs/1802.01561</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>D Silver, T Hubert, J Schrittwieser, I Antonoglou</td>\n",
              "      <td>2017-12-05</td>\n",
              "      <td>Mastering Chess and Shogi by Self-Play with a ...</td>\n",
              "      <td>https://arxiv.org/abs/1712.01815</td>\n",
              "      <td>N/A</td>\n",
              "      <td>TPU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...</td>\n",
              "      <td>2017-08-04</td>\n",
              "      <td>Revisiting Unreasonable Effectiveness of Data ...</td>\n",
              "      <td>https://arxiv.org/abs/1707.02968</td>\n",
              "      <td>50</td>\n",
              "      <td>K80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>N Shazeer, A Mirhoseini, K Maziarz, A Davis</td>\n",
              "      <td>2017-01-23</td>\n",
              "      <td>Outrageously Large Neural Networks: The Sparse...</td>\n",
              "      <td>https://arxiv.org/abs/1701.06538</td>\n",
              "      <td>64</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>C Liu, B Zoph, M Neumann, J Shlens</td>\n",
              "      <td>2017-12-02</td>\n",
              "      <td>Progressive Neural Architecture Search</td>\n",
              "      <td>https://arxiv.org/abs/1712.00559</td>\n",
              "      <td>N/A</td>\n",
              "      <td>P100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...</td>\n",
              "      <td>2017-06-12</td>\n",
              "      <td>Attention Is All You Need</td>\n",
              "      <td>https://proceedings.neurips.cc/paper/2017/file...</td>\n",
              "      <td>8</td>\n",
              "      <td>P100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>Matej Moravčík, Martin Schmid, Neil Burch, Vil...</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>DeepStack: Expert-Level Artificial Intelligenc...</td>\n",
              "      <td>https://arxiv.org/abs/1701.01724</td>\n",
              "      <td>N/A</td>\n",
              "      <td>NVIDIA GeForce GTX 1080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Author(s) Publication date  \\\n",
              "3    Aitor Lewkowycz, Anders Andreassen, David Doha...       2022-06-29   \n",
              "4    Aakanksha Chowdhery, Sharan Narang, Jacob Devl...       2022-04-04   \n",
              "6    Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...       2022-03-29   \n",
              "7    Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...       2022-06-22   \n",
              "8    Romal Thoppilan, Daniel De Freitas, Jamie Hall...       2022-02-10   \n",
              "21   Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...       2022-04-14   \n",
              "100  Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...       2020-05-28   \n",
              "103  Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...       2020-06-30   \n",
              "108  Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...       2020-02-09   \n",
              "111  Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...       2020-04-29   \n",
              "132  Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...       2020-10-22   \n",
              "185  Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...       2018-02-05   \n",
              "208  D Silver, T Hubert, J Schrittwieser, I Antonoglou       2017-12-05   \n",
              "211  ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...       2017-08-04   \n",
              "212        N Shazeer, A Mirhoseini, K Maziarz, A Davis       2017-01-23   \n",
              "213                 C Liu, B Zoph, M Neumann, J Shlens       2017-12-02   \n",
              "214  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...       2017-06-12   \n",
              "215  Matej Moravčík, Martin Schmid, Neil Burch, Vil...       2017-01-06   \n",
              "\n",
              "                                             Reference  \\\n",
              "3    Solving Quantitative Reasoning Problems with L...   \n",
              "4        PaLM: Scaling Language Modeling with Pathways   \n",
              "6       Training Compute-Optimal Large Language Models   \n",
              "7    Scaling Autoregressive Models for Content-Rich...   \n",
              "8       LaMDA: Language Models for Dialog Applications   \n",
              "21     Efficient Language Modeling with Sparse all-MLP   \n",
              "100             Language models are Few- Shot Learners   \n",
              "103  GShard: Scaling Giant Models with Conditional ...   \n",
              "108  ALBERT: A Lite BERT for Self-supervised Learni...   \n",
              "111  Once for all: Train one network and specialize...   \n",
              "132  An Image is Worth 16x16 Words: Transformers fo...   \n",
              "185  IMPALA: Scalable Distributed Deep-RL with Impo...   \n",
              "208  Mastering Chess and Shogi by Self-Play with a ...   \n",
              "211  Revisiting Unreasonable Effectiveness of Data ...   \n",
              "212  Outrageously Large Neural Networks: The Sparse...   \n",
              "213             Progressive Neural Architecture Search   \n",
              "214                          Attention Is All You Need   \n",
              "215  DeepStack: Expert-Level Artificial Intelligenc...   \n",
              "\n",
              "                                                  Link  \\\n",
              "3                     https://arxiv.org/abs/2206.14858   \n",
              "4                     https://arxiv.org/abs/2204.02311   \n",
              "6                     https://arxiv.org/abs/2203.15556   \n",
              "7                   https://arxiv.org/abs/2206.10789v1   \n",
              "8                     https://arxiv.org/abs/2201.08239   \n",
              "21                    https://arxiv.org/abs/2203.06850   \n",
              "100                   https://arxiv.org/abs/2005.14165   \n",
              "103                   https://arxiv.org/abs/2006.16668   \n",
              "108                   https://arxiv.org/abs/1909.11942   \n",
              "111                   https://arxiv.org/abs/1908.09791   \n",
              "132                   https://arxiv.org/abs/2010.11929   \n",
              "185                   https://arxiv.org/abs/1802.01561   \n",
              "208                   https://arxiv.org/abs/1712.01815   \n",
              "211                   https://arxiv.org/abs/1707.02968   \n",
              "212                   https://arxiv.org/abs/1701.06538   \n",
              "213                   https://arxiv.org/abs/1712.00559   \n",
              "214  https://proceedings.neurips.cc/paper/2017/file...   \n",
              "215                   https://arxiv.org/abs/1701.01724   \n",
              "\n",
              "    Number of hardware units           Hardware model  \n",
              "3                        N/A                    TPUv4  \n",
              "4                       6144                   TPU v4  \n",
              "6                        N/A              TPUv3/TPUv4  \n",
              "7                        N/A               CloudTPUv4  \n",
              "8                       1024                   TPU-v3  \n",
              "21                       N/A                 32G V100  \n",
              "100                      N/A                     V100  \n",
              "103                     2048                   TPU v3  \n",
              "108                      N/A             Cloud TPU V3  \n",
              "111                      N/A                     V100  \n",
              "132                      N/A                    TPUv3  \n",
              "185                      N/A                      N/A  \n",
              "208                      N/A                      TPU  \n",
              "211                       50                      K80  \n",
              "212                       64                      N/A  \n",
              "213                      N/A                     P100  \n",
              "214                        8                     P100  \n",
              "215                      N/A  NVIDIA GeForce GTX 1080  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of hardware units\n",
            "N/A != 1024\n",
            "N/A != 32\n",
            "2048 != 1024\n",
            "N/A != 512\n",
            "N/A != 32\n",
            "N/A != 1\n",
            "N/A != 64\n",
            "N/A != 100\n",
            "N/A != 20\n",
            "Hardware model\n",
            "TPU v4 != TPUv4\n",
            "TPUv3/TPUv4 != TPUv3, TPUv4\n",
            "CloudTPUv4 != TPUv4\n",
            "TPU-v3 != TPUv3\n",
            "32G V100 != V100\n",
            "TPU v3 != TPUv3\n",
            "Cloud TPU V3 != TPUv3\n",
            "N/A != P100\n",
            "TPU != TPUv2\n",
            "N/A != K40\n",
            "NVIDIA GeForce GTX 1080 != N/A\n",
            "Number of hardware units: 9/18\n",
            "Hardware model: 7/18\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Number of hardware units': 9, 'Hardware model': 7}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_answers(df, keys, CORRECT_ANSWERS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Count tokens in papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_davinci_tokens = 0\n",
        "total_gpt_tokens = 0\n",
        "davinci_tokenizer = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
        "gpt4_tokenizer = tiktoken.encoding_for_model(\"gpt-4\")  # same for gpt-3.5-turbo\n",
        "for i, row in df.iterrows():\n",
        "    url = row['Link']\n",
        "    # replace \"abs\" with \"pdf\" in arxiv url links\n",
        "    url = url.replace('abs', 'pdf')\n",
        "    paper_title = row['Reference'].replace(' ', '_').replace(':', '').replace('\"', '').lower()\n",
        "    with open('input_data/' + paper_title + '.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    davinci_tokens = davinci_tokenizer.encode(text, disallowed_special=())\n",
        "    gpt_tokens = gpt4_tokenizer.encode(text, disallowed_special=())\n",
        "\n",
        "    total_davinci_tokens += len(davinci_tokens)\n",
        "    total_gpt_tokens += len(gpt_tokens)\n",
        "\n",
        "    print(f\"{row['Reference']}: {len(davinci_tokens)} tokens for davinci, {len(gpt_tokens)} tokens for gpt-4\")\n",
        "\n",
        "print(f\"Total tokens: {total_davinci_tokens} for davinci, {total_gpt_tokens} for gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(gpt4_tokenizer.encode(chat_message_template))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b7c6d073edd95e87cfa6524fac94a63a54fb86c88ec13e975ab85409bf024d2f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
