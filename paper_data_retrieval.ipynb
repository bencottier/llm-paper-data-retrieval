{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieving data from papers using GPT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /Users/bencottier/miniconda3/envs/nlp\n",
            "\n",
            "  added / updated specs:\n",
            "    - openai\n",
            "\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    conda-forge::ca-certificates-2022.12.~ --> pkgs/main::ca-certificates-2023.01.10-hecd8cb5_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  certifi            conda-forge/noarch::certifi-2022.12.7~ --> pkgs/main/osx-64::certifi-2022.12.7-py39hecd8cb5_0\n",
            "  openssl            conda-forge::openssl-1.1.1t-hfd90126_0 --> pkgs/main::openssl-1.1.1t-hca72f7f_0\n",
            "\n",
            "\n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda install -y openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ee3887XnrDa",
        "outputId": "5cde1173-289a-44e4-d6a1-c911b058c4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.11.0\n",
            "  latest version: 23.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /Users/bencottier/miniconda3/envs/nlp\n",
            "\n",
            "  added / updated specs:\n",
            "    - pdfminer.six\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2023.5.7   |       h8857fd0_0         145 KB  conda-forge\n",
            "    certifi-2023.5.7           |     pyhd8ed1ab_0         149 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         294 KB\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2023.5.7-h8857fd0_0\n",
            "  certifi            pkgs/main/osx-64::certifi-2022.12.7-p~ --> conda-forge/noarch::certifi-2023.5.7-pyhd8ed1ab_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  openssl              pkgs/main::openssl-1.1.1t-hca72f7f_0 --> conda-forge::openssl-1.1.1t-hfd90126_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "ca-certificates-2023 | 145 KB    | ##################################### | 100% \n",
            "certifi-2023.5.7     | 149 KB    | ##################################### | 100% \n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda install -y -c conda-forge pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from pdfminer.high_level import extract_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('output_data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_paper_text = \"\"\"\n",
        "2 Related work\n",
        "\n",
        "Language models and dialog models: Language models have attracted much attention recently thanks to their\n",
        "successes in NLP applications (e.g., [19, 20, 21, 2, 1, 22, 23, 5, 12, 24]). Our study of scaling laws with respect to\n",
        "model sizes is inspired by recent work on the scaling laws of neural language models [12, 13]. Similar to their ﬁndings,\n",
        "our results show that model scaling improves our quality (sensibleness, speciﬁcity, and interestingness), safety and\n",
        "groundedness metrics to some extent. However, ﬁne-tuning combined with scaling signiﬁcantly improves performance\n",
        "on all metrics.\n",
        "\n",
        "Our work is also closely related to recent successes in applying language models to dialog modeling (e.g., [25, 26,\n",
        "17, 18]), which built on earlier research in neural dialog modeling (e.g., [14, 15, 16, 27, 28]). One of our ﬁne-tuning\n",
        "stages requires training on dialog-only data, which is related to Wolf et al. [29], Dinan et al. [25] and Zhang et al. [30].\n",
        "Our use of ﬁne-tuning on crowdworker-annotated data to improve interestingness is comparable to Roller et al. [18].\n",
        "However, we aim to maximize the interestingness of the model’s output distinctly from its ability to engage the user in\n",
        "further interaction.\n",
        "\n",
        "Our ﬁnding that pure scaling has a limited effect on key measures of open-domain dialog model performance echoes\n",
        "that of Shuster et al. [31], who also focus on the problem of groundedness. Recent studies on scaling have found that\n",
        "performance on question-answering tasks improves with model size [32, 33], similar to our ﬁndings on pre-trained\n",
        "LaMDA prior to ﬁne-tuning.\n",
        "\n",
        "Our approach to improving model groundedness is broadly consistent with a growing literature on augmenting neural\n",
        "language models with retrieval systems. Most of the existing literature focuses on the problem of open-domain\n",
        "question-answering rather than dialog generation, and the models themselves are used to index and rank knowledge\n",
        "sources, rather than trained to use an intermediate tool. Given these differences, we note that the range of existing\n",
        "approaches to this problem include the RNNLM [34], RAG [35], REALM [36], and FiD [37] architectures. Zhu et\n",
        "al. [38] provide a survey of further recent work. See Karpukhin et al. [39] for details on the ‘dense passage retriever’\n",
        "used in RAG. Recent work in this direction has expanded and elaborated on neural models’ ability to retrieve and rank\n",
        "passages [40]. The RETRO architecture demonstrates that language models can be primed with results retrieved from\n",
        "a database as large as two trillion tokens [41]. At a broad level, our approach is also comparable to that of Byrne et\n",
        "al. [42], which ﬁne-tunes the model to use external APIs for movie ticketing dialog.\n",
        "\n",
        "Parts of our ﬁndings are similar to recent studies on dialog groundedness. Granting access to external knowledge\n",
        "bases has been shown to reduce the rate at which models hallucinate unsourced statements in dialog across a variety of\n",
        "retrieval systems and model architectures [31]. Another study ﬁnds that a question-answering system’s accuracy is\n",
        "improved by separating it into a reasoning unit and a response generator, analogous to our separation of ‘Base’ and\n",
        "‘Research’ models in our study [43]. Meanwhile, the WebGPT framework includes a language system that can interact\n",
        "with the open web via a text-only interface, and learns to imitate humans in answering questions by citing external\n",
        "sources [44]. Komeili et al. [45] compare different types of pre-trained models and retrieval methods, and reach a\n",
        "similar conclusion that augmenting language models with a search engine provides more factually grounded responses.\n",
        "They encode the input context with grounded information from search to generate the next response, while we augment\n",
        "the generated responses with information from known sources in our method. This allows us to ﬁne-tune the model for\n",
        "groundedness without sacriﬁcing gains in safety or quality from other ﬁne-tuning treatments.\n",
        "\n",
        "Dialog metrics: Deﬁning effective metrics for dialog models remains an open research topic. Our approach is\n",
        "inspired by Adiwardana et al. [17], who argued for human-like metrics, such as sensibleness and speciﬁcity. Many\n",
        "automated metrics for dialog models have been studied, including perplexity [16, 17], F1, Hits@1/N [25], USR [46],\n",
        "or BLEU/ROUGE [47, 15, 27]. However, such automated metrics may not correlate well with human judgment [48].\n",
        "More reliable metrics for dialog modeling require human evaluation [49, 50, 18, 25, 17, 51], as used in this paper.\n",
        "\n",
        "Earlier research attempted to combine multifaceted evaluations of dialog quality into a single headline metric [52]. We\n",
        "follow the pattern established in Adiwardana et al. [17] and Roller et al. [18] by considering the different components\n",
        "of our evaluations separately. In addition to sensibleness and speciﬁcity per Adiwardana et al. [17], we add new metrics:\n",
        "interestingness, safety, and groundedness. An advantage of using several different metrics is their debuggability: by\n",
        "exploring responses with low safety or groundedness scores, we have been able to develop targeted methods to improve\n",
        "them.\n",
        "\n",
        "Safety and safety of dialog models:\n",
        "Inappropriate and unsafe risks and behaviors of language models have been\n",
        "extensively discussed and studied in previous works (e.g., [53, 54]). Issues encountered include toxicity (e.g., [55, 56,\n",
        "57]), bias (e.g., [58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]), and inappropriately revealing personally\n",
        "identifying information (PII) from training data [73]. Weidinger et al. [54] identify 21 risks associated with large-scale\n",
        "\n",
        "3\n",
        "\n",
        "\flanguage models and discuss the points of origin for these risks. While many mitigation strategies have also been\n",
        "suggested (e.g., [74, 75, 76, 77, 78, 79, 80, 81, 82]), meaningfully addressing these issues remains an active research\n",
        "area.\n",
        "\n",
        "Similar issues have also been discussed speciﬁcally for dialog models [53]. For instance, examples of bias, offensiveness,\n",
        "and hate speech have been found both in training data drawn from social media, and consequently in the output of dialog\n",
        "models trained on such data [83]. Dialog models [84] can learn, and even amplify, biases in the training data. Echoing\n",
        "Gehman et al. [85], we ﬁnd ﬁne-tuning effective to augment language models for safety. The method we use in this\n",
        "paper follows previous attempts to tackle these issues by training separate layers to detect unsafe output [17, 86, 18, 79].\n",
        "Our strategy is similar to recent work that also uses ﬁne-tuning [87]. While their safety guidelines were derived from\n",
        "human rights principles, they similarly ﬁnd that increasing scale has no impact on toxicity metrics, while ﬁne-tuning on\n",
        "safety evaluations does.\n",
        "\n",
        "Groundedness metrics: Similar to other recent research into groundedness cited above, we assess groundedness\n",
        "by asking crowdworkers to judge whether the model’s output is in accordance with authoritative external sources.\n",
        "The recently-proposed Attributable to Identiﬁed Sources (AIS) framework [88] articulates a more precise approach\n",
        "to assess output of language models that pertains to the external world. It splits evaluation into two stages, where\n",
        "crowdworkers are asked: (1) if they can understand and identify the information shared in a dialog turn, and (2) if all\n",
        "of this information can be attributed to a source. Meanwhile, a recent study has reopened the question of automatic\n",
        "evaluation, with the Q2 metric showing performance comparable to human annotation [89].\n",
        "\n",
        "3 LaMDA pre-training\n",
        "\n",
        "LaMDA was pre-trained to predict the next token in a text corpus. Unlike previous dialog models trained on dialog data\n",
        "alone [17, 18], we pre-trained LaMDA on a dataset created from public dialog data and other public web documents.\n",
        "Therefore, LaMDA can be used as a general language model prior to ﬁne-tuning.\n",
        "\n",
        "The pre-training dataset consists of 2.97B documents, 1.12B dialogs, and 13.39B dialog utterances, for a total of 1.56T\n",
        "words (Appendix E). Over 90% of the pre-training dataset is in the English language. We used the SentencePiece\n",
        "library [90] to tokenize the dataset into 2.81T byte pair encoding (BPE) tokens [91], with a vocabulary of 32K tokens.\n",
        "For comparison, the total number of words in the training set for Meena [17] was 40B words, which is nearly 40x\n",
        "smaller.\n",
        "\n",
        "The largest LaMDA model has 137B non-embedding parameters, which is ~50x more parameters than Meena [17].\n",
        "We use a decoder-only Transformer [92] language model as the model architecture for LaMDA. The Transformer has\n",
        "64 layers, dmodel = 8192, df f = 65536, h = 128, dk = dv = 128, relative attention as described in T5 [11], and\n",
        "gated-GELU activation as described in Raffel et al. [93].\n",
        "\n",
        "We pre-trained LaMDA on 1024 TPU-v3 chips for a total of about 57.7 days, and 256K tokens per batch. We used\n",
        "the Lingvo framework [94] for training and achieved 123 TFLOPS/sec with 56.5% FLOPS utilization with the 2D\n",
        "sharding algorithm, as described in GSPMD [95] (see Section 10 for carbon footprint estimates). We also trained\n",
        "smaller 2B-parameter and 8B-parameter models to measure the effects of model scaling on our metrics. Hyperparameter\n",
        "details for the models of different sizes can be found in Table 27, Appendix D.\n",
        "\n",
        "Figure 2 gives an overview of the pre-training stage. We call the model before any ﬁne-tuning \"PT\", for PreTrained.\n",
        "\n",
        "Figure 2: LaMDA pre-training as a language model.\n",
        "\n",
        "4\n",
        "\n",
        "\fPT uses the same sample-and-rank strategy as Meena [17] for decoding. We ﬁrst sample 16 independent candidate\n",
        "responses using top-k (k = 40) sampling (no temperature). The ﬁnal output is the highest-scoring candidate, where the\n",
        "score is based on the candidate’s log-likelihood and its length.\n",
        "\n",
        "4 Metrics\n",
        "\n",
        "Evaluating generative models in general, and open-ended dialog models in particular, is difﬁcult. See the Related\n",
        "Work section for a general review of recent work in this area. In this section, we describe the metrics that we use for\n",
        "evaluation.\n",
        "\n",
        "4.1 Foundation metrics: Quality, Safety and Groundedness\n",
        "\n",
        "Sensibleness, Speciﬁcity, Interestingness (SSI): Our overall quality score is an average of sensibleness, speciﬁcity,\n",
        "and interestingness (SSI).\n",
        "\n",
        "Adiwardana et al. [17] propose the sensibleness and speciﬁcity average (SSA) metric to measure the quality of Meena.\n",
        "This metric is a simple average of two scores: sensibleness and speciﬁcity.\n",
        "\n",
        "The ﬁrst score, sensibleness, measures whether a model’s responses make sense in context and do not contradict\n",
        "anything that was said earlier. Humans tend to take this basic aspect of communication for granted, but generative\n",
        "models often struggle to meet this requirement. However, if sensibleness alone is used to evaluate models, we could\n",
        "inadvertently reward models for playing it safe by always producing short, generic, and boring responses. The\n",
        "GenericBot algorithm [17], which answers every question with “I don’t know” and every statement with “Ok,” scores\n",
        "70% on sensibleness, which even surpasses some large dialog models [17].\n",
        "\n",
        "The second score, speciﬁcity, is used to measure whether a response is speciﬁc to a given context. For example, if a user\n",
        "says “I love Eurovision” and the model responds “Me too,” then it would score 0 on speciﬁcity, since this response could\n",
        "be used in many different contexts. If it answers “Me too. I love Eurovision songs,” then it would score 1. Adiwardana\n",
        "et al. [17] report that Meena narrows the gap to average human performance in the SSA metric.\n",
        "\n",
        "As the model’s performance increases, however, we ﬁnd that sensibleness and speciﬁcity are not sufﬁcient to measure\n",
        "the quality of a dialog model. For example, a response to “How do I throw a ball?” could be “You can throw a ball by\n",
        "ﬁrst picking it up and then throwing it”, which makes sense and is speciﬁc to the question. An alternative deeper and\n",
        "more satisfying answer could be “One way to toss a ball is to hold it ﬁrmly in both hands and then swing your arm\n",
        "down and up again, extending your elbow and then releasing the ball upwards.”\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-7EJDodhVtqVMFdhqaqJqkZLn8QU4m at 0x7f7df05b5630> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"| ---------------------- | -------------------------- | ------ |\\n| 6144                   | TPU v4                    | N/A    |\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1683644836,\n",
              "  \"id\": \"cmpl-7EJDodhVtqVMFdhqaqJqkZLn8QU4m\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 27,\n",
              "    \"prompt_tokens\": 753,\n",
              "    \"total_tokens\": 780\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=f\"A table summarizing the training hardware from this paper:\\n\\n====\\n\\n{example_paper_text}\\n\\n====\\n\\n| Number of GPUs or TPUs | Hardware model (e.g. A100) | FLOP/s |\\n\",\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. N/A\n",
            "2. TPUv4\n",
            "3. N/A\n"
          ]
        }
      ],
      "source": [
        "prompt_text = f\"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "1. How many GPUs or TPUs were used to train the model? Just state the number. If the number of GPUs or TPUs is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "3. What FLOP/s (AKA: FLOP/second, FLOPS) was achieved during training? Include the same units as written in the paper. If FLOP/s is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "3. 21 TFLOP/s\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "3. 21 petaflops\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "3. 127e12 FLOPS\n",
        "\n",
        "====\n",
        "\n",
        "{example_paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt_text,\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7EKgaCRIo9oPO03hur3oopCp9ly1t at 0x7f7da0af64a0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"1. 1024 TPUs were used to pre-train the LaMDA model.\\n2. TPU-v3 was used to pre-train the LaMDA model.\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1683650464,\n",
              "  \"id\": \"chatcmpl-7EKgaCRIo9oPO03hur3oopCp9ly1t\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 37,\n",
              "    \"prompt_tokens\": 3071,\n",
              "    \"total_tokens\": 3108\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_text = \"\"\"\n",
        "Read the following excerpt of a Machine Learning research paper and answer the questions below.\n",
        "1. How many GPUs or TPUs were used to train the model? Just state the number. If the number of GPUs or TPUs is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\"\"\"\n",
        "\n",
        "openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a world expert in Machine Learning.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"{prompt_text}\\n\\n====\\n\\n{example_paper_text}\\n\\n====\\n\\n\"},\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_message_template = \"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def parse_text_gpt_chat(text):\n",
        "    prompt_text = chat_message_template.format(paper_text=text)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt_text},\n",
        "        ]\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def parse_gpt_chat_response(response):\n",
        "    # E.g. \"1. 6144 TPUs\\n2. TPU v4\\n3. N/A\\n\"\n",
        "    answers = response[\"choices\"][0][\"message\"][\"content\"].strip().split(\"\\n\")\n",
        "    answers = [a.split(\".\")[-1].strip() for a in answers]\n",
        "    return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "638"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chat_message_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "3. What FLOP/s (AKA: FLOP/second, FLOPS) was achieved during training? Include the same units as written in the paper. If FLOP/s is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "3. 21 TFLOP/s\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "3. 21 petaflops\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "3. 127e12 FLOPS\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def parse_text_gpt(text):\n",
        "    prompt_text = prompt_template.format(paper_text=text)\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=prompt_text,\n",
        "        temperature=0,\n",
        "        max_tokens=100,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def parse_gpt_response(response):\n",
        "    # E.g. \"1. 6144 TPUs\\n2. TPU v4\\n3. N/A\\n\"\n",
        "    answers = response[\"choices\"][0][\"text\"].strip().split(\"\\n\")\n",
        "    answers = [a.split(\".\")[-1].strip() for a in answers]\n",
        "    return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I've heard that English has about 4 chars per token on average.\n",
        "# But papers may have parts with a lot of digits, which I think are one token each.\n",
        "# GPT-3 token limit (including output) is 4096.\n",
        "# 4096 * 2 minus the initial prompt should be safe.\n",
        "CHAR_LIMIT = 4096*2 - len(chat_message_template)\n",
        "NUM_QUESTIONS = 2\n",
        "\n",
        "def parse_paper(df, i, row, keys):\n",
        "    url = row['Link']\n",
        "\n",
        "    # replace \"abs\" with \"pdf\" in arxiv url links\n",
        "    url = url.replace('abs', 'pdf')\n",
        "    print(f\"Looking into \\\"{row['Reference']}\\\"\")\n",
        "\n",
        "    # try:\n",
        "    #     response = requests.get(url)\n",
        "    # except Exception as e:\n",
        "    #     print(f\"There's something wrong with downloading: {e}\")\n",
        "    #     raise e\n",
        "\n",
        "    # file = open(\"download.pdf\", \"wb\")\n",
        "    # file.seek(0) # overwrite previous file\n",
        "    # file.write(response.content)\n",
        "    # file.close()\n",
        "\n",
        "    try:\n",
        "        # text = extract_text('download.pdf')\n",
        "        paper_title = row['Reference'].replace(' ', '_').replace(':', '').replace('\"', '').lower()\n",
        "        with open('input_data/' + paper_title + '.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        text_pos = 0\n",
        "        final_answers = [None] * NUM_QUESTIONS\n",
        "        while text_pos < len(text):\n",
        "            # Get model answers for the next chunk of the text\n",
        "            text_chunk = text[text_pos : text_pos + CHAR_LIMIT]\n",
        "            answers = parse_gpt_chat_response(parse_text_gpt_chat(text_chunk))\n",
        "            # Process each answer\n",
        "            for i in range(NUM_QUESTIONS):\n",
        "                if final_answers[i] is None and answers[i] is not None:\n",
        "                    # Take the first answer as the final answer initially\n",
        "                    final_answers[i] = answers[i]\n",
        "                elif \"N/A\" in final_answers[i] and not (\"N/A\" in answers[i]):\n",
        "                    \"\"\"\n",
        "                    If the answer was \"N/A\" previously but there's at least \n",
        "                    one non-\"N/A\" answer for a later chunk, then use the \n",
        "                    first non-\"N/A\" answer as the final answer\n",
        "                    \"\"\"\n",
        "                    final_answers[i] = answers[i]\n",
        "            # Move to the next chunk of text\n",
        "            text_pos += CHAR_LIMIT\n",
        "        \n",
        "        for key, answer in zip(keys, final_answers):\n",
        "            df.loc[i,key] = answer if answer else \"none\"\n",
        "    except Exception as e:\n",
        "        print(f\"There's something wrong with extracting the text: {e}\")\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkwpFPzmWyG6",
        "outputId": "057026f6-0b56-49cd-a84e-2ba4773b4eae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/g6/w_cfz8c507j6mbt5mdv7wt_h0000gn/T/ipykernel_1162/93879255.py:16: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking into \"GPT-4 Technical Report\"\n",
            "---\n",
            "Looking into \"Phenaki: Variable Length Video Generation From Open Domain Textual Description\"\n",
            "---\n",
            "Looking into \"Solving Quantitative Reasoning Problems with Language Models\"\n",
            "---\n",
            "Looking into \"PaLM: Scaling Language Modeling with Pathways\"\n",
            "---\n",
            "Looking into \"Training Compute-Optimal Large Language Models\"\n",
            "---\n",
            "Looking into \"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\"\n",
            "---\n",
            "Looking into \"LaMDA: Language Models for Dialog Applications\"\n",
            "---\n",
            "Looking into \"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\"\n",
            "---\n",
            "Looking into \"High-Resolution Image Synthesis with Latent Diffusion Models\"\n",
            "There's something wrong with extracting the text: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ec853e2be56148771033a5ee58bd849c in your message.)\n",
            "Looking into \"Robust Speech Recognition via Large-Scale Weak Supervision\"\n",
            "There's something wrong with extracting the text: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d4cf1e8864a9c967a515d9f3100caad5 in your message.)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author(s)</th>\n",
              "      <th>Publication date</th>\n",
              "      <th>Reference</th>\n",
              "      <th>Link</th>\n",
              "      <th>Number of hardware units</th>\n",
              "      <th>Hardware model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OpenAI</td>\n",
              "      <td>2023-03-15</td>\n",
              "      <td>GPT-4 Technical Report</td>\n",
              "      <td>https://arxiv.org/abs/2303.08774</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ruben Villegas, Mohammad Babaeizadeh, Pieter-J...</td>\n",
              "      <td>2022-10-05</td>\n",
              "      <td>Phenaki: Variable Length Video Generation From...</td>\n",
              "      <td>https://arxiv.org/abs/2210.02399</td>\n",
              "      <td>128</td>\n",
              "      <td>Titan V</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aitor Lewkowycz, Anders Andreassen, David Doha...</td>\n",
              "      <td>2022-06-29</td>\n",
              "      <td>Solving Quantitative Reasoning Problems with L...</td>\n",
              "      <td>https://arxiv.org/abs/2206.14858</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aakanksha Chowdhery, Sharan Narang, Jacob Devl...</td>\n",
              "      <td>2022-04-04</td>\n",
              "      <td>PaLM: Scaling Language Modeling with Pathways</td>\n",
              "      <td>https://arxiv.org/abs/2204.02311</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
              "      <td>2022-03-29</td>\n",
              "      <td>Training Compute-Optimal Large Language Models</td>\n",
              "      <td>https://arxiv.org/abs/2203.15556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...</td>\n",
              "      <td>2022-06-22</td>\n",
              "      <td>Scaling Autoregressive Models for Content-Rich...</td>\n",
              "      <td>https://arxiv.org/abs/2206.10789v1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Romal Thoppilan, Daniel De Freitas, Jamie Hall...</td>\n",
              "      <td>2022-02-10</td>\n",
              "      <td>LaMDA: Language Models for Dialog Applications</td>\n",
              "      <td>https://arxiv.org/abs/2201.08239</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Saleh Soltan, Shankar Ananthakrishnan, Jack Fi...</td>\n",
              "      <td>2022-08-02</td>\n",
              "      <td>AlexaTM 20B: Few-Shot Learning Using a Large-S...</td>\n",
              "      <td>https://arxiv.org/abs/2208.01448</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Robin Rombach, Andreas Blattmann, Dominik Lore...</td>\n",
              "      <td>2022-04-13</td>\n",
              "      <td>High-Resolution Image Synthesis with Latent Di...</td>\n",
              "      <td>https://arxiv.org/abs/2112.10752</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Alec Radford, Jong Wook Kim, Tao Xu, Greg Broc...</td>\n",
              "      <td>2022-09-21</td>\n",
              "      <td>Robust Speech Recognition via Large-Scale Weak...</td>\n",
              "      <td>https://cdn.openai.com/papers/whisper.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Author(s) Publication date  \\\n",
              "0                                              OpenAI       2023-03-15   \n",
              "1   Ruben Villegas, Mohammad Babaeizadeh, Pieter-J...       2022-10-05   \n",
              "2   Aitor Lewkowycz, Anders Andreassen, David Doha...       2022-06-29   \n",
              "3   Aakanksha Chowdhery, Sharan Narang, Jacob Devl...       2022-04-04   \n",
              "5   Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...       2022-03-29   \n",
              "6   Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...       2022-06-22   \n",
              "7   Romal Thoppilan, Daniel De Freitas, Jamie Hall...       2022-02-10   \n",
              "9   Saleh Soltan, Shankar Ananthakrishnan, Jack Fi...       2022-08-02   \n",
              "12  Robin Rombach, Andreas Blattmann, Dominik Lore...       2022-04-13   \n",
              "13  Alec Radford, Jong Wook Kim, Tao Xu, Greg Broc...       2022-09-21   \n",
              "\n",
              "                                            Reference  \\\n",
              "0                              GPT-4 Technical Report   \n",
              "1   Phenaki: Variable Length Video Generation From...   \n",
              "2   Solving Quantitative Reasoning Problems with L...   \n",
              "3       PaLM: Scaling Language Modeling with Pathways   \n",
              "5      Training Compute-Optimal Large Language Models   \n",
              "6   Scaling Autoregressive Models for Content-Rich...   \n",
              "7      LaMDA: Language Models for Dialog Applications   \n",
              "9   AlexaTM 20B: Few-Shot Learning Using a Large-S...   \n",
              "12  High-Resolution Image Synthesis with Latent Di...   \n",
              "13  Robust Speech Recognition via Large-Scale Weak...   \n",
              "\n",
              "                                         Link Number of hardware units  \\\n",
              "0            https://arxiv.org/abs/2303.08774                      NaN   \n",
              "1            https://arxiv.org/abs/2210.02399                      128   \n",
              "2            https://arxiv.org/abs/2206.14858                      NaN   \n",
              "3            https://arxiv.org/abs/2204.02311                      NaN   \n",
              "5            https://arxiv.org/abs/2203.15556                      NaN   \n",
              "6          https://arxiv.org/abs/2206.10789v1                      NaN   \n",
              "7            https://arxiv.org/abs/2201.08239                      NaN   \n",
              "9            https://arxiv.org/abs/2208.01448                      NaN   \n",
              "12           https://arxiv.org/abs/2112.10752                      NaN   \n",
              "13  https://cdn.openai.com/papers/whisper.pdf                      NaN   \n",
              "\n",
              "   Hardware model  \n",
              "0             NaN  \n",
              "1         Titan V  \n",
              "2             NaN  \n",
              "3             NaN  \n",
              "5             NaN  \n",
              "6             NaN  \n",
              "7             NaN  \n",
              "9             NaN  \n",
              "12            NaN  \n",
              "13            NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download dataset from the Parameters, Compute and Data Trends in ML sheet\n",
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/export?format=csv#gid=0')\n",
        "\n",
        "year_start = 2017\n",
        "\n",
        "# Recode columns\n",
        "df['Publication date'] = pd.to_datetime(df['Publication date'], errors='coerce', dayfirst=True)\n",
        "\n",
        "# Filter for papers of only the last 5 years\n",
        "df = df[df['Publication date'] > f'{year_start}-01-01']\n",
        "\n",
        "# Keep only bibliographical data\n",
        "df = df.filter(['Author(s)', 'Publication date', 'Reference', 'Link'])\n",
        "df = df[df['Link'].notna()]\n",
        "# Keep only links which forward to a pdf or an arxiv link\n",
        "df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n",
        "\n",
        "keys = ['Number of hardware units', 'Hardware model', 'Training FLOP/s']\n",
        "\n",
        "# Enable for test running with the first ten papers\n",
        "df = df[:10]\n",
        "# Or a specific paper\n",
        "# idx = 4\n",
        "# df = df[idx:idx+1]\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    try:\n",
        "        parse_paper(df, i, row, keys)\n",
        "        print(\"---\")\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "display(df)\n",
        "\n",
        "timestamp = datetime.datetime.now()\n",
        "df.to_csv(f'output_data/parsed_paper_data_{timestamp.strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b7c6d073edd95e87cfa6524fac94a63a54fb86c88ec13e975ab85409bf024d2f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
