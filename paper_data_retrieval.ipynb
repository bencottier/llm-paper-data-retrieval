{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEPHl7N7YJWn"
      },
      "source": [
        "# TODO\n",
        "If we're already at it, let's also look for keywords, such as:\n",
        "- related to GPU/TPU Hardware\n",
        "- training time\n",
        "- inference compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!conda install -y openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ee3887XnrDa",
        "outputId": "5cde1173-289a-44e4-d6a1-c911b058c4ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.11.0\n",
            "  latest version: 23.3.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /Users/bencottier/miniconda3/envs/nlp\n",
            "\n",
            "  added / updated specs:\n",
            "    - pdfminer.six\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    pdfminer.six-20221105      |     pyhd8ed1ab_0         4.9 MB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         4.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  pdfminer.six       conda-forge/noarch::pdfminer.six-20221105-pyhd8ed1ab_0\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2022.12.7-h033912b_0\n",
            "  certifi            pkgs/main/osx-64::certifi-2022.12.7-p~ --> conda-forge/noarch::certifi-2022.12.7-pyhd8ed1ab_0\n",
            "  openssl              pkgs/main::openssl-1.1.1t-hca72f7f_0 --> conda-forge::openssl-1.1.1t-hfd90126_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "pdfminer.six-2022110 | 4.9 MB    | ##################################### | 100% \n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda install -y -c conda-forge pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from pdfminer.high_level import extract_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('data', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "paper_text = \"PaLM: Scaling Language Modeling with Pathways\\nAakanksha Chowdhery∗ Sharan Narang∗ Jacob Devlin∗\\nMaarten Bosma Gaurav Mishra Adam Roberts Paul Barham\\nHyung Won Chung Charles Sutton Sebastian Gehrmann Parker Schuh Kensen Shi\\nSasha Tsvyashchenko Joshua Maynez Abhishek Rao† Parker Barnes Yi Tay\\nNoam Shazeer‡ Vinodkumar Prabhakaran Emily Reif Nan Du Ben Hutchinson\\nReiner Pope James Bradbury Jacob Austin Michael Isard Guy Gur-Ari\\nPengcheng Yin Toju Duke Anselm Levskaya Sanjay Ghemawat Sunipa Dev\\nHenryk Michalewski Xavier Garcia Vedant Misra Kevin Robinson Liam Fedus\\nDenny Zhou Daphne Ippolito David Luan‡ Hyeontaek Lim Barret Zoph\\nAlexander Spiridonov Ryan Sepassi David Dohan Shivani Agrawal Mark Omernick\\nAndrew M. Dai Thanumalayan Sankaranarayana Pillai Marie Pellat Aitor Lewkowycz\\nErica Moreira Rewon Child Oleksandr Polozov† Katherine Lee Zongwei Zhou\\nXuezhi Wang Brennan Saeta Mark Diaz Orhan Firat Michele Catasta† Jason Wei\\nKathy Meier-Hellstern Douglas Eck Jeff Dean Slav Petrov Noah Fiedel\\nGoogle Research\\nAbstract\\nLarge language models have been shown to achieve remarkable performance across a variety of natural\\nlanguage tasks using few-shot learning, which drastically reduces the number of task-specific training\\nexamples needed to adapt the model to a particular application. To further our understanding of the\\nimpact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer\\nlanguage model, which we call Pathways Language Model (PaLM).\\nWe trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient\\ntraining across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-\\nthe-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a\\nnumber of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-\\nof-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the\\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous\\nimprovements from model scale, meaning that performance steeply increased as we scaled to our largest\\nmodel. PaLM also has strong capabilities in multilingual tasks and source code generation, which we\\ndemonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias\\nand toxicity, and study the extent of training data memorization with respect to model scale. Finally,\\nwe discuss the ethical considerations related to large language models and discuss potential mitigation\\nstrategies.\\n∗Equal Contribution. Author contributions and ordering details are listed in Appendix A.\\nCorrespondence authors: chowdhery@google.com, sharannarang@google.com\\nIn addition to other contributions, the last five authors advised the overall project.\\n†Alphabet, X, the Moonshot Factory\\n‡Work done while at Google\\n\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject text_completion id=cmpl-79yJyqaIneUGQe2PMqgAb52dLm6ET at 0x7fe8f08cd4a0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"logprobs\": null,\n",
              "      \"text\": \"| ---------------------- | -------------------------- | ------ |\\n| 6144                   | TPU v4                    | N/A    |\"\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1682611182,\n",
              "  \"id\": \"cmpl-79yJyqaIneUGQe2PMqgAb52dLm6ET\",\n",
              "  \"model\": \"text-davinci-003\",\n",
              "  \"object\": \"text_completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 27,\n",
              "    \"prompt_tokens\": 753,\n",
              "    \"total_tokens\": 780\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=f\"A table summarizing the training hardware from this paper:\\n\\n====\\n\\n{paper_text}\\n\\n====\\n\\n| Number of GPUs or TPUs | Hardware model (e.g. A100) | FLOP/s |\\n\",\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidRequestError",
          "evalue": "'messages' is a required property",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[103], line 29\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_text \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mRead the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mN/A\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m1. How many GPUs or TPUs were used to train the model? Just state the number. If the number of GPUs or TPUs is not mentioned in the text, write \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mN/A\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(  \u001b[39m# Completion\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# text-davinci-003\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt_text,\n\u001b[1;32m     32\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     34\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m     frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     36\u001b[0m     presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[39mprint\u001b[39m(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/openai/api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    681\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: 'messages' is a required property"
          ]
        }
      ],
      "source": [
        "prompt_text = f\"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "1. How many GPUs or TPUs were used to train the model? Just state the number. If the number of GPUs or TPUs is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "3. What FLOP/s (AKA: FLOP/second, FLOPS) was achieved during training? Include the same units as written in the paper. If FLOP/s is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "3. 21 TFLOP/s\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "3. 21 petaflops\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "3. 127e12 FLOPS\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt_text,\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Read the Machine Learning research paper below and answer the following questions. Just state the answer without explanation. If the answer is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "1. How many GPUs or TPUs or chips were used to train the model? Just state the number. If the number of GPUs or TPUs or chips is not mentioned in the text, write \"N/A\".\n",
        "2. What model of GPU or TPU was used to train the model? Examples include: \"A100\", \"V100\", \"P100\", \"TPUv3\", \"TPUv4\". If the GPU or TPU is not mentioned in the text, write \"N/A\".\n",
        "3. What FLOP/s (AKA: FLOP/second, FLOPS) was achieved during training? Include the same units as written in the paper. If FLOP/s is not mentioned in the text, write \"N/A\".\n",
        "\n",
        "Here are some example answers:\n",
        "\n",
        "1. 1\n",
        "2. V100\n",
        "3. 21 TFLOP/s\n",
        "\n",
        "1. N/A\n",
        "2. Titan V\n",
        "3. 21 petaflops\n",
        "\n",
        "1. 32\n",
        "2. N/A\n",
        "3. 127e12 FLOPS\n",
        "\n",
        "====\n",
        "\n",
        "{paper_text}\n",
        "\n",
        "====\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def parse_text_gpt(text):\n",
        "    prompt_text = prompt_template.format(paper_text=text)\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=prompt_text,\n",
        "        temperature=0,\n",
        "        max_tokens=100,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    return response\n",
        "\n",
        "def parse_gpt_response(response):\n",
        "    # E.g. \"1. 6144 TPUs\\n2. TPU v4\\n3. N/A\\n\"\n",
        "    answers = response[\"choices\"][0][\"text\"].strip().split(\"\\n\")\n",
        "    answers = [a.split(\".\")[-1].strip() for a in answers]\n",
        "    return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I've heard that English has about 4 chars per token on average.\n",
        "# `text-davinci-003` token limit (including output) is 4097.\n",
        "# So 4097 * 3 should be pretty safe.\n",
        "CHAR_LIMIT = 4097*3\n",
        "\n",
        "def parse_paper(df, i, row, keys):\n",
        "    url = row['Link']\n",
        "\n",
        "    # replace \"abs\" with \"pdf\" in arxiv url links\n",
        "    url = url.replace('abs', 'pdf')\n",
        "    print(f\"Looking into \\\"{row['Reference']}\\\"\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "    except Exception as e:\n",
        "        print(f\"There's something wrong with downloading: {e}\")\n",
        "        raise e\n",
        "\n",
        "    file = open(\"download.pdf\", \"wb\")\n",
        "    file.seek(0) # overwrite previous file\n",
        "    file.write(response.content)\n",
        "    file.close()\n",
        "\n",
        "    try:\n",
        "        text = extract_text('download.pdf')\n",
        "\n",
        "        answers = parse_gpt_response(parse_text_gpt(text[:CHAR_LIMIT]))\n",
        "\n",
        "        for key, answer in zip(keys, answers):\n",
        "            df.loc[i,key]  = answer if answer else \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"There's something wrong with extracting the text: {e}\")\n",
        "        raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkwpFPzmWyG6",
        "outputId": "057026f6-0b56-49cd-a84e-2ba4773b4eae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/g6/w_cfz8c507j6mbt5mdv7wt_h0000gn/T/ipykernel_33098/28115734.py:16: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
            "  df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking into \"GPT-4 Technical Report\"\n",
            "---\n",
            "Looking into \"Phenaki: Variable Length Video Generation From Open Domain Textual Description\"\n",
            "---\n",
            "Looking into \"Solving Quantitative Reasoning Problems with Language Models\"\n",
            "---\n",
            "Looking into \"PaLM: Scaling Language Modeling with Pathways\"\n",
            "---\n",
            "Looking into \"Training Compute-Optimal Large Language Models\"\n",
            "---\n",
            "Looking into \"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\"\n",
            "---\n",
            "Looking into \"LaMDA: Language Models for Dialog Applications\"\n",
            "---\n",
            "Looking into \"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\"\n",
            "---\n",
            "Looking into \"High-Resolution Image Synthesis with Latent Diffusion Models\"\n",
            "---\n",
            "Looking into \"Robust Speech Recognition via Large-Scale Weak Supervision\"\n",
            "---\n",
            "Looking into \"Primer: Searching for Efficient Transformers for Language Modeling\"\n",
            "---\n",
            "Looking into \"Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish\"\n",
            "---\n",
            "Looking into \"Efficient Language Modeling with Sparse all-MLP\"\n",
            "---\n",
            "Looking into \"Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts\"\n",
            "---\n",
            "Looking into \"Training language models to follow instructions with human feedback\"\n",
            "---\n",
            "Looking into \"Improving language models by retrieving from trillions of tokens\"\n",
            "---\n",
            "Looking into \"MuZero with Self-competition for Rate Control in VP9 Video Compression\"\n",
            "---\n",
            "Looking into \"DeepNet: Scaling Transformers to 1,000 Layers\"\n",
            "---\n",
            "Looking into \"Formal Mathematics Statement Curriculum Learning\"\n",
            "---\n",
            "Looking into \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"\n",
            "---\n",
            "Looking into \"Flamingo: a Visual Language Model for Few-Shot Learning\"\n",
            "---\n",
            "Looking into \"Unifying Language Learning Paradigms\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4320 tokens (4220 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"Language Models are General-Purpose Interfaces\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4448 tokens (4348 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"Make-A-Video: Text-to-Video Generation without Text-Video Data\"\n",
            "---\n",
            "Looking into \"Galactica: A Large Language Model for Science\"\n",
            "---\n",
            "Looking into \"Imagen Video: High Definition Video Generation with Diffusion Models\"\n",
            "---\n",
            "Looking into \"Efficient Large-Scale Language Model Training on GPU Clusters\"\n",
            "---\n",
            "Looking into \"Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\"\n",
            "---\n",
            "Looking into \"Jurassic-1: Technical Details and Evaluation\"\n",
            "---\n",
            "Looking into \"Scaling up visual and vision-language representation learning with noisy text supervision\"\n",
            "---\n",
            "Looking into \"Meta pseudo labels\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4226 tokens (4126 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\"\n",
            "---\n",
            "Looking into \"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation\"\n",
            "---\n",
            "Looking into \"FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS\"\n",
            "---\n",
            "Looking into \"CogView: Mastering Text-to-Image Generation via Transformers\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4166 tokens (4066 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"nan\"\n",
            "---\n",
            "Looking into \"Learning Transferable Visual Models From Natural Language Supervision\"\n",
            "---\n",
            "Looking into \"DeBERTa: Decoding-enhanced BERT with Disentangled Attention\"\n",
            "---\n",
            "Looking into \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\"\n",
            "---\n",
            "Looking into \"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining\"\n",
            "---\n",
            "Looking into \"NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4184 tokens (4084 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"Self-supervised Pretraining of Visual Features in the Wild\"\n",
            "---\n",
            "Looking into \"Scaling Vision Transformers\"\n",
            "---\n",
            "Looking into \"Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models\"\n",
            "---\n",
            "Looking into \"nan\"\n",
            "---\n",
            "Looking into \"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding\"\n",
            "---\n",
            "Looking into \"Denoising Diffusion Probabilistic Models\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 5830 tokens (5730 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\"\n",
            "---\n",
            "Looking into \"Learning Transferable Visual Models From Natural Language Supervision\"\n",
            "---\n",
            "Looking into \"BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition\"\n",
            "---\n",
            "Looking into \"M6: A Chinese Multimodal Pretrainer\"\n",
            "---\n",
            "Looking into \"M6: A Chinese Multimodal Pretrainer\"\n",
            "---\n",
            "Looking into \"Recipes for building an open-domain chatbot\"\n",
            "---\n",
            "Looking into \"M6-T: Exploring Sparse Expert Models and Beyond\"\n",
            "---\n",
            "Looking into \"Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models\"\n",
            "---\n",
            "Looking into \"Larger-Scale Transformers for Multilingual Masked Language Modeling\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4966 tokens (4866 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"Finetuned Language Models Are Zero-Shot Learners\"\n",
            "---\n",
            "Looking into \"Multitask Prompted Training Enables Zero-Shot Task Generalization\"\n",
            "---\n",
            "Looking into \"CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4465 tokens (4365 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"Mastering Atari Games with Limited Data\"\n",
            "---\n",
            "Looking into \"Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4161 tokens (4061 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"Player of Games\"\n",
            "---\n",
            "Looking into \"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4161 tokens (4061 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"Few-shot Learning with Multilingual Language Models\"\n",
            "---\n",
            "Looking into \"ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation\"\n",
            "---\n",
            "Looking into \"Language models are Few- Shot Learners\"\n",
            "---\n",
            "Looking into \"Towards a Human-like Open-Domain Chatbot\"\n",
            "---\n",
            "Looking into \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\"\n",
            "---\n",
            "Looking into \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\"\n",
            "---\n",
            "Looking into \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\"\n",
            "---\n",
            "Looking into \"AraGPT2: Pre-Trained Transformer for Arabic Language Generation\"\n",
            "---\n",
            "Looking into \"CPM: A Large-scale Generative Chinese Pre-trained Language Model\"\n",
            "---\n",
            "Looking into \"Once for all: Train one network and specialize it for efficient deployment.\"\n",
            "---\n",
            "Looking into \"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 5551 tokens (5451 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"High- performance, Distributed Training of Large scale Deep Learning Recommendation Models\"\n",
            "There's something wrong with extracting the text: No /Root object! - Is this really a PDF?\n",
            "Looking into \"ELECTRA: pre-training text encoders as discriminators rather than generators\"\n",
            "---\n",
            "Looking into \"KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.\"\n",
            "---\n",
            "Looking into \"BERT-of-Theseus: Compressing BERT by Progressive Module Replacing\"\n",
            "---\n",
            "Looking into \"Perceiver IO: A General Architecture for Structured Inputs & Outputs\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4586 tokens (4486 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"A Simple Framework for Contrastive Learning of Visual Representations\"\n",
            "---\n",
            "Looking into \"Electra: pre-training text encoders as discriminators rather than generators\"\n",
            "---\n",
            "Looking into \"MetNet: A Neural Weather Model for Precipitation Forecasting\"\n",
            "---\n",
            "Looking into \"Agent57: Outperforming the Atari Human Benchmark\"\n",
            "---\n",
            "Looking into \" Random Erasing Data Augmentation \"\n",
            "---\n",
            "Looking into \"MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\"\n",
            "There's something wrong with extracting the text: This model's maximum context length is 4097 tokens, however you requested 4767 tokens (4667 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.\n",
            "Looking into \"CURL: Contrastive Unsupervised Representations for Reinforcement Learning\"\n",
            "---\n",
            "Looking into \"First return, then explore\"\n",
            "---\n",
            "Looking into \"SqueezeBERT: What can computer vision teach NLP about efficient neural networks?\"\n",
            "---\n",
            "Looking into \"Hopfield Networks is All You Need\"\n",
            "---\n",
            "Looking into \"ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation\"\n",
            "There's something wrong with extracting the text: Unexpected EOF\n",
            "Looking into \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Big self- supervised models are strong semi-supervised learners.\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Dota 2 with Large Scale Deep Reinforcement Learning\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\"\n",
            "There's something wrong with extracting the text: Unexpected EOF\n",
            "Looking into \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"\n",
            "There's something wrong with extracting the text: Unexpected EOF\n",
            "Looking into \"Dota 2 with Large Scale Deep Reinforcement Learning\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"MnasNet: Platform-Aware Neural Architecture Search for Mobile\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"MnasNet: Platform-Aware Neural Architecture Search for Mobile\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Solving Rubik’s Cube with a Robot Hand\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Mastering Atari Go Chess and Shogi by Planning with a Learned Model\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"ProxylessNAS: Direct neural architecture search on target task and hardware\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search\"\n",
            "Looking into \"Deep Learning Recommendation Model for Personalization and Recommendation Systems\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Decoupled weight decay regularization.\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"The Hanabi Challenge: A New Frontier for AI Research\"\n",
            "Looking into \"Superhuman AI for multiplayer poker\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"Multi-Task Deep Neural Networks for Natural Language Understanding\"\n",
            "There's something wrong with extracting the text: You exceeded your current quota, please check your plan and billing details.\n",
            "Looking into \"SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\"\n"
          ]
        }
      ],
      "source": [
        "# Download dataset from the Parameters, Compute and Data Trends in ML sheet\n",
        "df = pd.read_csv('https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/export?format=csv#gid=0')\n",
        "\n",
        "year_start = 2017\n",
        "\n",
        "# Recode columns\n",
        "df['Publication date'] = pd.to_datetime(df['Publication date'], errors='coerce', dayfirst=True)\n",
        "\n",
        "# Filter for papers of only the last 5 years\n",
        "df = df[df['Publication date'] > f'{year_start}-01-01']\n",
        "\n",
        "# Keep only bibliographical data\n",
        "df = df.filter(['Author(s)', 'Publication date', 'Reference', 'Link'])\n",
        "df = df[df['Link'].notna()]\n",
        "# Keep only links which forward to a pdf or an arxiv link\n",
        "df = df[df['Link'].str.contains('(arxiv|.pdf$)', regex=True)]\n",
        "\n",
        "keys = ['Number of hardware units', 'Hardware model', 'Training FLOP/s']\n",
        "\n",
        "# Enable for test running with the first ten papers\n",
        "# df = df[:10]\n",
        "# Or a specific paper\n",
        "# idx = 3\n",
        "# df = df[idx:idx+1]\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    try:\n",
        "        parse_paper(df, i, row, keys)\n",
        "        print(\"---\")\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "display(df)\n",
        "\n",
        "timestamp = datetime.datetime.now()\n",
        "# df.to_csv(f'data/parsed_paper_data_{timestamp.strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b7c6d073edd95e87cfa6524fac94a63a54fb86c88ec13e975ab85409bf024d2f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
