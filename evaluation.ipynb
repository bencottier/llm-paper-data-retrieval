{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Author(s)</th>\n",
       "      <th>Publication date</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Link</th>\n",
       "      <th>Number of hardware units</th>\n",
       "      <th>Hardware model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Aitor Lewkowycz, Anders Andreassen, David Doha...</td>\n",
       "      <td>2022-06-29</td>\n",
       "      <td>Solving Quantitative Reasoning Problems with L...</td>\n",
       "      <td>https://arxiv.org/abs/2206.14858</td>\n",
       "      <td>The 8B model was trained on a v4-128, the 62B ...</td>\n",
       "      <td>v4 TPU (\"We used the t5x framework (Roberts et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Aakanksha Chowdhery, Sharan Narang, Jacob Devl...</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>PaLM: Scaling Language Modeling with Pathways</td>\n",
       "      <td>https://arxiv.org/abs/2204.02311</td>\n",
       "      <td>6144 (\"We trained PaLM on 6144 TPU v4 chips\")</td>\n",
       "      <td>TPU v4 (\"We trained PaLM on 6144 TPU v4 chips\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
       "      <td>2022-03-29</td>\n",
       "      <td>Training Compute-Optimal Large Language Models</td>\n",
       "      <td>https://arxiv.org/abs/2203.15556</td>\n",
       "      <td>N/A</td>\n",
       "      <td>TPUv3/TPUv4 (\"All models in this analysis have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...</td>\n",
       "      <td>2022-06-22</td>\n",
       "      <td>Scaling Autoregressive Models for Content-Rich...</td>\n",
       "      <td>https://arxiv.org/abs/2206.10789v1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>CloudTPUv4 (\"We implement our models in Lingvo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Romal Thoppilan, Daniel De Freitas, Jamie Hall...</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>LaMDA: Language Models for Dialog Applications</td>\n",
       "      <td>https://arxiv.org/abs/2201.08239</td>\n",
       "      <td>1024 (\"We pre-trained LaMDA on 1024 TPU-v3 chi...</td>\n",
       "      <td>TPU-v3 (\"We pre-trained LaMDA on 1024 TPU-v3 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...</td>\n",
       "      <td>2022-04-14</td>\n",
       "      <td>Efficient Language Modeling with Sparse all-MLP</td>\n",
       "      <td>https://arxiv.org/abs/2203.06850</td>\n",
       "      <td>32 (\"Num of GPUs: 32\")</td>\n",
       "      <td>Nvidia 32G V100 (\"All the models are trained o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...</td>\n",
       "      <td>2020-05-28</td>\n",
       "      <td>Language models are Few- Shot Learners</td>\n",
       "      <td>https://arxiv.org/abs/2005.14165</td>\n",
       "      <td>N/A</td>\n",
       "      <td>V100 (\"All models were trained on V100 GPU’s o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>103</td>\n",
       "      <td>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...</td>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>GShard: Scaling Giant Models with Conditional ...</td>\n",
       "      <td>https://arxiv.org/abs/2006.16668</td>\n",
       "      <td>2048 (\"We demonstrate that such a giant model ...</td>\n",
       "      <td>TPU v3 (\"We demonstrate that such a giant mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>108</td>\n",
       "      <td>Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...</td>\n",
       "      <td>2020-02-09</td>\n",
       "      <td>ALBERT: A Lite BERT for Self-supervised Learni...</td>\n",
       "      <td>https://arxiv.org/abs/1909.11942</td>\n",
       "      <td>The number of TPUs used for training ranged fr...</td>\n",
       "      <td>Cloud TPU V3 (\"Training was done on Cloud TPU ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>111</td>\n",
       "      <td>Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...</td>\n",
       "      <td>2020-04-29</td>\n",
       "      <td>Once for all: Train one network and specialize...</td>\n",
       "      <td>https://arxiv.org/abs/1908.09791</td>\n",
       "      <td>32 GPUs (\"The full network is trained for 180 ...</td>\n",
       "      <td>V100 (\"The whole training process takes around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>132</td>\n",
       "      <td>Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>An Image is Worth 16x16 Words: Transformers fo...</td>\n",
       "      <td>https://arxiv.org/abs/2010.11929</td>\n",
       "      <td>8 cores (\"it could be trained using a standard...</td>\n",
       "      <td>TPUv3 (\"it could be trained using a standard c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>185</td>\n",
       "      <td>Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>IMPALA: Scalable Distributed Deep-RL with Impo...</td>\n",
       "      <td>https://arxiv.org/abs/1802.01561</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Nvidia P100 (\"1 Nvidia P100\")</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>208</td>\n",
       "      <td>D Silver, T Hubert, J Schrittwieser, I Antonoglou</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>Mastering Chess and Shogi by Self-Play with a ...</td>\n",
       "      <td>https://arxiv.org/abs/1712.01815</td>\n",
       "      <td>5,000 first-generation TPUs were used to gener...</td>\n",
       "      <td>First-generation TPUs and second-generation TP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>211</td>\n",
       "      <td>ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>Revisiting Unreasonable Effectiveness of Data ...</td>\n",
       "      <td>https://arxiv.org/abs/1707.02968</td>\n",
       "      <td>50 GPUs (\"We use asynchronous gradient descent...</td>\n",
       "      <td>NVIDIA K80 (\"We use asynchronous gradient desc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>212</td>\n",
       "      <td>N Shazeer, A Mirhoseini, K Maziarz, A Davis</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>Outrageously Large Neural Networks: The Sparse...</td>\n",
       "      <td>https://arxiv.org/abs/1701.06538</td>\n",
       "      <td>16-32 (\"We trained our models using TensorFlow...</td>\n",
       "      <td>Tesla K40 (\"We trained our models using Tensor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>213</td>\n",
       "      <td>C Liu, B Zoph, M Neumann, J Shlens</td>\n",
       "      <td>2017-12-02</td>\n",
       "      <td>Progressive Neural Architecture Search</td>\n",
       "      <td>https://arxiv.org/abs/1712.00559</td>\n",
       "      <td>N/A</td>\n",
       "      <td>P100 (\"In the Mobile setting, we use distribut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>214</td>\n",
       "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...</td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>https://proceedings.neurips.cc/paper/2017/file...</td>\n",
       "      <td>8 (\"We trained our models on one machine with ...</td>\n",
       "      <td>NVIDIA P100 (\"We trained our models on one mac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>215</td>\n",
       "      <td>Matej Moravčík, Martin Schmid, Neil Burch, Vil...</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>DeepStack: Expert-Level Artificial Intelligenc...</td>\n",
       "      <td>https://arxiv.org/abs/1701.01724</td>\n",
       "      <td>1 (\"DeepStack uses an estimated opponent range...</td>\n",
       "      <td>NVIDIA GeForce GTX 1080 (\"With sparse and dept...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                          Author(s)  \\\n",
       "0            3  Aitor Lewkowycz, Anders Andreassen, David Doha...   \n",
       "1            4  Aakanksha Chowdhery, Sharan Narang, Jacob Devl...   \n",
       "2            6  Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...   \n",
       "3            7  Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu...   \n",
       "4            8  Romal Thoppilan, Daniel De Freitas, Jamie Hall...   \n",
       "5           21  Ping Yu, Mikel Artexte, Myle Ott, Sam  Shleife...   \n",
       "6          100  Tom B. Brown, Benjamin Mann, Nick Ryder, Melan...   \n",
       "7          103  Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,...   \n",
       "8          108  Zhenzhong Lan, Mingda Chen, Sebastian Goodman,...   \n",
       "9          111  Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhan...   \n",
       "10         132  Alexey Dosovitskiy, Lucas Beyer, Alexander Kol...   \n",
       "11         185  Lasse Espeholt, Hubert Soyer, Remi Munos, Kare...   \n",
       "12         208  D Silver, T Hubert, J Schrittwieser, I Antonoglou   \n",
       "13         211  ChenSun,AbhinavShrivastava,SaurabhSingh,andAbh...   \n",
       "14         212        N Shazeer, A Mirhoseini, K Maziarz, A Davis   \n",
       "15         213                 C Liu, B Zoph, M Neumann, J Shlens   \n",
       "16         214  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...   \n",
       "17         215  Matej Moravčík, Martin Schmid, Neil Burch, Vil...   \n",
       "\n",
       "   Publication date                                          Reference  \\\n",
       "0        2022-06-29  Solving Quantitative Reasoning Problems with L...   \n",
       "1        2022-04-04      PaLM: Scaling Language Modeling with Pathways   \n",
       "2        2022-03-29     Training Compute-Optimal Large Language Models   \n",
       "3        2022-06-22  Scaling Autoregressive Models for Content-Rich...   \n",
       "4        2022-02-10     LaMDA: Language Models for Dialog Applications   \n",
       "5        2022-04-14    Efficient Language Modeling with Sparse all-MLP   \n",
       "6        2020-05-28             Language models are Few- Shot Learners   \n",
       "7        2020-06-30  GShard: Scaling Giant Models with Conditional ...   \n",
       "8        2020-02-09  ALBERT: A Lite BERT for Self-supervised Learni...   \n",
       "9        2020-04-29  Once for all: Train one network and specialize...   \n",
       "10       2020-10-22  An Image is Worth 16x16 Words: Transformers fo...   \n",
       "11       2018-02-05  IMPALA: Scalable Distributed Deep-RL with Impo...   \n",
       "12       2017-12-05  Mastering Chess and Shogi by Self-Play with a ...   \n",
       "13       2017-08-04  Revisiting Unreasonable Effectiveness of Data ...   \n",
       "14       2017-01-23  Outrageously Large Neural Networks: The Sparse...   \n",
       "15       2017-12-02             Progressive Neural Architecture Search   \n",
       "16       2017-06-12                          Attention Is All You Need   \n",
       "17       2017-01-06  DeepStack: Expert-Level Artificial Intelligenc...   \n",
       "\n",
       "                                                 Link  \\\n",
       "0                    https://arxiv.org/abs/2206.14858   \n",
       "1                    https://arxiv.org/abs/2204.02311   \n",
       "2                    https://arxiv.org/abs/2203.15556   \n",
       "3                  https://arxiv.org/abs/2206.10789v1   \n",
       "4                    https://arxiv.org/abs/2201.08239   \n",
       "5                    https://arxiv.org/abs/2203.06850   \n",
       "6                    https://arxiv.org/abs/2005.14165   \n",
       "7                    https://arxiv.org/abs/2006.16668   \n",
       "8                    https://arxiv.org/abs/1909.11942   \n",
       "9                    https://arxiv.org/abs/1908.09791   \n",
       "10                   https://arxiv.org/abs/2010.11929   \n",
       "11                   https://arxiv.org/abs/1802.01561   \n",
       "12                   https://arxiv.org/abs/1712.01815   \n",
       "13                   https://arxiv.org/abs/1707.02968   \n",
       "14                   https://arxiv.org/abs/1701.06538   \n",
       "15                   https://arxiv.org/abs/1712.00559   \n",
       "16  https://proceedings.neurips.cc/paper/2017/file...   \n",
       "17                   https://arxiv.org/abs/1701.01724   \n",
       "\n",
       "                             Number of hardware units  \\\n",
       "0   The 8B model was trained on a v4-128, the 62B ...   \n",
       "1       6144 (\"We trained PaLM on 6144 TPU v4 chips\")   \n",
       "2                                                 N/A   \n",
       "3                                                 N/A   \n",
       "4   1024 (\"We pre-trained LaMDA on 1024 TPU-v3 chi...   \n",
       "5                              32 (\"Num of GPUs: 32\")   \n",
       "6                                                 N/A   \n",
       "7   2048 (\"We demonstrate that such a giant model ...   \n",
       "8   The number of TPUs used for training ranged fr...   \n",
       "9   32 GPUs (\"The full network is trained for 180 ...   \n",
       "10  8 cores (\"it could be trained using a standard...   \n",
       "11                                                N/A   \n",
       "12  5,000 first-generation TPUs were used to gener...   \n",
       "13  50 GPUs (\"We use asynchronous gradient descent...   \n",
       "14  16-32 (\"We trained our models using TensorFlow...   \n",
       "15                                                N/A   \n",
       "16  8 (\"We trained our models on one machine with ...   \n",
       "17  1 (\"DeepStack uses an estimated opponent range...   \n",
       "\n",
       "                                       Hardware model  \n",
       "0   v4 TPU (\"We used the t5x framework (Roberts et...  \n",
       "1     TPU v4 (\"We trained PaLM on 6144 TPU v4 chips\")  \n",
       "2   TPUv3/TPUv4 (\"All models in this analysis have...  \n",
       "3   CloudTPUv4 (\"We implement our models in Lingvo...  \n",
       "4   TPU-v3 (\"We pre-trained LaMDA on 1024 TPU-v3 c...  \n",
       "5   Nvidia 32G V100 (\"All the models are trained o...  \n",
       "6   V100 (\"All models were trained on V100 GPU’s o...  \n",
       "7   TPU v3 (\"We demonstrate that such a giant mode...  \n",
       "8   Cloud TPU V3 (\"Training was done on Cloud TPU ...  \n",
       "9   V100 (\"The whole training process takes around...  \n",
       "10  TPUv3 (\"it could be trained using a standard c...  \n",
       "11                      Nvidia P100 (\"1 Nvidia P100\")  \n",
       "12  First-generation TPUs and second-generation TP...  \n",
       "13  NVIDIA K80 (\"We use asynchronous gradient desc...  \n",
       "14  Tesla K40 (\"We trained our models using Tensor...  \n",
       "15  P100 (\"In the Mobile setting, we use distribut...  \n",
       "16  NVIDIA P100 (\"We trained our models on one mac...  \n",
       "17  NVIDIA GeForce GTX 1080 (\"With sparse and dept...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Latest for GPT-3.5: 'output_data/parsed_paper_data_2023-05-17_15-50-57.csv'\n",
    "# Latest for GPT-4: 'output_data/parsed_paper_data_2023-05-17_16-24-14.csv'\n",
    "df = pd.read_csv('output_data/parsed_paper_data_2023-05-17_16-24-14.csv', keep_default_na=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     v4 TPU (\"We used the t5x framework (Roberts et...\n",
       "1       TPU v4 (\"We trained PaLM on 6144 TPU v4 chips\")\n",
       "2     TPUv3/TPUv4 (\"All models in this analysis have...\n",
       "3     CloudTPUv4 (\"We implement our models in Lingvo...\n",
       "4     TPU-v3 (\"We pre-trained LaMDA on 1024 TPU-v3 c...\n",
       "5     Nvidia 32G V100 (\"All the models are trained o...\n",
       "6     V100 (\"All models were trained on V100 GPU’s o...\n",
       "7     TPU v3 (\"We demonstrate that such a giant mode...\n",
       "8     Cloud TPU V3 (\"Training was done on Cloud TPU ...\n",
       "9     V100 (\"The whole training process takes around...\n",
       "10    TPUv3 (\"it could be trained using a standard c...\n",
       "11                        Nvidia P100 (\"1 Nvidia P100\")\n",
       "12    First-generation TPUs and second-generation TP...\n",
       "13    NVIDIA K80 (\"We use asynchronous gradient desc...\n",
       "14    Tesla K40 (\"We trained our models using Tensor...\n",
       "15    P100 (\"In the Mobile setting, we use distribut...\n",
       "16    NVIDIA P100 (\"We trained our models on one mac...\n",
       "17    NVIDIA GeForce GTX 1080 (\"With sparse and dept...\n",
       "Name: Hardware model, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Hardware model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'Hardware model'\n",
    "answers = df[key].values\n",
    "i = 0\n",
    "answer = answers[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We used the t5x framework (Roberts et al., 2022) and trained our models with v4 TPU on Google Cloud.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the supporting quote from the answer string\n",
    "# The supporting quote should be in parentheses\n",
    "# E.g. 'TPU v4 ( \"With Pathways, we trained a 540B parameter language model on 6144 TPU v4 chips\" )'\n",
    "parsed_answer = answer.split(\"(\", maxsplit=1)\n",
    "# E.g. ['TPU v4 ', ' \"With Pathways, we trained a 540B parameter language model on 6144 TPU v4 chips\" )']\n",
    "parsed_answer = parsed_answer[-1].strip('\"() ')\n",
    "quote = parsed_answer\n",
    "quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_data/solving_quantitative_reasoning_problems_with_language_models.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote in text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_parenthetical(string):\n",
    "    \"\"\"\n",
    "    Extract the parenthetical from a string corresponding to the last occurring closing parenthesis.\n",
    "\n",
    "    Example:\n",
    "        string = \"This is a (sample) string (to extract (substring))\"\n",
    "        extract_last_parenthetical(string) -> \"to extract (substring)\"\n",
    "    \n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    substring = ''\n",
    "    for i, char in enumerate(string[::-1]):\n",
    "        if char == ')':\n",
    "            stack.append(i)\n",
    "        elif char == '(':\n",
    "            idx = stack.pop()\n",
    "            if len(stack) == 0:\n",
    "                substring = string[-i:-(idx+1)]\n",
    "                break     \n",
    "    return substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to extract (substring)'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_last_parenthetical(\"This is a (sample) string (to extract (substring))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote not found in efficient_language_modeling_with_sparse_all-mlp: Num of GPUs: 32\n",
      "Quote not found in gshard_scaling_giant_models_with_conditional_computation_and_automatic_sharding: We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days\n",
      "Quote not found in gshard_scaling_giant_models_with_conditional_computation_and_automatic_sharding: We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days\n",
      "Quote not found in albert_a_lite_bert_for_self-supervised_learning_of_language_representations.: Training was done on Cloud TPU V3\n",
      "Quote not found in an_image_is_worth_16x16_words_transformers_for_image_recognition_at_scale: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days\n",
      "Quote not found in an_image_is_worth_16x16_words_transformers_for_image_recognition_at_scale: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days\n",
      "Quote not found in mastering_chess_and_shogi_by_self-play_with_a_general_reinforcement_learning_algorithm: Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters, using 5,000 first-generation TPUs (15) to generate self-play games and 64 second-generation TPUs to train the neural networks\n",
      "Quote not found in mastering_chess_and_shogi_by_self-play_with_a_general_reinforcement_learning_algorithm: Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters, using 5,000 first-generation TPUs (15) to generate self-play games and 64 second-generation TPUs to train the neural networks\n",
      "Quote not found in outrageously_large_neural_networks_the_sparsely-gated_mixture-of-experts_layer: We trained our models using TensorFlow (Abadi et al., 2016) on clusters containing 16-32 Tesla K40 GPUs\n",
      "Quote not found in outrageously_large_neural_networks_the_sparsely-gated_mixture-of-experts_layer: We trained our models using TensorFlow (Abadi et al., 2016) on clusters containing 16-32 Tesla K40 GPUs\n"
     ]
    }
   ],
   "source": [
    "quote_matches = defaultdict(int)\n",
    "for i, row in df.iterrows():\n",
    "    for key in ['Number of hardware units', 'Hardware model']:\n",
    "        answer = row[key]\n",
    "        if \"N/A\" in answer: continue\n",
    "        # Get the supporting quote from the answer string\n",
    "        # The supporting quote should be in parentheses\n",
    "        # E.g. 'TPU v4 ( \"With Pathways, we trained a 540B parameter language model on 6144 TPU v4 chips\" )'\n",
    "        quote = extract_last_parenthetical(answer)\n",
    "        quote = quote.strip('\"() .')\n",
    "\n",
    "        paper_title = row['Reference'].replace(' ', '_').replace(':', '').replace('\"', '').lower()\n",
    "        with open('input_data/' + paper_title + '.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        if quote.lower() in text.lower().replace(\"\\n\", \" \"):\n",
    "            quote_matches[key] += 1\n",
    "        else:\n",
    "            print(f\"Quote not found in {paper_title}: {quote}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'Number of hardware units': 8, 'Hardware model': 11})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_matches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking whether the flagged cases for GPT-4 actually pass if we allow some reasonable flexibility (but not allowing any hallucination or change of meaning):\n",
    "- [QUESTIONABLE] Quote not found in efficient_language_modeling_with_sparse_all-mlp: Num of GPUs: 32\n",
    "  - It's not a direct quote but it's a correct read from a Table if you look at the PDF.\n",
    "  - The table has \"Num of\", \"GPUs\", and \"32\" in separate positions of the text.\n",
    "- [PASSED] Quote not found in gshard_scaling_giant_models_with_conditional_computation_and_automatic_sharding: We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days\n",
    "  - Actual quote: \"We demonstrate that such a giant model can efﬁcienctly be trained on 2048 TPU v3 accelerators in 4 days\"\n",
    "  - There's a special character, \"ﬁ\"\n",
    "- [PASSED] Quote not found in albert_a_lite_bert_for_self-supervised_learning_of_language_representations.: Training was done on Cloud TPU V3\n",
    "  - Actual quote: \"Train-ing was done on Cloud TPU V3.\"\n",
    "  - Hyphenation across line break.\n",
    "- [PASSED] Quote not found in an_image_is_worth_16x16_words_transformers_for_image_recognition_at_scale: it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days\n",
    "  - Actual quote: \"it could be trained using a standard cloud TPUv3 with 8 cores in ap-proximately 30 days.\"\n",
    "  - Hyphenation across line break.\n",
    "- [PASSED] Quote not found in mastering_chess_and_shogi_by_self-play_with_a_general_reinforcement_learning_algorithm: Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters, using 5,000 first-generation TPUs (15) to generate self-play games and 64 second-generation TPUs to train the neural networks\n",
    "  - Actual quote: \"Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters, using 5,000 ﬁrst-generation TPUs (15) to generate self-play games and 64 second-generation TPUs to train the neural networks.\"\n",
    "  - Looks correct, probably a punctuation mismatch\n",
    "- [PASSED] Quote not found in outrageously_large_neural_networks_the_sparsely-gated_mixture-of-experts_layer: We trained our models using TensorFlow (Abadi et al., 2016) on clusters containing 16-32 Tesla K40 GPUs\n",
    "  - Actual quote: \"We trained our models using TensorFlow (Abadi et al., 2016) on clus-ters containing 16-32 Tesla K40 GPUs.\"\n",
    "  - Hyphenation again"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking whether the flagged cases for GPT-3.5 actually pass if we allow some reasonable flexibility (but not allowing any hallucination or change of meaning):\n",
    "\n",
    "- [PASSED] Quote not found in scaling_autoregressive_models_for_content-rich_text-to-image_generation: We implement our models in Lingvo and scale with GSPMD on CloudTPUv4 hardware for both training and inference\n",
    "  - The quote is there if you remove references like \"[39]\"\n",
    "- [PASSED] Quote not found in gshard_scaling_giant_models_with_conditional_computation_and_automatic_sharding: Figure 1: \"Our best quality dense single Transformer model (2.3B parameters) achieving ∆BLEU of 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3 core-years\n",
    "  - This is in the paper: \"Our best quality dense single Transformer model (2.3B parameters) achieving ∆BLEU of 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3 core-years.\"\n",
    "  - And that quote is part of the caption for Figure 1 in the paper. GPT just mashed them together.\n",
    "- [PASSED] Quote not found in albert_a_lite_bert_for_self-supervised_learning_of_language_representations.: All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate 0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise specified. Training was done on Cloud TPU V3\n",
    "  - This is an exact quote, I think it's just an issue with missing spaces on newlines\n",
    "- [PASSED] Quote not found in mastering_chess_and_shogi_by_self-play_with_a_general_reinforcement_learning_algorithm: We trained a separate instance of AlphaZero for each game. Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters, using 5,000 first-generation TPUs (15) to generate self-play games and 64 second-generation TPUs to train the neural networks\n",
    "  - Actual quote: \"We trained a separate instance of AlphaZero for each game. Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters, using 5,000 ﬁrst-generation TPUs (15) to generate self-play games and 64 second-generation TPUs to train the neural networks\"\n",
    "  - Looks correct, maybe an issue with newline vs. spaces again\n",
    "- [QUESTIONABLE] Quote not found in mastering_chess_and_shogi_by_self-play_with_a_general_reinforcement_learning_algorithm: AlphaZero was executed on a single machine with 4 TPUs during evaluation\n",
    "  - Closest I could find: \"During evaluation, AlphaZero selects moves greedily with respect to the root visit count. Each MCTS was executed on a single machine with 4 TPUs.\"\n",
    "  - Hmm, this is borderline. \n",
    "- [PASSED] Quote not found in attention_is_all_you_need: Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature\n",
    "  - Actual quote: \"Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature\"\n",
    "  - Looks correct, maybe an issue with newline vs. spaces again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
